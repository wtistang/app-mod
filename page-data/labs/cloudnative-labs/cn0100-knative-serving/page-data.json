{"componentChunkName":"component---src-pages-labs-cloudnative-labs-cn-0100-knative-serving-index-mdx","path":"/labs/cloudnative-labs/cn0100-knative-serving/","result":{"pageContext":{"frontmatter":{"title":"Knative Introduction","description":null},"relativePagePath":"/labs/cloudnative-labs/cn0100-knative-serving/index.mdx","titleType":"page","MdxNode":{"id":"3563884a-90d1-58fa-a711-87f095a4ab04","children":[],"parent":"36f5e697-6490-59ca-adec-521d4455f627","internal":{"content":"---\ntitle: Knative Introduction\ndescription: \n---\nThe goal of this session is to provide a hands-on experience withon how to work with OpenShift Kanative.\n\n### 1. OpenShift Serverless Introduction\nOpenShift Serverless is based on Knative, an open-source project started by Google. Specifically, OpenShift Serverless uses the Serving component of Knative. Knative Serving extends Kubernetes using Custom Resource Definitions (CRDs) to support deploying and serving of serverless applications and functions. Knative Serving is used to run containerized applications with Knative abstracting away details, such as networking, autoscaling (including scaling down to zero), and revision tracking. \n\nKnative Serving is ideal for running your application services inside Kubernetes by providing a more simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. The Knative platform will manage your service’s deployments, revisions, networking and scaling. Knative Serving exposes your service via an HTTP URL and has a lot of sane defaults for its configurations. For many practical use cases you might need to tweak the defaults to your needs and might also need to adjust the traffic distribution amongst the service revisions. As the Knative Serving Service has the built in ability to automatically scale down to zero when not in use, it is apt to call it as “serverless service”.\n\nIn this lab, you learn how to install OpenShift Serverless Operator and deploying a sample Quarkus application as a Knative Serving application. You will see that Knative Serving is easy to get started with and scales to support advanced scenarios.\n\n\n### 2. Objective\n\nThe objectives of this lab are to help you:\n\n* learn how to install OpenShift Serverless Operator\n* learn how to deploy a Knative Serving application\n\n### 3.\tPrerequisites\n\nThe following prerequisites must be completed prior to beginning this lab:\n*\tFamiliarity with basic Linux commands\n*\tHave internet access\n* Have basic knowledge of OpenShift Container Platform (OCP) web consle and commandline operations\n*\tHave a SkyTap App Mod Lab environment ready\n\n### 4.\tWhat is Already Completed\n\nA six Linux VMs App Mod Lab environment has been provided for this lab. \n \n  ![](images/lab-vms.png)\n\n*\tThe Red Hat OpenShift Container Platform (OCP) v4.6, is installed in 5 VMs, the master1 VM, the master2 VM, the master3 VM, the dns VM and the nfs VM, with 3 master nodes and 3 compute nodes (the master nodes are serving as computer nodes as well).\n*\tThe Workstation VM is the one you will use to access and work with OCP cluster in this lab.\n  The login credentials for the Workstation VM are:\n  User ID: **ibmdemo**\n  Password: **passw0rd**\n  Note: Use the Password above in the Workstation VM Terminal for sudo in the Lab.\n*\tThe CLI commands used in this lab are listed in the **Commands.txt** file located at the **/home/ibmdemo/add-mod-labs/cn0100st** directory of the Workstation VM for you to copy and paste these commands to the Terminal window during the lab.\n\n### 5.\tLab Tasks\n\nDuring this lab, you complete the following tasks:\n*\tInstall OpenShift Serverless Operator through OCP web console\n* deploy a sample deploy a Knative Serving application\n\n### 6.\tExecute Lab Tasks\n\n#### 6.1 Log in to the Workstation VM and get started \n1.  If the VMs are not already started, start them by clicking the play button for the whole group.\n\n  ![](images/start-vms.png)\n\n\n2.\tAfter the VMs are started, click the Workstation VM icon to access it. \n\n  ![](images/access-workstation.png)\n\n  The Workstation Linux Desktop is displayed. You execute all the lab tasks on this workstation VM.\n\n3.\tIf requested to log in to the Workstation OS, use credentials: **ibmdemo**/**passw0rd**\n\n#### 6.2 Install OpenShift Serverless Operator\n\nThe best way to add serverless capabilities to an OpenShift cluster is by installing the OpenShift Serverless Operator. Adding the operator to an OpenShift cluster is straightforward.\n\n1. Open a Firefox web browser window by clicking its icon on the Desktop toolbar.\n\n  ![](images/firefox-icon.png) \n\n2. From the browser window, click the **OpenShift web console** bookmark to open it.\n\n  ![](images/ocp-bookmark.png) \n\n3. If prompted to log in to Red Hat OpenShift Container Platform, click **htpasswd** field. Then log in with **ibmadmin**/**engageibm** as the username and password.\n \n  ![](images/ocp-console-login-1.png) \n\n  ![](images/ocp-console-login-2.png) \n\n  The OpenShift web console page is displayed. \n  \n  ![](images/ocp-page.png)\n\n4. Set the project to **openshift-operators** and click **Operators** > **Operator Hub** menu item.\n\n  ![](images/ocp-operatorhub-link.png)\n\n5. Search for **OpenShift Serverless** in the **Filter by keyword** search box. The Red Hat OpenShift Serverless Operator is shown.\n\n  ![](images/ocp-search-serverless-operator.png)\n\n6. Click **OpenShift Serverless Operator** to start installing. \n\n  ![](images/ocp-serverless-operator.png)\n\n7. Review all information on the page and click **Install**.\n\n  ![](images/ocp-install-serverless-operator.png)\n\n8. Accept all the default options and clicke **Install**.\n\n  ![](images/ocp-install-serverless-operator-2.png)\n\n  The installation process is started. OpenShift Serverless Operator depends on Red Hat OpenShift Service Mesh, which in turn depends on Elasticsearch, Jaeger, and Kiali. Wait till the Status column for all operators has a green checkmark indicating InstallSucceeded.\n\n9. Once the installation is completed, click **View Operator** to view its details.\n\n  ![](images/ocp-view-serverless-operator.png)\n\n#### 6.3 Create Serverless Serving Object\n\n1. Navigate to **Operators**>**Installed Operators** and change the project to **knative-serving**. This is where the Knative Serving object that manages all serverless applications on your cluster will live.\n\n  ![](images/ocp-installed-operators-knative-serving.png)\n\n  You see that all the operators installed in openshift-operators project get copied here.\n\n2. Click on the **OpenShift Serverless Operator** to access its details page.\n\n  ![](images/ocp-installed-serverless-operator-knative-serving.png)\n\n3. In the Operator Details view, click the **Knative Serving** tab link. If the view shows a 404 page, just refresh after a few seconds.\n\n  ![](images/ocp-knative-serving-tab.png)\n\n4. Click the **Create Knative Serving** button. \n\n  ![](images/ocp-create-knative-serving.png)\n\n5. In the Create Knative Serving form view, click **Create** to create the Serving object using the default out-of-box YAML file.\n\n  ![](images/ocp-create-knative-serving-2.png)\n\n  After a few minutes, Knative Serving object is created.\n\n  ![](images/ocp-knative-serving-created.png) \n  Once the Knative Serving object is created, users on your cluster can start deploying serverless applications. \n\n#### 6.4 Deploy Serverless Application\n\n1. Open a terminal window by clicing its icon from the Desktop toolbar.\n\n  ![](images/terminal-icon.png)\n\n2. Navigate to **/home/ibmdemo/app-mod-labs/cn0100st** directy.\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/cn0100st/ \n  ```\n3. List the contents in the directory.\n\n  ```\n  ls -l\n  ```\n\n  ```\n  total 12\n  -rw-rw-r-- 1 ibmdemo ibmdemo 352 Jul 15 12:52 colors-service-blue.yaml\n  -rw-rw-r-- 1 ibmdemo ibmdemo 354 Jul 15 12:53 colors-service-green.yaml\n  -rw-rw-r-- 1 ibmdemo ibmdemo 296 Jun 26  2020 Commands.txt\n  ```\n  The two YAML files are the knative service YAMLs you are going to deploy.\n\n4. Review the contents of **colors-service-blue** and **colors-service-green** with commands:\n\n  ```\n  cat colors-service-blue.yaml\n  ```\n  \n  ```\n  apiVersion: serving.knative.dev/v1\n  kind: Service\n  metadata:\n    name: blue-green-canary\n  spec:\n    template:\n      spec:\n        containers:\n          - image: quay.io/rhdevelopers/blue-green-canary\n            env:\n              - name: BLUE_GREEN_CANARY_COLOR\n                value: \"#6bbded\"\n              - name: BLUE_GREEN_CANARY_MESSAGE\n                value: \"Hello\"\n  ```\n\n  ```\n  cat colors-service-green.yaml\n  ```\n  \n  ```\n  apiVersion: serving.knative.dev/v1\n  kind: Service\n  metadata:\n    name: blue-green-canary\n  spec:\n    template:\n      spec:\n        containers:\n          - image: quay.io/rhdevelopers/blue-green-canary\n            env:\n              - name: BLUE_GREEN_CANARY_COLOR\n                value: \"#5bbf45\"\n              - name: BLUE_GREEN_CANARY_MESSAGE\n                value: \"Namaste\"\n  ```\n  As you can see they are sample Red Hat serverless service YAML for the same service **blue-green-canary** with different color schemes.\n\n5. Next issue the **oc login** command to login to the OCP cluster:\n\n  ```\n  oc login https://api.demo.ibmdte.net:6443\n  ```\n\n  when promted, enter the login credientials as: **ibmadmin**/**engageibm**.\n  \n  ```\n  Authentication required for https://api.demo.ibmdte.net:6443 (openshift)\n  Username: ibmadmin\n  Password: \n  Login successful.\n\n  You have access to 66 projects, the list has been suppressed. You can list all projects with ' projects'\n\n  Using project \"default\".\n  ``` \n  ```\n6. Create a new project called **cn0100st**.\n\n  ```\n  oc new-project cn0100st\n  ```\n  \n  ```\n  Now using project \"cn0100st\" on server \"https://api.demo.ibmdte.net:6443\".\n\n  You can add applications to this project with the 'new-app' command. For example, try:\n\n    oc new-app rails-postgresql-example\n\n  to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:\n\n    kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname\n\n  ```\n\n7. Deploy the severless application with command:\n\n  ```\n  oc apply -f colors-service-blue.yaml\n  ```\n  \n  ```\n  service.serving.knative.dev/blue-green-canary created\n  ```\n  \n8. Go back to to OCP web console and naviagte to **Workloads**>**pods** under **cn0100st** project, you can see the serverless pod is running.\n\n  ![](images/ocp-serverless-pod.png)\n\n9. Run the Knative CLI (kn) command below in the terminal window to obtain the service URL information.\n\n  ```\n  kn service describe blue-green-canary -o url\n  ```\n\n  ```\n  url: http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n  ```\n\n  The service URL is: **http://blue-green-canary-cn0100st.apps.demo.ibmdte.net**\n\n10. Launch the service URL in a new browser window to invoke the service, you see a blue background browser page, with greeting as **Hello**.\n\n  ![](images/ocp-invoke-serverless-service.png)\n\n11. Close the service window, wait for a few minutes and go back to the OCP web console and check the pod under the **cn0100st** project, you see there is no any pods. This is because it is a serverless service, which means that once no more traffic is seen going into the service, it comes down automatically. This is called **scale-to-zero**.\n\n12. Re-launch the service URL in a new browser window to invoke the service, after a few seconds, the service comes back. If you go back to the OC web console, you see the service pod is running again.\n\n#### 6.5 Traffic distribution \n1. From the termical window, deploy the version 2 of the **blue-green-canary** service with command:\n\n  ```\n  oc apply -f colors-service-green.yaml\n  ```\n\n  ```\n  service.serving.knative.dev/blue-green-canary configured\n  ```\n2. Go back to the web browser window to invoke the service again using the service URL, you see a green color browser page with greeting **Namaste**\n\n  ![](images/ocp-invoke-serverless-service-v2.png.png)\n\n3. Check the service revisions with **kn revision list** command:\n\n  ```\n  kn revision list\n  ```\n  \n  ```\n  NAME                      SERVICE             TRAFFIC   TAGS   GENERATION   AGE   CONDITIONS   READY   REASON\n  blue-green-canary-00002   blue-green-canary   100%             2            13m   4 OK / 4     True    \n  blue-green-canary-00001   blue-green-canary                    1            42m   3 OK / 4     True    \n  ``\n\n  As you can see from the output, the Knative service **blue-green-canary** now has two revisions namely **blue-green-canary-00001** and **blue-green-canary-00002**. When Knative rolls out a new revision, it increments the **GENERATION** by 1 and then routes 100% of the **TRAFFIC** to it, hence you can use the **GENERATION** or **TRAFFIC** to identify the latest reivsion.\n\n  Since the **Revision** names are autogenerated it is hard to comprehend to which code/configuration set it corresponds to. Knative provides a solution to tag the revision names with a logical definition, so it is easy to identify the revision and to perform traffic distribution amongst them..\n  \n  Knative offers a simple way of switching 100% of the traffic from one Knative service revision to another newly rolled out revision. If the new revision has erroneous behavior then it is easy to rollback the change with tags.\n  \n4.  Next you apply a tag named **blue** to revision **blue-green-canary-00001** and apply a tag named **green** and a tag named **latest** to revision **blue-green-canary-00002**.\n\n  ```\n  kn service update blue-green-canary --tag=blue-green-canary-00001=blue\n  kn service update blue-green-canary --tag=blue-green-canary-00002=green\n  kn service update blue-green-canary --tag=@latest=latest\n  ```\n  ```\n   --tag=blue-green-canary-00001=blue\n  Updating Service 'blue-green-canary' in namespace 'cn0100st':\n\n    0.059s The Route is still working to reflect the latest desired specification.\n    0.138s Ingress has not yet been reconciled.\n    0.187s Waiting for load balancer to be ready\n    0.368s Ready to serve.\n\n  Service 'blue-green-canary' with latest revision 'blue-green-canary-00002' (unchanged) is available at URL:\n  http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n\n  ```\n\n  ```\n  Updating Service 'blue-green-canary' in namespace 'cn0100st':\n\n  0.043s The Route is still working to reflect the latest desired specification.\n  0.147s Ingress has not yet been reconciled.\n  0.213s Waiting for load balancer to be ready\n  0.316s Ready to serve.\n\n  Service 'blue-green-canary' with latest revision 'blue-green-canary-00002' (unchanged) is available at URL:\n  http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n\n  ```\n  ```\n  Updating Service 'blue-green-canary' in namespace 'cn0100st':\n\n  0.054s The Route is still working to reflect the latest desired specification.\n  0.224s Ingress has not yet been reconciled.\n  0.290s Waiting for load balancer to be ready\n  0.361s Ready to serve.\n\n  Service 'blue-green-canary' with latest revision 'blue-green-canary-00002' (unchanged) is available at URL:\n  http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n  ```\n5. Check the service revisions with **kn revision list** command again, you see the tags are added to the revisions.\n\n  ```\n  kn revision list\n  ```\n\n  ```\n  NAME                      SERVICE             TRAFFIC   TAGS           GENERATION   AGE   CONDITIONS   READY   REASON\n  blue-green-canary-00002   blue-green-canary   100%      latest,green   2            15h   4 OK / 4     True    \n  blue-green-canary-00001   blue-green-canary             blue           1            16h   3 OK / 4     True  \n  ```\n  \n  With the tags added to the revision, you can apply the Blue-Green deployment pattern with the Knative Service. Assumming that due to a critical bug in revision **blue-green-canary-00002** you need to roll back to revision **blue-green-canary-00001**, you can apply the Blue-Green deployment pattern to the service using **kn** command to roll back the older revision.\n\n6. Run the **kn** command below to roll back the revision.\n\n  ```\n  kn service update blue-green-canary --traffic blue=100,green=0,latest=0\n  ```\n  ```\n  Updating Service 'blue-green-canary' in namespace 'cn0100st':\n\n  0.029s The Route is still working to reflect the latest desired specification.\n  0.111s Ingress has not yet been reconciled.\n  0.155s Waiting for load balancer to be ready\n  0.335s Ready to serve.\n\n  Service 'blue-green-canary' with latest revision 'blue-green-canary-00002' (unchanged) is available at URL:\n  http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n\n  ```\n7. Check the service revisions with **kn revision list** command one more time, you see the tranfic is now directed to the **blue-green-canary-00001** revision.\n\n  ```\n  kn revision list\n  ```\n\n  ```\n  NAME                      SERVICE             TRAFFIC   TAGS           GENERATION   AGE   CONDITIONS   READY   REASON\n  blue-green-canary-00002   blue-green-canary             latest,green   2            15h   4 OK / 4     True    \n  blue-green-canary-00001   blue-green-canary   100%      blue           1            16h   4 OK / 4     True  \n  ```\n\n8. Go back to the web browser window to invoke the service by refrshing it, you see a blue color browser page with greeting **Hello**\n\n  ![](images/ocp-invoke-serverless-service.png.png)\n\n9. Check the service pod with **oc get pod**, you see that only the **blue-green-canary-00001** pod is avaible and the **blue-green-canary-00002** pod has been terminated.\n\n  ```\n  oc get pod\n  ```\n\n  ```\n  NAME                                                  READY   STATUS    RESTARTS   AGE\n  blue-green-canary-00001-deployment-578d785bf5-wp2hp   2/2     Running   0          6m32s\n  ```\n\n  You can also apply the Canary Release Pattern to the serverless service. A Canary release is more effective when you want to reduce the risk of introducing new feature. It allows you a more effective feature-feedback loop before rolling out the change to your entire user base. Knative allows you to split the traffic between revisions in increments as small as 1%.\n\n10. Issue the **kn** command below to allow 80% of the traffic to the **blue-green-canary-00001** revision and the rest of the tranfic to go the the **blue-green-canary-00002** revision.\n\n  ```\n  kn service update blue-green-canary --traffic=\"blue=80\" --traffic=\"green=20\"\n  ```\n11. Check the service revisions with **kn revision list** command one more time, you see the tranfic is directed to both revisions as designed.\n\n  ```\n  kn revision list\n  ```\n\n  ```\n  NAME                      SERVICE             TRAFFIC   TAGS           GENERATION   AGE   CONDITIONS   READY   REASON\n  blue-green-canary-00002   blue-green-canary   20%       latest,green   2            15h   4 OK / 4     True    \n  blue-green-canary-00001   blue-green-canary   80%       blue           1            16h   4 OK / 4     True    \n  ```\n\n12. Check the service pod with **oc get pod**, you see that both revision pods are avaible now.\n\n  ```\n  oc get pod\n  ```\n\n  ```\n  NAME                                                  READY   STATUS    RESTARTS   AGE\n  blue-green-canary-00001-deployment-578d785bf5-wp2hp   2/2     Running   0          17m\n  blue-green-canary-00002-deployment-845bcd7469-22rjx   2/2     Running   0          2m41s\n  ```\n13. Go to the service web page, you can see the outputs of both service revisions.\n14. Close the service web page.\n\n#### 6.5 Scaling\nAs you know that **scale-to-zero** is one of the main properties making Knative a serverless platform. After a defined time of idleness (the so called **stable-window**) a revision is considered inactive. Now, all routes pointing to the now inactive revision will be pointed to the so-called activator. This reprogramming of the network is asynchronous in nature so the **scale-to-zero-grace-period** should give enough slack for this to happen. Once the **scale-to-zero-grace-period** is over, the revision will finally be scaled to zero replicas. If another request tries to get to this revision, the activator will get it, instruct the autoscaler to create new pods for the revision as quickly as possible and buffer the request until those new pods are created.\n\nBy default the **scale-to-zero-grace-period** is **30s**, and the **stable-window** is **60s**. Firing a request to the greeter service will bring up the pod if it is already terminated to serve the request. Leaving it without any further requests will automatically cause it to scale to zero in approx 60-70 secs. There are at least 20 seconds after the pod starts to terminate and before it’s completely terminated. This gives the Kourier Ingress enough time to leave out the pod from its own networking configuration.\n\nBy default Knative Serving allows **100** concurrent requests into a pod. This is defined by the **container-concurrency-target-default** setting in the configmap config-autoscaler in the **knative-serving** namespace.\n\nIn the next lab task, you make your service handle only 10 concurrent requests. This will cause Knative autoscaler to scale to more pods as soon as you run more than 10 requests in parallel against the revision.\n1. From the terminal window, view the content of the **service-10.yaml** file with command:\n\n  ```\n  cat service-10.yaml\n  ```\n\n  ```\n  apiVersion: serving.knative.dev/v1\n  kind: Service\n  metadata:\n    name: prime-generator\n  spec:\n    template:\n      metadata:\n        annotations:\n          # Target 10 in-flight-requests per pod.\n          autoscaling.knative.dev/target: \"10\"\n      spec:\n        containers:\n        - image: quay.io/rhdevelopers/prime-generator:v27-quarkus\n          livenessProbe:\n            httpGet:\n              path: /healthz\n          readinessProbe:\n            httpGet:\n              path: /healthz\n  ```\n  The Knative service definition allows each service pod to handle max of 10 in-flight requests per pod (configured via **autoscaling.knative.dev/target** annotation) before automatically scaling to new pod(s).\n2. Deploy the service with command:\n\n  ```\n  oc apply -f service-10.yaml\n  ```\n3. Go back to OCP web console, navigate to **Workloads**>**Pods** under **cn0100st** project scope and wait for all pods to be terminated.\n\n  ![](images/ocp-cn0100st-pod.png)\n4. Run the command below to load the service.\n\n  ```\n  export SVC_URL=$(kn service describe prime-generator -o url) \n  hey -c 50 -z 10s \"$SVC_URL/?sleep=3&upto=10000&memload=100\"\n  ```\n  \n  You are sending some load to the **prime-generator** service. The command sends 50 concurrent requests (-c 50) for the next 10s (-z 10s)\n\n5. From the OCP web console, you can see that after you have successfully run this small load test, the number of greeter service pods will have scaled to more pods automatically.\n\n  ![](images/ocp-cn0100st-add-pods.png)\n\n  In real world scenarios your service might need to handle sudden spikes in requests. Knative starts each service with a default of 1 replica. As described above, this will eventually be scaled to zero as described above. If your app needs to stay particularly responsive under any circumstances and/or has a long startup time, it might be beneficial to always keep a minimum number of pods around. This can be done via an the annotation **autoscaling.knative.dev/minScale**.\n  The following task, you learn how to make Knative create services that start with a replica count of 2 and never scale below it.\n\n6. From the termical window, issue the following command to view the **service-min-max-scale.yaml** file.\n\n  ```\n  cat service-min-max-scale.yaml\n  ```\n\n  ```\n  apiVersion: serving.knative.dev/v1\n  kind: Service\n  metadata:\n    name: prime-generator\n  spec:\n    template:\n      metadata:\n        annotations:\n          # the minimum number of pods to scale down to\n          autoscaling.knative.dev/minScale: \"2\"\n          # Target 10 in-flight-requests per pod.\n          autoscaling.knative.dev/target: \"10\"\n      spec:\n        containers:\n        - image: quay.io/rhdevelopers/prime-generator:v27-quarkus\n          livenessProbe:\n            httpGet:\n              path: /healthz\n          readinessProbe:\n            httpGet:\n              path: /healthz\n  ```\n  The deployment of this service will always have a minimum of 2 pods and it allows each service pod to handle max of 10 in-flight requests per pod before automatically scaling to new pods.\n\n7. Deploy the service with command:\n\n  ```\n  oc apply -f service-min-max-scale.yaml\n  ```\n\n8. Now apply some work load to the service.\n\n  ```\n  hey -c 50 -z 10s \"$SVC_URL/?sleep=3&upto=10000&memload=100\"\n  ```\n9. Observe the pod changes in the OCP web console.  You can that shen all requests are done and if beyond the scale-to-zero-grace-period, Knative has terminated all but 2 pods. This is because you have configured Knative to always run two pods via the annotation autoscaling.knative.dev/minScale: \"2\".\n### 7.\tSummary\nIn this lab, you have learned some basic features and functions of OpenShift Serverless (Knative) and how to use it. To learn more about App Mod and DevOps, please continue with the rest of the lab series.\n\n**Congratulations! You have successfully completed OpenShift Serverless (Knative) Introduction Lab!**\n\n","type":"Mdx","contentDigest":"ecc690666fe1f1e84cd79af4ce9c2876","owner":"gatsby-plugin-mdx","counter":2321},"frontmatter":{"title":"Knative Introduction","description":null},"exports":{},"rawBody":"---\ntitle: Knative Introduction\ndescription: \n---\nThe goal of this session is to provide a hands-on experience withon how to work with OpenShift Kanative.\n\n### 1. OpenShift Serverless Introduction\nOpenShift Serverless is based on Knative, an open-source project started by Google. Specifically, OpenShift Serverless uses the Serving component of Knative. Knative Serving extends Kubernetes using Custom Resource Definitions (CRDs) to support deploying and serving of serverless applications and functions. Knative Serving is used to run containerized applications with Knative abstracting away details, such as networking, autoscaling (including scaling down to zero), and revision tracking. \n\nKnative Serving is ideal for running your application services inside Kubernetes by providing a more simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. The Knative platform will manage your service’s deployments, revisions, networking and scaling. Knative Serving exposes your service via an HTTP URL and has a lot of sane defaults for its configurations. For many practical use cases you might need to tweak the defaults to your needs and might also need to adjust the traffic distribution amongst the service revisions. As the Knative Serving Service has the built in ability to automatically scale down to zero when not in use, it is apt to call it as “serverless service”.\n\nIn this lab, you learn how to install OpenShift Serverless Operator and deploying a sample Quarkus application as a Knative Serving application. You will see that Knative Serving is easy to get started with and scales to support advanced scenarios.\n\n\n### 2. Objective\n\nThe objectives of this lab are to help you:\n\n* learn how to install OpenShift Serverless Operator\n* learn how to deploy a Knative Serving application\n\n### 3.\tPrerequisites\n\nThe following prerequisites must be completed prior to beginning this lab:\n*\tFamiliarity with basic Linux commands\n*\tHave internet access\n* Have basic knowledge of OpenShift Container Platform (OCP) web consle and commandline operations\n*\tHave a SkyTap App Mod Lab environment ready\n\n### 4.\tWhat is Already Completed\n\nA six Linux VMs App Mod Lab environment has been provided for this lab. \n \n  ![](images/lab-vms.png)\n\n*\tThe Red Hat OpenShift Container Platform (OCP) v4.6, is installed in 5 VMs, the master1 VM, the master2 VM, the master3 VM, the dns VM and the nfs VM, with 3 master nodes and 3 compute nodes (the master nodes are serving as computer nodes as well).\n*\tThe Workstation VM is the one you will use to access and work with OCP cluster in this lab.\n  The login credentials for the Workstation VM are:\n  User ID: **ibmdemo**\n  Password: **passw0rd**\n  Note: Use the Password above in the Workstation VM Terminal for sudo in the Lab.\n*\tThe CLI commands used in this lab are listed in the **Commands.txt** file located at the **/home/ibmdemo/add-mod-labs/cn0100st** directory of the Workstation VM for you to copy and paste these commands to the Terminal window during the lab.\n\n### 5.\tLab Tasks\n\nDuring this lab, you complete the following tasks:\n*\tInstall OpenShift Serverless Operator through OCP web console\n* deploy a sample deploy a Knative Serving application\n\n### 6.\tExecute Lab Tasks\n\n#### 6.1 Log in to the Workstation VM and get started \n1.  If the VMs are not already started, start them by clicking the play button for the whole group.\n\n  ![](images/start-vms.png)\n\n\n2.\tAfter the VMs are started, click the Workstation VM icon to access it. \n\n  ![](images/access-workstation.png)\n\n  The Workstation Linux Desktop is displayed. You execute all the lab tasks on this workstation VM.\n\n3.\tIf requested to log in to the Workstation OS, use credentials: **ibmdemo**/**passw0rd**\n\n#### 6.2 Install OpenShift Serverless Operator\n\nThe best way to add serverless capabilities to an OpenShift cluster is by installing the OpenShift Serverless Operator. Adding the operator to an OpenShift cluster is straightforward.\n\n1. Open a Firefox web browser window by clicking its icon on the Desktop toolbar.\n\n  ![](images/firefox-icon.png) \n\n2. From the browser window, click the **OpenShift web console** bookmark to open it.\n\n  ![](images/ocp-bookmark.png) \n\n3. If prompted to log in to Red Hat OpenShift Container Platform, click **htpasswd** field. Then log in with **ibmadmin**/**engageibm** as the username and password.\n \n  ![](images/ocp-console-login-1.png) \n\n  ![](images/ocp-console-login-2.png) \n\n  The OpenShift web console page is displayed. \n  \n  ![](images/ocp-page.png)\n\n4. Set the project to **openshift-operators** and click **Operators** > **Operator Hub** menu item.\n\n  ![](images/ocp-operatorhub-link.png)\n\n5. Search for **OpenShift Serverless** in the **Filter by keyword** search box. The Red Hat OpenShift Serverless Operator is shown.\n\n  ![](images/ocp-search-serverless-operator.png)\n\n6. Click **OpenShift Serverless Operator** to start installing. \n\n  ![](images/ocp-serverless-operator.png)\n\n7. Review all information on the page and click **Install**.\n\n  ![](images/ocp-install-serverless-operator.png)\n\n8. Accept all the default options and clicke **Install**.\n\n  ![](images/ocp-install-serverless-operator-2.png)\n\n  The installation process is started. OpenShift Serverless Operator depends on Red Hat OpenShift Service Mesh, which in turn depends on Elasticsearch, Jaeger, and Kiali. Wait till the Status column for all operators has a green checkmark indicating InstallSucceeded.\n\n9. Once the installation is completed, click **View Operator** to view its details.\n\n  ![](images/ocp-view-serverless-operator.png)\n\n#### 6.3 Create Serverless Serving Object\n\n1. Navigate to **Operators**>**Installed Operators** and change the project to **knative-serving**. This is where the Knative Serving object that manages all serverless applications on your cluster will live.\n\n  ![](images/ocp-installed-operators-knative-serving.png)\n\n  You see that all the operators installed in openshift-operators project get copied here.\n\n2. Click on the **OpenShift Serverless Operator** to access its details page.\n\n  ![](images/ocp-installed-serverless-operator-knative-serving.png)\n\n3. In the Operator Details view, click the **Knative Serving** tab link. If the view shows a 404 page, just refresh after a few seconds.\n\n  ![](images/ocp-knative-serving-tab.png)\n\n4. Click the **Create Knative Serving** button. \n\n  ![](images/ocp-create-knative-serving.png)\n\n5. In the Create Knative Serving form view, click **Create** to create the Serving object using the default out-of-box YAML file.\n\n  ![](images/ocp-create-knative-serving-2.png)\n\n  After a few minutes, Knative Serving object is created.\n\n  ![](images/ocp-knative-serving-created.png) \n  Once the Knative Serving object is created, users on your cluster can start deploying serverless applications. \n\n#### 6.4 Deploy Serverless Application\n\n1. Open a terminal window by clicing its icon from the Desktop toolbar.\n\n  ![](images/terminal-icon.png)\n\n2. Navigate to **/home/ibmdemo/app-mod-labs/cn0100st** directy.\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/cn0100st/ \n  ```\n3. List the contents in the directory.\n\n  ```\n  ls -l\n  ```\n\n  ```\n  total 12\n  -rw-rw-r-- 1 ibmdemo ibmdemo 352 Jul 15 12:52 colors-service-blue.yaml\n  -rw-rw-r-- 1 ibmdemo ibmdemo 354 Jul 15 12:53 colors-service-green.yaml\n  -rw-rw-r-- 1 ibmdemo ibmdemo 296 Jun 26  2020 Commands.txt\n  ```\n  The two YAML files are the knative service YAMLs you are going to deploy.\n\n4. Review the contents of **colors-service-blue** and **colors-service-green** with commands:\n\n  ```\n  cat colors-service-blue.yaml\n  ```\n  \n  ```\n  apiVersion: serving.knative.dev/v1\n  kind: Service\n  metadata:\n    name: blue-green-canary\n  spec:\n    template:\n      spec:\n        containers:\n          - image: quay.io/rhdevelopers/blue-green-canary\n            env:\n              - name: BLUE_GREEN_CANARY_COLOR\n                value: \"#6bbded\"\n              - name: BLUE_GREEN_CANARY_MESSAGE\n                value: \"Hello\"\n  ```\n\n  ```\n  cat colors-service-green.yaml\n  ```\n  \n  ```\n  apiVersion: serving.knative.dev/v1\n  kind: Service\n  metadata:\n    name: blue-green-canary\n  spec:\n    template:\n      spec:\n        containers:\n          - image: quay.io/rhdevelopers/blue-green-canary\n            env:\n              - name: BLUE_GREEN_CANARY_COLOR\n                value: \"#5bbf45\"\n              - name: BLUE_GREEN_CANARY_MESSAGE\n                value: \"Namaste\"\n  ```\n  As you can see they are sample Red Hat serverless service YAML for the same service **blue-green-canary** with different color schemes.\n\n5. Next issue the **oc login** command to login to the OCP cluster:\n\n  ```\n  oc login https://api.demo.ibmdte.net:6443\n  ```\n\n  when promted, enter the login credientials as: **ibmadmin**/**engageibm**.\n  \n  ```\n  Authentication required for https://api.demo.ibmdte.net:6443 (openshift)\n  Username: ibmadmin\n  Password: \n  Login successful.\n\n  You have access to 66 projects, the list has been suppressed. You can list all projects with ' projects'\n\n  Using project \"default\".\n  ``` \n  ```\n6. Create a new project called **cn0100st**.\n\n  ```\n  oc new-project cn0100st\n  ```\n  \n  ```\n  Now using project \"cn0100st\" on server \"https://api.demo.ibmdte.net:6443\".\n\n  You can add applications to this project with the 'new-app' command. For example, try:\n\n    oc new-app rails-postgresql-example\n\n  to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:\n\n    kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname\n\n  ```\n\n7. Deploy the severless application with command:\n\n  ```\n  oc apply -f colors-service-blue.yaml\n  ```\n  \n  ```\n  service.serving.knative.dev/blue-green-canary created\n  ```\n  \n8. Go back to to OCP web console and naviagte to **Workloads**>**pods** under **cn0100st** project, you can see the serverless pod is running.\n\n  ![](images/ocp-serverless-pod.png)\n\n9. Run the Knative CLI (kn) command below in the terminal window to obtain the service URL information.\n\n  ```\n  kn service describe blue-green-canary -o url\n  ```\n\n  ```\n  url: http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n  ```\n\n  The service URL is: **http://blue-green-canary-cn0100st.apps.demo.ibmdte.net**\n\n10. Launch the service URL in a new browser window to invoke the service, you see a blue background browser page, with greeting as **Hello**.\n\n  ![](images/ocp-invoke-serverless-service.png)\n\n11. Close the service window, wait for a few minutes and go back to the OCP web console and check the pod under the **cn0100st** project, you see there is no any pods. This is because it is a serverless service, which means that once no more traffic is seen going into the service, it comes down automatically. This is called **scale-to-zero**.\n\n12. Re-launch the service URL in a new browser window to invoke the service, after a few seconds, the service comes back. If you go back to the OC web console, you see the service pod is running again.\n\n#### 6.5 Traffic distribution \n1. From the termical window, deploy the version 2 of the **blue-green-canary** service with command:\n\n  ```\n  oc apply -f colors-service-green.yaml\n  ```\n\n  ```\n  service.serving.knative.dev/blue-green-canary configured\n  ```\n2. Go back to the web browser window to invoke the service again using the service URL, you see a green color browser page with greeting **Namaste**\n\n  ![](images/ocp-invoke-serverless-service-v2.png.png)\n\n3. Check the service revisions with **kn revision list** command:\n\n  ```\n  kn revision list\n  ```\n  \n  ```\n  NAME                      SERVICE             TRAFFIC   TAGS   GENERATION   AGE   CONDITIONS   READY   REASON\n  blue-green-canary-00002   blue-green-canary   100%             2            13m   4 OK / 4     True    \n  blue-green-canary-00001   blue-green-canary                    1            42m   3 OK / 4     True    \n  ``\n\n  As you can see from the output, the Knative service **blue-green-canary** now has two revisions namely **blue-green-canary-00001** and **blue-green-canary-00002**. When Knative rolls out a new revision, it increments the **GENERATION** by 1 and then routes 100% of the **TRAFFIC** to it, hence you can use the **GENERATION** or **TRAFFIC** to identify the latest reivsion.\n\n  Since the **Revision** names are autogenerated it is hard to comprehend to which code/configuration set it corresponds to. Knative provides a solution to tag the revision names with a logical definition, so it is easy to identify the revision and to perform traffic distribution amongst them..\n  \n  Knative offers a simple way of switching 100% of the traffic from one Knative service revision to another newly rolled out revision. If the new revision has erroneous behavior then it is easy to rollback the change with tags.\n  \n4.  Next you apply a tag named **blue** to revision **blue-green-canary-00001** and apply a tag named **green** and a tag named **latest** to revision **blue-green-canary-00002**.\n\n  ```\n  kn service update blue-green-canary --tag=blue-green-canary-00001=blue\n  kn service update blue-green-canary --tag=blue-green-canary-00002=green\n  kn service update blue-green-canary --tag=@latest=latest\n  ```\n  ```\n   --tag=blue-green-canary-00001=blue\n  Updating Service 'blue-green-canary' in namespace 'cn0100st':\n\n    0.059s The Route is still working to reflect the latest desired specification.\n    0.138s Ingress has not yet been reconciled.\n    0.187s Waiting for load balancer to be ready\n    0.368s Ready to serve.\n\n  Service 'blue-green-canary' with latest revision 'blue-green-canary-00002' (unchanged) is available at URL:\n  http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n\n  ```\n\n  ```\n  Updating Service 'blue-green-canary' in namespace 'cn0100st':\n\n  0.043s The Route is still working to reflect the latest desired specification.\n  0.147s Ingress has not yet been reconciled.\n  0.213s Waiting for load balancer to be ready\n  0.316s Ready to serve.\n\n  Service 'blue-green-canary' with latest revision 'blue-green-canary-00002' (unchanged) is available at URL:\n  http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n\n  ```\n  ```\n  Updating Service 'blue-green-canary' in namespace 'cn0100st':\n\n  0.054s The Route is still working to reflect the latest desired specification.\n  0.224s Ingress has not yet been reconciled.\n  0.290s Waiting for load balancer to be ready\n  0.361s Ready to serve.\n\n  Service 'blue-green-canary' with latest revision 'blue-green-canary-00002' (unchanged) is available at URL:\n  http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n  ```\n5. Check the service revisions with **kn revision list** command again, you see the tags are added to the revisions.\n\n  ```\n  kn revision list\n  ```\n\n  ```\n  NAME                      SERVICE             TRAFFIC   TAGS           GENERATION   AGE   CONDITIONS   READY   REASON\n  blue-green-canary-00002   blue-green-canary   100%      latest,green   2            15h   4 OK / 4     True    \n  blue-green-canary-00001   blue-green-canary             blue           1            16h   3 OK / 4     True  \n  ```\n  \n  With the tags added to the revision, you can apply the Blue-Green deployment pattern with the Knative Service. Assumming that due to a critical bug in revision **blue-green-canary-00002** you need to roll back to revision **blue-green-canary-00001**, you can apply the Blue-Green deployment pattern to the service using **kn** command to roll back the older revision.\n\n6. Run the **kn** command below to roll back the revision.\n\n  ```\n  kn service update blue-green-canary --traffic blue=100,green=0,latest=0\n  ```\n  ```\n  Updating Service 'blue-green-canary' in namespace 'cn0100st':\n\n  0.029s The Route is still working to reflect the latest desired specification.\n  0.111s Ingress has not yet been reconciled.\n  0.155s Waiting for load balancer to be ready\n  0.335s Ready to serve.\n\n  Service 'blue-green-canary' with latest revision 'blue-green-canary-00002' (unchanged) is available at URL:\n  http://blue-green-canary-cn0100st.apps.demo.ibmdte.net\n\n  ```\n7. Check the service revisions with **kn revision list** command one more time, you see the tranfic is now directed to the **blue-green-canary-00001** revision.\n\n  ```\n  kn revision list\n  ```\n\n  ```\n  NAME                      SERVICE             TRAFFIC   TAGS           GENERATION   AGE   CONDITIONS   READY   REASON\n  blue-green-canary-00002   blue-green-canary             latest,green   2            15h   4 OK / 4     True    \n  blue-green-canary-00001   blue-green-canary   100%      blue           1            16h   4 OK / 4     True  \n  ```\n\n8. Go back to the web browser window to invoke the service by refrshing it, you see a blue color browser page with greeting **Hello**\n\n  ![](images/ocp-invoke-serverless-service.png.png)\n\n9. Check the service pod with **oc get pod**, you see that only the **blue-green-canary-00001** pod is avaible and the **blue-green-canary-00002** pod has been terminated.\n\n  ```\n  oc get pod\n  ```\n\n  ```\n  NAME                                                  READY   STATUS    RESTARTS   AGE\n  blue-green-canary-00001-deployment-578d785bf5-wp2hp   2/2     Running   0          6m32s\n  ```\n\n  You can also apply the Canary Release Pattern to the serverless service. A Canary release is more effective when you want to reduce the risk of introducing new feature. It allows you a more effective feature-feedback loop before rolling out the change to your entire user base. Knative allows you to split the traffic between revisions in increments as small as 1%.\n\n10. Issue the **kn** command below to allow 80% of the traffic to the **blue-green-canary-00001** revision and the rest of the tranfic to go the the **blue-green-canary-00002** revision.\n\n  ```\n  kn service update blue-green-canary --traffic=\"blue=80\" --traffic=\"green=20\"\n  ```\n11. Check the service revisions with **kn revision list** command one more time, you see the tranfic is directed to both revisions as designed.\n\n  ```\n  kn revision list\n  ```\n\n  ```\n  NAME                      SERVICE             TRAFFIC   TAGS           GENERATION   AGE   CONDITIONS   READY   REASON\n  blue-green-canary-00002   blue-green-canary   20%       latest,green   2            15h   4 OK / 4     True    \n  blue-green-canary-00001   blue-green-canary   80%       blue           1            16h   4 OK / 4     True    \n  ```\n\n12. Check the service pod with **oc get pod**, you see that both revision pods are avaible now.\n\n  ```\n  oc get pod\n  ```\n\n  ```\n  NAME                                                  READY   STATUS    RESTARTS   AGE\n  blue-green-canary-00001-deployment-578d785bf5-wp2hp   2/2     Running   0          17m\n  blue-green-canary-00002-deployment-845bcd7469-22rjx   2/2     Running   0          2m41s\n  ```\n13. Go to the service web page, you can see the outputs of both service revisions.\n14. Close the service web page.\n\n#### 6.5 Scaling\nAs you know that **scale-to-zero** is one of the main properties making Knative a serverless platform. After a defined time of idleness (the so called **stable-window**) a revision is considered inactive. Now, all routes pointing to the now inactive revision will be pointed to the so-called activator. This reprogramming of the network is asynchronous in nature so the **scale-to-zero-grace-period** should give enough slack for this to happen. Once the **scale-to-zero-grace-period** is over, the revision will finally be scaled to zero replicas. If another request tries to get to this revision, the activator will get it, instruct the autoscaler to create new pods for the revision as quickly as possible and buffer the request until those new pods are created.\n\nBy default the **scale-to-zero-grace-period** is **30s**, and the **stable-window** is **60s**. Firing a request to the greeter service will bring up the pod if it is already terminated to serve the request. Leaving it without any further requests will automatically cause it to scale to zero in approx 60-70 secs. There are at least 20 seconds after the pod starts to terminate and before it’s completely terminated. This gives the Kourier Ingress enough time to leave out the pod from its own networking configuration.\n\nBy default Knative Serving allows **100** concurrent requests into a pod. This is defined by the **container-concurrency-target-default** setting in the configmap config-autoscaler in the **knative-serving** namespace.\n\nIn the next lab task, you make your service handle only 10 concurrent requests. This will cause Knative autoscaler to scale to more pods as soon as you run more than 10 requests in parallel against the revision.\n1. From the terminal window, view the content of the **service-10.yaml** file with command:\n\n  ```\n  cat service-10.yaml\n  ```\n\n  ```\n  apiVersion: serving.knative.dev/v1\n  kind: Service\n  metadata:\n    name: prime-generator\n  spec:\n    template:\n      metadata:\n        annotations:\n          # Target 10 in-flight-requests per pod.\n          autoscaling.knative.dev/target: \"10\"\n      spec:\n        containers:\n        - image: quay.io/rhdevelopers/prime-generator:v27-quarkus\n          livenessProbe:\n            httpGet:\n              path: /healthz\n          readinessProbe:\n            httpGet:\n              path: /healthz\n  ```\n  The Knative service definition allows each service pod to handle max of 10 in-flight requests per pod (configured via **autoscaling.knative.dev/target** annotation) before automatically scaling to new pod(s).\n2. Deploy the service with command:\n\n  ```\n  oc apply -f service-10.yaml\n  ```\n3. Go back to OCP web console, navigate to **Workloads**>**Pods** under **cn0100st** project scope and wait for all pods to be terminated.\n\n  ![](images/ocp-cn0100st-pod.png)\n4. Run the command below to load the service.\n\n  ```\n  export SVC_URL=$(kn service describe prime-generator -o url) \n  hey -c 50 -z 10s \"$SVC_URL/?sleep=3&upto=10000&memload=100\"\n  ```\n  \n  You are sending some load to the **prime-generator** service. The command sends 50 concurrent requests (-c 50) for the next 10s (-z 10s)\n\n5. From the OCP web console, you can see that after you have successfully run this small load test, the number of greeter service pods will have scaled to more pods automatically.\n\n  ![](images/ocp-cn0100st-add-pods.png)\n\n  In real world scenarios your service might need to handle sudden spikes in requests. Knative starts each service with a default of 1 replica. As described above, this will eventually be scaled to zero as described above. If your app needs to stay particularly responsive under any circumstances and/or has a long startup time, it might be beneficial to always keep a minimum number of pods around. This can be done via an the annotation **autoscaling.knative.dev/minScale**.\n  The following task, you learn how to make Knative create services that start with a replica count of 2 and never scale below it.\n\n6. From the termical window, issue the following command to view the **service-min-max-scale.yaml** file.\n\n  ```\n  cat service-min-max-scale.yaml\n  ```\n\n  ```\n  apiVersion: serving.knative.dev/v1\n  kind: Service\n  metadata:\n    name: prime-generator\n  spec:\n    template:\n      metadata:\n        annotations:\n          # the minimum number of pods to scale down to\n          autoscaling.knative.dev/minScale: \"2\"\n          # Target 10 in-flight-requests per pod.\n          autoscaling.knative.dev/target: \"10\"\n      spec:\n        containers:\n        - image: quay.io/rhdevelopers/prime-generator:v27-quarkus\n          livenessProbe:\n            httpGet:\n              path: /healthz\n          readinessProbe:\n            httpGet:\n              path: /healthz\n  ```\n  The deployment of this service will always have a minimum of 2 pods and it allows each service pod to handle max of 10 in-flight requests per pod before automatically scaling to new pods.\n\n7. Deploy the service with command:\n\n  ```\n  oc apply -f service-min-max-scale.yaml\n  ```\n\n8. Now apply some work load to the service.\n\n  ```\n  hey -c 50 -z 10s \"$SVC_URL/?sleep=3&upto=10000&memload=100\"\n  ```\n9. Observe the pod changes in the OCP web console.  You can that shen all requests are done and if beyond the scale-to-zero-grace-period, Knative has terminated all but 2 pods. This is because you have configured Knative to always run two pods via the annotation autoscaling.knative.dev/minScale: \"2\".\n### 7.\tSummary\nIn this lab, you have learned some basic features and functions of OpenShift Serverless (Knative) and how to use it. To learn more about App Mod and DevOps, please continue with the rest of the lab series.\n\n**Congratulations! You have successfully completed OpenShift Serverless (Knative) Introduction Lab!**\n\n","fileAbsolutePath":"/Users/yitang/Downloads/app-mod/src/pages/labs/cloudnative-labs/cn0100-knative-serving/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}