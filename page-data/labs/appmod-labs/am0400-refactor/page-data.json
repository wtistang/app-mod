{"componentChunkName":"component---src-pages-labs-appmod-labs-am-0400-refactor-index-mdx","path":"/labs/appmod-labs/am0400-refactor/","result":{"pageContext":{"frontmatter":{"title":"App Modernization with Microservice","description":null},"relativePagePath":"/labs/appmod-labs/am0400-refactor/index.mdx","titleType":"page","MdxNode":{"id":"6934faa9-96ee-563f-817c-595db6a9acdd","children":[],"parent":"73ee1add-4c2f-536a-8fad-93721ed5b4ad","internal":{"content":"---\ntitle: App Modernization with Microservice\ndescription: \n---\nThis lab exercise is a part of the Application Modernization lab series which focus on the evaluation, re-platforming, rehosting and refactoring application modernization approaches and other application modernization related solutions. In Part 1 of the Application Modernization Journey (Lab AM0100ST), you go through the process to evaluate the existing On-Prem Java applications and to identify the candidate to be moved to the cloud using the IBM Cloud Transformation Advisor. In Part 2 of the Application Modernization Journey (Lab AM0200ST), you learn how to re-platform an existing WebSphere application to a Liberty container and to deploy it to a Red Heat OpenShift Container Platform (OCP) cluster. In Part 3 of the Application Modernization Journey (Lab AM0300ST), you use the rehost process to move an existing WebSphere application to a WebSphere Application Server (WAS) container and to deploy it to a OCP cluster. In this lab, you start to refactor a monolithic Java application by moving one of its functions into a microservice and modifying the application to access the microservice via REST. Over time this pattern of moving services from an application to a microservice will allow for the eventual sunset of the initial monolithic application, once all of its function has been deployed as microservices. \n### 1. Business Scenario\n\nYour company has a traditional WebSphere Application Server application called PlantsByWebSphere, a monolith web application allowing users to search and order garden products online.  The application is running is a traditional WAS environment.\n\n![](images/pbw-app-home-page.png)\n\nAs a tech leader, you are leading the effort to modernize the monolith application using the refactor option. At your first approach, you focus on refactoring the image module of the monolith application and break the monolith application into two parts:\n\n* The original monolith ( **1**  in the image below) has been modified with a Servlet Filter to intercept requests for images and redirect the requests to a new image microservice application, the old image module is disabled in the modified code.  The modified monolith application is still running in the WAS environment.\n* The image module of the application is refactored as a new microservice application as showing in **2** in the image below. The microservice exposes the image service via a JAX-RS interface which accesses the image library and images in the database via JPA reusing the same code that was used in the original monolith application. The new image microservice application is containerized and deployed to a OCP cluster. \n\n![](images/modified-pbw-app.png)\n\nFor those interested, appendix 1 and appendix 2 contain the Servlet Filter and Image Service code.\n### 2. Objective\n\nThe objectives of this lab are to help you:\n\n* get familiar with the application modernization refactor process.\n*\tlearn the process to deploy a refactored microservice to OCP cluster.\n*\tlearn how to configure the modified monolith application to consume the refactored microservice from WAS console.\n\n### 3.\tPrerequisites\n\nThe following prerequisites must be completed prior to beginning this lab:\n* Familiarity with basic Linux commands\n* Have internet access\n*\tHave basic WAS Admin and Java app development knowledge.\n*\tHave completed Lab SU0100 of this lab series and have the CP4Apps installed in IBM Red Hat OpenShift Kubernetes Service (OCP) cluster on IBM Cloud.\n\n### 4.\tWhat is Already Completed\n\nThe Workstation VM is pre-configured for you to access and work with the OCP cluster in this lab.\n\nThe login credentials for the Workstation VM are provided in your Reservation Details page (**https://www.ibm.com/demos/my-reservations/**).\n\nThe CLI commands used in this lab are listed in the Commands.txt file located at the **/home/ibmdemo/app-mod-labs/am0400** directory of the Workstation VM for you to copy and paste these commands to the Terminal window during the lab.\n\n### 5.\tLab Tasks\nDuring this lab, you complete the following tasks:\n*\treview the original and modified versions of PlantsByWebSphere application on local WAS server.\n*\tdeploy the image microservice to a OCP cluster.\n*\ttest and verify the deployed image microservice.\n*\tconfigure the modified PlantsByWebSphere application to consume the image microservice in WAS console.\n*\ttest and verify the modified PlantsByWebSphere.\n\n\n### 6.\tExecute Lab Tasks\n#### 6.1 Log in to the Workstation VM and get started \n1.  Launch the VM from your local workstation web browser with the VM URL.\n \n    ![](images/vm-url.png)\n\n2.\tClick **Connect** to access the VM.\n \n    ![](images/vm-connect.png)\n\n3.\tEnter the password you obtained from the Reservation Details page and click **Send Password**.\n\n    ![](images/vm-password.png)\n\n    The VM desktop is displayed.\n\n    ![](images/vm-desktop.png)\n\n    The Workstation Desktop is displayed. You execute all the lab tasks on this VM.\n\n#### 6.2 Review the on-premises WebSphere application \nIn this task, you take a look at **Mod Resorts** application deployed to the local WebSphere Application Server (WAS) environment. You are going to move this application to the cloud using Open Liberty Operator later.\n\n1.\tStart WebSphere Application Server\n\n    In the workstation VM, you have a local WebSphere Application Server V9 which hosts the Mod Resorts application. \n\n    To start the WAS server:\n\n    a.\tOpen a terminal window by clicking **Applications**>**Terminal**.\n \n    ![](images/vm-terminal-link.png)\n\n    b.\tIn the terminal window, issue the command below to start the WAS server (You can copy and paste the command from the Commands.txt file in the /home/ibmdemo/app-mod-labs/am0200 directory).\n\n    ```\n    /home/ibmdemo/app-mod-labs/shared/startWASV9.sh\n    ```\n    \n    Within a few minutes the WAS server is ready.\n\n    c.\tAccess the WAS Admin Console to view the application deployed by clicking **Applications**>**Firefox** to open a browser window.\n \n    ![](images/vm-browser-link.png)\n\n    d.\tFrom the web browser window, type the WebSphere Integrated Solution Console URL as: https://localhost:9044/ibm/console and press **Enter** to launch the WAS V9 console.\n\n    e.\tIn the WAS Admin Console login page, enter the User ID and Password as: **wsadmin**/**passw0rd** and click **Login**. \n\n    f.\tOn the WAS Console page, click **Applications** -> **Application Types** -> **WebSphere enterprise applications** to view the apps deployed.\n \n    ![](images/was-enterprise-apps-link.png)\n\n    In the Enterprise Applications list, you can see two versions of the **PlantsByWebSphere** applications deployed, the original and the modified. Due to the port conflict, both versions of the application cannot run at the same time. Currently the original version is running. \n    \n    ![](images/was-enterprise-apps.png)\n\n2.\tView the original **PlantsByWebSphere** application\n\n    a. From the web browser window, click new Tab to open a new browser window with the **PlantsByWebSphere** application URL: http://localhost:9081/PlantsByWebSphere.\n    \n    The **PlantsByWebSphere** application home page is displayed.\n\n    ![](images/pbw-app-home-page-2.png)\n\n    This is a typical online shopping application, you can click the Flowers, Fruits & Vegetables, Trees and Accessories tabs to view the product catalogs. \n    \n    ![](images/pbw-catalog-page.png)\n\n3. View the modified **PlantsByWebSphere** application.\n\n\n  a.\tGo back to WAS console windows and navigate to **Applications** > **Application Types** > **WebSphere enterprise applications** page.\n\n__b.\tStop the original PlantsByWebSphere application by checking the box next to PlantsByWebSphereV90 and clicking Stop.\n \n__c.\tStart the modified version of the application by checking the box next to PlantsByWebSphereV90V2 and clicking Start.\n \n__d.\tClick the refresh icon next to the Application Status.\n \nYou see that the modified application is running now\n \n__e.\tGo to the PlantsByWebSphere application window to relaunch it with URL: http://localhost:9081/PlantsByWebSphere.  You see that its home page looks below without any images.  This is because the original image molder is disable and the new image microservice the application is looking for is not available yet.\n \nClick **WHERE TO?** dropdown menu to see the city list.\n\n    ![](images/mod-resorts-app-where-to.png)\n\n    c. Click **PARIS, FRANCE** from the list, it shows the weather of the city.\n\n    ![](images/mod-resorts-app-paris.png)\n\n    You have reviewed the application.  Next you learn how to re-platform the application in WAS container without any code change and to deploy it to an OpenShift cluster.\n\n    d. Stop the WAS server by issuing the following command in the Terminal window:\n\n    ```\n    /home/ibmdemo/IBM/WebSphere/AppServer/profiles/AppSrv01/bin/stopServer.sh server1\n    ```   \n\n#### 6.3\tBuild a WAS Base Server Container Image.\nIn this task, you are going to build a WAS Base server Docker container image with the Mod Resorts application installed. \n\nNote: according to Docker's best practices you should create a new WAS Base image which adds a single application and the corresponding configuration. You should avoid configuring the image manually (after it started) via Admin Console or wsadmin unless it is for debugging purposes, because such changes won't be present if you spawn a new container from the image.\n\nThere are five key files you needed to build your WAS Base server container image:\n*\tDockerfile – the file defines how the Docker image that has your application and configuration pre-loaded is built.\n*\tApp runtime – the ear or war file of your application.\n*\tapp-install.props – the file defines how to install your application in the WAS server.\n*\tappConfig.py – the WAS admin script in Jython format configures the WAS server for your application.\n*\tPASSWORD – the file contains the WAS server console password use in the WAS container.\n\n1.\tReview Dockerfile, app-install.props file, and appConfig.py file.\n\n    a.\tClick **Applications**>**Files** to open the File Manager.\n\n    ![](images/vm-files-link.png)\n  \n    b.\tNavigate to **/home/ibmdemo/app-mod-labs/am0300** directory.\n \n    c.\tDouble click the **Dockerfile** to open it in Text editor for reviewing.\n \n    ![](images/ocp-twas-dockerfile.png)\n    As you can see, the Dockerfile file defines the following activities to create a WAS Base container image:\n    *\tGet the base WAS image from Docker Hub\n    *\tAdd WAS admin console password to the WAS Base image\n    *\tAdd the WAS server configuration script to the WAS base image\n    *\tAdd the application installation script to the WAS base image\n    *\tAdd the application runtime file to the WAS base image\n    *\tRun the configuration script to config the WAS server instance inside the container to configure the WAS server and to install the application\n    \n    d.\tGo back to File Manager and double click **app-install.props** file to review it. This is the script file you use to install the Mod Resorts application to the WAS server instance. \n    \n    ![](images/ocp-twas-app-install-props.png)\n    \n    e.\tIn the File Manager window, double click **appConfig.py** file to review its contents. This is a standard WAS admin script with Jython format for configuring the WAS to run the Mod Resorts application. This is the WAS admin script file you use to convert your application settings, include JDBC and JMS resources, from a WAS ND cell to a WAS Base container. \n \n    ![](images/pco-twas-appconfig-py.png)\n\n2.\tBuild the WAS Base server container image.\n\n    a. Go back to the Terminal window and navigate to the **/home/ibmdemo/app-mod-labs/am0300** directory with command:\n    \n    ```\n    cd /home/ibmdemo/app-mod-labs/am0300 \n    ```\n    \n    b. Execute the following command to build the WAS Base docker container image with the Dockerfile you just reviewed:\t\n\n    ```\n    docker build . -t modresorts-twas:latest\n    ```\n\n    This creates a WAS Base docker image called modresorts-twas:latest.\n\n    c. After the docker container image is created, you can issue the command below to check it:\n\n    ```\n    docker images |grep twas\n    ```\n    \n    You see the docker image is created.\n\n    d. To verify the docker image, you can create and run a local container called modresorts-twas with the command below:\n\n    ```\n    docker run --name modsorts-twas -p 9443:9443 -d modresorts-twas:latest\n    ```\n\n    e. once the container is created, you can test the Moderesorts application by launching it in a web browser window with the URL: **https://localhost:9443/resorts**.\n\n#### 6.4\tPush the WAS Container Image to OCP Image Registry\nAfter the WAS container image is built, you need to push it to an image registry first. In this lab, you are using the OCP Image Registry to host your WAS container image.\n\n1.\tFrom the browser window, click OCP cluster Web Console bookmark to open it with your IBM credentials.\n\n  ![](images/ocp-console-bookmarked.png) \n2.\tIn the OpenShift Web Console page, click the **Action** icon next to your username and select **Copy Login** Command to get the OCP login command.\n\n  ![](images/ocp-copy-login-command.png) \n3.\tIn the next page, click **Display Token** link.\n\n  ![](images/display-token.png) \n4.\tCopy the OCP login command to the clipboard.\n\n  ![](images/copy-token.png) \n5.\tGo back to the Terminal window, right-click to paste the OCP login command window press Enter to log in to the OCP cluster.\n \n  ![](images/run-oc-login-command.png) \n6.\tCreate a new project (namespace) as **demo**.\n\n  ```\n  oc new-project demo\n  ```\n  You see the message to confirm that the demo project is created.\n\n7.\tGet OCP internal image registry URL and cluster URL with commands:\n\n  ```\n  export INTERNAL_REG_HOST=`oc get route default-route --template='{{ .spec.host }}' -n openshift-image-registry`\n  export CLUSTER_URL=`echo $INTERNAL_REG_HOST | sed 's/default-route-openshift-image-registry.//g'`\n  ```\n8.\tLog in to the OpenShift Docker registry with the command:\n    \n  ```\n  docker login -u $(oc whoami) -p $(oc whoami -t) $INTERNAL_REG_HOST\n  ```\n\n9. Execute the following command to push your docker image to OpenShift image repository.\n\n  ```\n  docker tag modresorts-twas:latest $INTERNAL_REG_HOST/`oc project -q`/modresorts-twas:latest\n\n  docker push $INTERNAL_REG_HOST/`oc project -q`/modresorts-twas:latest\n\n  ```\n\n  When it is done, your application docker image is pushed to the OCP image registry.\n\n10.\tVerify the pushed Docker image in OCP cluster.\n\n    a.\tFrom the OCP Web Console Home page, click **Builds**>**Images Streams**.\n  \n   ![](images/ocp-builds-is.png) \n   \n    b.\tChange project (namespace) from **default** to **demo**.\n   \n   ![](images/ocp-demos-project.png) \n\n    c.\tYou can see the image you just pushed is listed. Click its link to view its details.\n \n   ![](images/ocp-demo-is.png) \n\n    In the Image Stream Details Page Overview section, you see the public image repository you used to push the image.  Notice that the public image repository is mapped to an internal image repository which is used to deploy the application.  The internal image repository is: image-registry.openshift-image-registry.svc:5000/demo/modresorts.\n \n   ![](images/ocp-demo-is-details.png) \n \n#### 6.5\tDeploy the WAS Container to OCP cluster\nOnce the WAS container image is pushed to the OCP cluster image registry, you can deploy the WAS container to the OCP cluster with OpenShift CLI.\n\n1.\tView the deployment YAML files. In this lab you are going to use three YAML files to deploy the WAS container, these files are:\n    *\tdeploy.yaml – create a WAS container in the OpenShift cluster\n    *\tservice.yaml – create a service for the WAS container\n    *\troute.yaml – create a route of the WAS container service\n\n    a.\tFrom the File Manager window, navigate to **/home/ibmdemo/app-mod-labs/am0300/deploy** directory where these three YAML files are located.\n \n    b.\tDouble-click the **deploy.yaml** file to open it in the text editor. The file contains the basic requirements of the WAS container deployment. The container name is set as modresorts-app-twas and it is deployed to the demo namespace.\n \n    ![](images/ocp-twas-deploy-yaml.png)\n    \n    c.\t Double-click the service.yaml file to view its contents. The file exposes the Mod Resorts application ports 9080 and 9443.  The port type in the OpenShift cluster is set as CluerIP. \n\n    ![](images/ocp-twas-service-yaml.png)\n\n    d.\tDouble-click the route.yaml file to open it. The file defined the OpenShift route used for the WAS container which enables users to access the Mod Resorts application on a public address.  You need to change the **host**/**name** from **modresorts-app-twas.apps.demo.ibmdte.net** to your OCP cluster host name and **Save** your change. \n\n      Your OCP cluster hostname is stored in the **CLUSTER_URL** ariable and you can get it with commsnd:\n      \n      ```\n      echo $CLUSTER_URL\n      ```\n      \n    ![](images/ocp-twas-route-yaml.png)\n\n2.\tNow you can deploy the WAS container to OCP cluster with these YAML files. From the Terminal window, change to the /home/ibmdemo/app-mod-labs/am0300 directory.\n\n    ```\n    cd /home/ibmdemo/app-mod-labs/am0300\n    ```\n3.\tIssue OpenShift CLI command to deploy the WAS container to the OpenShift cluster:\n\n    ```\n    oc apply -f deploy -n demo\n    ``` \n    this command deploys the three YAML files in the deploy directory and create WAS container pod, service and route in the demo namespace.\n#### 6.6\tVerify the Deployment\n\nIn this task, you access the OpenShift Web Console to verify the WAS container deployment.\n\n1.\tGo back to the OCP Web Console, click **Workloads**>**Pods**, then select **demo** project from the project list.\n\n    ![](images/ocp-workloads-pods-demo.png)\n2.\tYou see that the WAS container is deployed and is in running status. You can click its name link to go to its overview page where you can see its memory and CPU usage and other detail information.\n\n    ![](images/ocp-twas-demo-pods.png)\n3.\tNow from the OCP cluster Web Console navigation panel, click **Networking**>**Services**. You see the WAS container service. Click the service name to view its details.\n \n    In the Service Details page, you can see the service ports as you defined in the service.yaml file. \n \n    ![](images/ocp-twas-demo-services.png)\n4.\tNext navigate to **Networking**>**Route** to view the WAS container service route. Click the **Location** URL to launch the Mod Resort application.\n \n    ![](images/ocp-twas-demo-routes.png)\n5.\tThe **Mod Resort** application URL is launched in a new web browser window. Type the application context root **/resorts** in the end of the application URL and press **Enter**.\n  \n6.\tClick **Advanced**>**Accept Risk and Continue**, the Mod Resorts application home page is displayed.\n\n7.\tClick **WHERE TO?** to view city list.\n\n8.\tClick **LAS VEGAS, USA** from the list, the weather of the city is displayed.\n\n### 7.\tSummary\n\nIn this lab, you learned the App Modernization rehosting process. You learned how to move an existing traditional WAS application to WAS Base container and how to deploy it to an OpenShift cluster without any code change. \nIBM Cloud Pak for Applications, built on the Red Hat OpenShift Container Platform, provides a long-term solution to help you transition between public, private, and hybrid clouds, and to create new business applications. As a key component of IBM Cloud Pak for Applications, traditional WebSphere application Server can still run in containers, and reap the benefits of consistency and reliability that containers provide.\n\n**Congratulations! You have successfully completed App Modernization using WAS Base Container on OCP Lab!**\n\n","type":"Mdx","contentDigest":"2e245e8c24c45652f823b3db80a7266a","owner":"gatsby-plugin-mdx","counter":2880},"frontmatter":{"title":"App Modernization with Microservice","description":null},"exports":{},"rawBody":"---\ntitle: App Modernization with Microservice\ndescription: \n---\nThis lab exercise is a part of the Application Modernization lab series which focus on the evaluation, re-platforming, rehosting and refactoring application modernization approaches and other application modernization related solutions. In Part 1 of the Application Modernization Journey (Lab AM0100ST), you go through the process to evaluate the existing On-Prem Java applications and to identify the candidate to be moved to the cloud using the IBM Cloud Transformation Advisor. In Part 2 of the Application Modernization Journey (Lab AM0200ST), you learn how to re-platform an existing WebSphere application to a Liberty container and to deploy it to a Red Heat OpenShift Container Platform (OCP) cluster. In Part 3 of the Application Modernization Journey (Lab AM0300ST), you use the rehost process to move an existing WebSphere application to a WebSphere Application Server (WAS) container and to deploy it to a OCP cluster. In this lab, you start to refactor a monolithic Java application by moving one of its functions into a microservice and modifying the application to access the microservice via REST. Over time this pattern of moving services from an application to a microservice will allow for the eventual sunset of the initial monolithic application, once all of its function has been deployed as microservices. \n### 1. Business Scenario\n\nYour company has a traditional WebSphere Application Server application called PlantsByWebSphere, a monolith web application allowing users to search and order garden products online.  The application is running is a traditional WAS environment.\n\n![](images/pbw-app-home-page.png)\n\nAs a tech leader, you are leading the effort to modernize the monolith application using the refactor option. At your first approach, you focus on refactoring the image module of the monolith application and break the monolith application into two parts:\n\n* The original monolith ( **1**  in the image below) has been modified with a Servlet Filter to intercept requests for images and redirect the requests to a new image microservice application, the old image module is disabled in the modified code.  The modified monolith application is still running in the WAS environment.\n* The image module of the application is refactored as a new microservice application as showing in **2** in the image below. The microservice exposes the image service via a JAX-RS interface which accesses the image library and images in the database via JPA reusing the same code that was used in the original monolith application. The new image microservice application is containerized and deployed to a OCP cluster. \n\n![](images/modified-pbw-app.png)\n\nFor those interested, appendix 1 and appendix 2 contain the Servlet Filter and Image Service code.\n### 2. Objective\n\nThe objectives of this lab are to help you:\n\n* get familiar with the application modernization refactor process.\n*\tlearn the process to deploy a refactored microservice to OCP cluster.\n*\tlearn how to configure the modified monolith application to consume the refactored microservice from WAS console.\n\n### 3.\tPrerequisites\n\nThe following prerequisites must be completed prior to beginning this lab:\n* Familiarity with basic Linux commands\n* Have internet access\n*\tHave basic WAS Admin and Java app development knowledge.\n*\tHave completed Lab SU0100 of this lab series and have the CP4Apps installed in IBM Red Hat OpenShift Kubernetes Service (OCP) cluster on IBM Cloud.\n\n### 4.\tWhat is Already Completed\n\nThe Workstation VM is pre-configured for you to access and work with the OCP cluster in this lab.\n\nThe login credentials for the Workstation VM are provided in your Reservation Details page (**https://www.ibm.com/demos/my-reservations/**).\n\nThe CLI commands used in this lab are listed in the Commands.txt file located at the **/home/ibmdemo/app-mod-labs/am0400** directory of the Workstation VM for you to copy and paste these commands to the Terminal window during the lab.\n\n### 5.\tLab Tasks\nDuring this lab, you complete the following tasks:\n*\treview the original and modified versions of PlantsByWebSphere application on local WAS server.\n*\tdeploy the image microservice to a OCP cluster.\n*\ttest and verify the deployed image microservice.\n*\tconfigure the modified PlantsByWebSphere application to consume the image microservice in WAS console.\n*\ttest and verify the modified PlantsByWebSphere.\n\n\n### 6.\tExecute Lab Tasks\n#### 6.1 Log in to the Workstation VM and get started \n1.  Launch the VM from your local workstation web browser with the VM URL.\n \n    ![](images/vm-url.png)\n\n2.\tClick **Connect** to access the VM.\n \n    ![](images/vm-connect.png)\n\n3.\tEnter the password you obtained from the Reservation Details page and click **Send Password**.\n\n    ![](images/vm-password.png)\n\n    The VM desktop is displayed.\n\n    ![](images/vm-desktop.png)\n\n    The Workstation Desktop is displayed. You execute all the lab tasks on this VM.\n\n#### 6.2 Review the on-premises WebSphere application \nIn this task, you take a look at **Mod Resorts** application deployed to the local WebSphere Application Server (WAS) environment. You are going to move this application to the cloud using Open Liberty Operator later.\n\n1.\tStart WebSphere Application Server\n\n    In the workstation VM, you have a local WebSphere Application Server V9 which hosts the Mod Resorts application. \n\n    To start the WAS server:\n\n    a.\tOpen a terminal window by clicking **Applications**>**Terminal**.\n \n    ![](images/vm-terminal-link.png)\n\n    b.\tIn the terminal window, issue the command below to start the WAS server (You can copy and paste the command from the Commands.txt file in the /home/ibmdemo/app-mod-labs/am0200 directory).\n\n    ```\n    /home/ibmdemo/app-mod-labs/shared/startWASV9.sh\n    ```\n    \n    Within a few minutes the WAS server is ready.\n\n    c.\tAccess the WAS Admin Console to view the application deployed by clicking **Applications**>**Firefox** to open a browser window.\n \n    ![](images/vm-browser-link.png)\n\n    d.\tFrom the web browser window, type the WebSphere Integrated Solution Console URL as: https://localhost:9044/ibm/console and press **Enter** to launch the WAS V9 console.\n\n    e.\tIn the WAS Admin Console login page, enter the User ID and Password as: **wsadmin**/**passw0rd** and click **Login**. \n\n    f.\tOn the WAS Console page, click **Applications** -> **Application Types** -> **WebSphere enterprise applications** to view the apps deployed.\n \n    ![](images/was-enterprise-apps-link.png)\n\n    In the Enterprise Applications list, you can see two versions of the **PlantsByWebSphere** applications deployed, the original and the modified. Due to the port conflict, both versions of the application cannot run at the same time. Currently the original version is running. \n    \n    ![](images/was-enterprise-apps.png)\n\n2.\tView the original **PlantsByWebSphere** application\n\n    a. From the web browser window, click new Tab to open a new browser window with the **PlantsByWebSphere** application URL: http://localhost:9081/PlantsByWebSphere.\n    \n    The **PlantsByWebSphere** application home page is displayed.\n\n    ![](images/pbw-app-home-page-2.png)\n\n    This is a typical online shopping application, you can click the Flowers, Fruits & Vegetables, Trees and Accessories tabs to view the product catalogs. \n    \n    ![](images/pbw-catalog-page.png)\n\n3. View the modified **PlantsByWebSphere** application.\n\n\n  a.\tGo back to WAS console windows and navigate to **Applications** > **Application Types** > **WebSphere enterprise applications** page.\n\n__b.\tStop the original PlantsByWebSphere application by checking the box next to PlantsByWebSphereV90 and clicking Stop.\n \n__c.\tStart the modified version of the application by checking the box next to PlantsByWebSphereV90V2 and clicking Start.\n \n__d.\tClick the refresh icon next to the Application Status.\n \nYou see that the modified application is running now\n \n__e.\tGo to the PlantsByWebSphere application window to relaunch it with URL: http://localhost:9081/PlantsByWebSphere.  You see that its home page looks below without any images.  This is because the original image molder is disable and the new image microservice the application is looking for is not available yet.\n \nClick **WHERE TO?** dropdown menu to see the city list.\n\n    ![](images/mod-resorts-app-where-to.png)\n\n    c. Click **PARIS, FRANCE** from the list, it shows the weather of the city.\n\n    ![](images/mod-resorts-app-paris.png)\n\n    You have reviewed the application.  Next you learn how to re-platform the application in WAS container without any code change and to deploy it to an OpenShift cluster.\n\n    d. Stop the WAS server by issuing the following command in the Terminal window:\n\n    ```\n    /home/ibmdemo/IBM/WebSphere/AppServer/profiles/AppSrv01/bin/stopServer.sh server1\n    ```   \n\n#### 6.3\tBuild a WAS Base Server Container Image.\nIn this task, you are going to build a WAS Base server Docker container image with the Mod Resorts application installed. \n\nNote: according to Docker's best practices you should create a new WAS Base image which adds a single application and the corresponding configuration. You should avoid configuring the image manually (after it started) via Admin Console or wsadmin unless it is for debugging purposes, because such changes won't be present if you spawn a new container from the image.\n\nThere are five key files you needed to build your WAS Base server container image:\n*\tDockerfile – the file defines how the Docker image that has your application and configuration pre-loaded is built.\n*\tApp runtime – the ear or war file of your application.\n*\tapp-install.props – the file defines how to install your application in the WAS server.\n*\tappConfig.py – the WAS admin script in Jython format configures the WAS server for your application.\n*\tPASSWORD – the file contains the WAS server console password use in the WAS container.\n\n1.\tReview Dockerfile, app-install.props file, and appConfig.py file.\n\n    a.\tClick **Applications**>**Files** to open the File Manager.\n\n    ![](images/vm-files-link.png)\n  \n    b.\tNavigate to **/home/ibmdemo/app-mod-labs/am0300** directory.\n \n    c.\tDouble click the **Dockerfile** to open it in Text editor for reviewing.\n \n    ![](images/ocp-twas-dockerfile.png)\n    As you can see, the Dockerfile file defines the following activities to create a WAS Base container image:\n    *\tGet the base WAS image from Docker Hub\n    *\tAdd WAS admin console password to the WAS Base image\n    *\tAdd the WAS server configuration script to the WAS base image\n    *\tAdd the application installation script to the WAS base image\n    *\tAdd the application runtime file to the WAS base image\n    *\tRun the configuration script to config the WAS server instance inside the container to configure the WAS server and to install the application\n    \n    d.\tGo back to File Manager and double click **app-install.props** file to review it. This is the script file you use to install the Mod Resorts application to the WAS server instance. \n    \n    ![](images/ocp-twas-app-install-props.png)\n    \n    e.\tIn the File Manager window, double click **appConfig.py** file to review its contents. This is a standard WAS admin script with Jython format for configuring the WAS to run the Mod Resorts application. This is the WAS admin script file you use to convert your application settings, include JDBC and JMS resources, from a WAS ND cell to a WAS Base container. \n \n    ![](images/pco-twas-appconfig-py.png)\n\n2.\tBuild the WAS Base server container image.\n\n    a. Go back to the Terminal window and navigate to the **/home/ibmdemo/app-mod-labs/am0300** directory with command:\n    \n    ```\n    cd /home/ibmdemo/app-mod-labs/am0300 \n    ```\n    \n    b. Execute the following command to build the WAS Base docker container image with the Dockerfile you just reviewed:\t\n\n    ```\n    docker build . -t modresorts-twas:latest\n    ```\n\n    This creates a WAS Base docker image called modresorts-twas:latest.\n\n    c. After the docker container image is created, you can issue the command below to check it:\n\n    ```\n    docker images |grep twas\n    ```\n    \n    You see the docker image is created.\n\n    d. To verify the docker image, you can create and run a local container called modresorts-twas with the command below:\n\n    ```\n    docker run --name modsorts-twas -p 9443:9443 -d modresorts-twas:latest\n    ```\n\n    e. once the container is created, you can test the Moderesorts application by launching it in a web browser window with the URL: **https://localhost:9443/resorts**.\n\n#### 6.4\tPush the WAS Container Image to OCP Image Registry\nAfter the WAS container image is built, you need to push it to an image registry first. In this lab, you are using the OCP Image Registry to host your WAS container image.\n\n1.\tFrom the browser window, click OCP cluster Web Console bookmark to open it with your IBM credentials.\n\n  ![](images/ocp-console-bookmarked.png) \n2.\tIn the OpenShift Web Console page, click the **Action** icon next to your username and select **Copy Login** Command to get the OCP login command.\n\n  ![](images/ocp-copy-login-command.png) \n3.\tIn the next page, click **Display Token** link.\n\n  ![](images/display-token.png) \n4.\tCopy the OCP login command to the clipboard.\n\n  ![](images/copy-token.png) \n5.\tGo back to the Terminal window, right-click to paste the OCP login command window press Enter to log in to the OCP cluster.\n \n  ![](images/run-oc-login-command.png) \n6.\tCreate a new project (namespace) as **demo**.\n\n  ```\n  oc new-project demo\n  ```\n  You see the message to confirm that the demo project is created.\n\n7.\tGet OCP internal image registry URL and cluster URL with commands:\n\n  ```\n  export INTERNAL_REG_HOST=`oc get route default-route --template='{{ .spec.host }}' -n openshift-image-registry`\n  export CLUSTER_URL=`echo $INTERNAL_REG_HOST | sed 's/default-route-openshift-image-registry.//g'`\n  ```\n8.\tLog in to the OpenShift Docker registry with the command:\n    \n  ```\n  docker login -u $(oc whoami) -p $(oc whoami -t) $INTERNAL_REG_HOST\n  ```\n\n9. Execute the following command to push your docker image to OpenShift image repository.\n\n  ```\n  docker tag modresorts-twas:latest $INTERNAL_REG_HOST/`oc project -q`/modresorts-twas:latest\n\n  docker push $INTERNAL_REG_HOST/`oc project -q`/modresorts-twas:latest\n\n  ```\n\n  When it is done, your application docker image is pushed to the OCP image registry.\n\n10.\tVerify the pushed Docker image in OCP cluster.\n\n    a.\tFrom the OCP Web Console Home page, click **Builds**>**Images Streams**.\n  \n   ![](images/ocp-builds-is.png) \n   \n    b.\tChange project (namespace) from **default** to **demo**.\n   \n   ![](images/ocp-demos-project.png) \n\n    c.\tYou can see the image you just pushed is listed. Click its link to view its details.\n \n   ![](images/ocp-demo-is.png) \n\n    In the Image Stream Details Page Overview section, you see the public image repository you used to push the image.  Notice that the public image repository is mapped to an internal image repository which is used to deploy the application.  The internal image repository is: image-registry.openshift-image-registry.svc:5000/demo/modresorts.\n \n   ![](images/ocp-demo-is-details.png) \n \n#### 6.5\tDeploy the WAS Container to OCP cluster\nOnce the WAS container image is pushed to the OCP cluster image registry, you can deploy the WAS container to the OCP cluster with OpenShift CLI.\n\n1.\tView the deployment YAML files. In this lab you are going to use three YAML files to deploy the WAS container, these files are:\n    *\tdeploy.yaml – create a WAS container in the OpenShift cluster\n    *\tservice.yaml – create a service for the WAS container\n    *\troute.yaml – create a route of the WAS container service\n\n    a.\tFrom the File Manager window, navigate to **/home/ibmdemo/app-mod-labs/am0300/deploy** directory where these three YAML files are located.\n \n    b.\tDouble-click the **deploy.yaml** file to open it in the text editor. The file contains the basic requirements of the WAS container deployment. The container name is set as modresorts-app-twas and it is deployed to the demo namespace.\n \n    ![](images/ocp-twas-deploy-yaml.png)\n    \n    c.\t Double-click the service.yaml file to view its contents. The file exposes the Mod Resorts application ports 9080 and 9443.  The port type in the OpenShift cluster is set as CluerIP. \n\n    ![](images/ocp-twas-service-yaml.png)\n\n    d.\tDouble-click the route.yaml file to open it. The file defined the OpenShift route used for the WAS container which enables users to access the Mod Resorts application on a public address.  You need to change the **host**/**name** from **modresorts-app-twas.apps.demo.ibmdte.net** to your OCP cluster host name and **Save** your change. \n\n      Your OCP cluster hostname is stored in the **CLUSTER_URL** ariable and you can get it with commsnd:\n      \n      ```\n      echo $CLUSTER_URL\n      ```\n      \n    ![](images/ocp-twas-route-yaml.png)\n\n2.\tNow you can deploy the WAS container to OCP cluster with these YAML files. From the Terminal window, change to the /home/ibmdemo/app-mod-labs/am0300 directory.\n\n    ```\n    cd /home/ibmdemo/app-mod-labs/am0300\n    ```\n3.\tIssue OpenShift CLI command to deploy the WAS container to the OpenShift cluster:\n\n    ```\n    oc apply -f deploy -n demo\n    ``` \n    this command deploys the three YAML files in the deploy directory and create WAS container pod, service and route in the demo namespace.\n#### 6.6\tVerify the Deployment\n\nIn this task, you access the OpenShift Web Console to verify the WAS container deployment.\n\n1.\tGo back to the OCP Web Console, click **Workloads**>**Pods**, then select **demo** project from the project list.\n\n    ![](images/ocp-workloads-pods-demo.png)\n2.\tYou see that the WAS container is deployed and is in running status. You can click its name link to go to its overview page where you can see its memory and CPU usage and other detail information.\n\n    ![](images/ocp-twas-demo-pods.png)\n3.\tNow from the OCP cluster Web Console navigation panel, click **Networking**>**Services**. You see the WAS container service. Click the service name to view its details.\n \n    In the Service Details page, you can see the service ports as you defined in the service.yaml file. \n \n    ![](images/ocp-twas-demo-services.png)\n4.\tNext navigate to **Networking**>**Route** to view the WAS container service route. Click the **Location** URL to launch the Mod Resort application.\n \n    ![](images/ocp-twas-demo-routes.png)\n5.\tThe **Mod Resort** application URL is launched in a new web browser window. Type the application context root **/resorts** in the end of the application URL and press **Enter**.\n  \n6.\tClick **Advanced**>**Accept Risk and Continue**, the Mod Resorts application home page is displayed.\n\n7.\tClick **WHERE TO?** to view city list.\n\n8.\tClick **LAS VEGAS, USA** from the list, the weather of the city is displayed.\n\n### 7.\tSummary\n\nIn this lab, you learned the App Modernization rehosting process. You learned how to move an existing traditional WAS application to WAS Base container and how to deploy it to an OpenShift cluster without any code change. \nIBM Cloud Pak for Applications, built on the Red Hat OpenShift Container Platform, provides a long-term solution to help you transition between public, private, and hybrid clouds, and to create new business applications. As a key component of IBM Cloud Pak for Applications, traditional WebSphere application Server can still run in containers, and reap the benefits of consistency and reliability that containers provide.\n\n**Congratulations! You have successfully completed App Modernization using WAS Base Container on OCP Lab!**\n\n","fileAbsolutePath":"/Users/yitang/Downloads/app-mod/src/pages/labs/appmod-labs/am0400-refactor/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}