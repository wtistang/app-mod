{"componentChunkName":"component---src-pages-labs-basic-labs-dk-0200-kubernetes-101-index-mdx","path":"/labs/basic-labs/dk0200-kubernetes101/","result":{"pageContext":{"frontmatter":{"title":"Kubernetes Introduction","description":null},"relativePagePath":"/labs/basic-labs/dk0200-kubernetes101/index.mdx","titleType":"page","MdxNode":{"id":"1ef9e220-1bb6-5053-a0bb-60bb635190dd","children":[],"parent":"fc57dbda-9cb7-5d24-98e3-352c7a9edb86","internal":{"content":"---\ntitle: Kubernetes Introduction\ndescription: \n---\nThis lab is intended primarily for system administrator/infrastructure professionals who manage the Kubernetes cluster. This lab explains Kubernetes principles and the internal working of the cluster.\n### 1.\tWhat is Kubernetes\n\nKubernetes is an open-source platform for building an ecosystem of components and tools to deploy, scale and manage containerized applications. Kubernetes is often referred to as a container orchestration framework. \n\nThe kubectl is a command line tool to communicate with the Kubernetes master node that runs an API server. The API server provides REST API endpoints and kubectl internally uses the REST APIs to communicate with the API server, which communicates with the Kubernetes Objects in the cluster. \n\n### 2. Objective\n\nThe objectives of this lab are to help you:\n\n* get familar with the kubectl commands\n*\tlearn how to push a Docker image to a Kubernetes cluster internal image registry\n* learn how to deploy an application to Kubernetes cluster\n\n### 3.\tPrerequisites\n\nThe following prerequisites must be completed prior to beginning this lab:\n*\tFamiliarity with basic Linux commands\n*\tHave internet access\n*\tHave a SkyTap App Mod Lab environment ready\n\n### 4.\tWhat is Already Completed\n\nA six Linux VMs App Mod environment has been provided for this lab. \n \n  ![](images/lab-vms.png)\n\n*\tThe Red Hat OpenShift Container Platform (OCP) v4.6, is installed in 5 VMs, the master1 VM, the master2 VM, the master3 VM, the dns VM and the nfs VM, with 3 master nodes and 3 compute nodes (the master nodes are serving as computer nodes as well).\n*\tThe Workstation VM is the one you will use to access and work with OCP cluster in this lab.\n  The login credentials for the Workstation VM are:\n  User ID: **ibmdemo**\n  Password: **passw0rd**\n  Note: Use the Password above in the Workstation VM Terminal for sudo in the Lab.\n*\tThe CLI commands used in this lab are listed in the Commands.txt file located at the /home/ibmdemo/add-mod-labs/dk0200st directory of the Workstation VM for you to copy and paste these commands to the Terminal window during the lab.\n\n### 5.\tLab Tasks\nDuring this lab, you complete the following tasks:\n*\treview the files used in the Docker image build\n*\tbuild Docker Images\n*\tcreate Docker containers and test container applications\n\n### 6.\tExecute Lab Tasks\n#### 6.1 Log in to the Workstation VM and get started \n1. If the VMs are not already started, start them by clicking the play button for the whole group.\n\n  ![](images/start-vms.png)\n\n\n2.\tAfter the VMs are started, click the Workstation VM icon to access it. \n\n  ![](images/access-workstation.png)\n\n  The Workstation Linux Desktop is displayed. You execute all the lab tasks on this workstation VM.\n\n3.\tIf requested to log in to the Workstation OS, use credentials: **ibmdemo/passw0rd**\n\n#### 6.2 Access the OCP Cluster\n\nThe Kubernetes command-line tool, **kubectl**, has been installed in the workstation.  It allows you to run commands against Kubernetes clusters, to deploy applications, to inspect, to manage cluster resources, and to view logs. kubectl must be configured for the environment that it can be running against. In this lab you are using the OpenShift Container Platform (OCP) cluster as the Kubernetes cluster. So you need to login into the OCP cluster with the OCP command line tool **oc** which can provide the required configuration\n\n1. Open a terminal window by clicking its icon on the Desktop tool bar.\n \n    ![](images/terminal-icon.png)\n\n2.\tIn the terminal window, issue the following command to navigate to the **/home/ibmdemo/app-mod-labs/dk0200st** directory.\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/dk0200st\n  ```\n3. Issue the **oc login** command to login to the OCP cluster:\n\n  ```\n  oc login https://api.demo.ibmdte.net:6443\n  ```\n\n  when promted, enter the login credientials as: **ibmadmin**/**engageibm**.\n  \n  ```\n  Authentication required for https://api.demo.ibmdte.net:6443 (openshift)\n  Username: ibmadmin\n  Password: \n  Login successful.\n\n  You have access to 66 projects, the list has been suppressed. You can list all projects with ' projects'\n\n  Using project \"default\".\n  ``` \n   \n  **oc** extends **kubectl** and offers the same capabilities as the kubectl but it is further extended to natively support OpenShift Container Platform features, such as OpenShift resources such as DeploymentConfigs, BuildConfigs, Routes, ImageStreams, and ImageStreamTags which are specific to OpenShift distributions, and not available in standard Kubernetes.\n\n  The kubectl command line interface is now configured to communicate with the OCP cluster.\n\n4. Test to verify the **kubectl** is installed.\n\n  ```\n  kubectl version --client\n  ```\n  The output is link this:\n  ```\n  Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0+724e12f93f\", GitCommit:\"4ac9078476ffe9a96e78a24d1b8083ae0283ec27\", GitTreeState:\"clean\", BuildDate:\"2019-12-04T06:58:28Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n  ```\n\n### 6.3 List Kubernetes Nodes\n\n1.\tList all the nodes in the OCP cluster with the command **kubectl get nodes**.\n\n  ```\n  kubectl get nodes\n  ```\n  ```\n  NAME      STATUS   ROLES           AGE    VERSION\n  master1   Ready    master,worker   379d   v1.19.0+43983cd\n  master2   Ready    master,worker   379d   v1.19.0+43983cd\n  master3   Ready    master,worker   379d   v1.19.0+43983cd\n  ```\n  \n  This is a 3 nodes cluster, with each node serves as master and worker node.\n    *\tMaster node(s) – runs four main components of Kubernetes. 1. API Server. 2. Scheduler, 3. Controller Manager and 4. etcd.\n    *\tWorker node(s) – run application workloads. \n\n2.\tYou can also use kubectl describe node  command to get more information about a specific node. In this sample below, the master1 node is used.\n\n  ```\n  kubectl describe node master1\n  ```\n  ```\n  Name:               master1\n  Roles:              master,worker\n  Labels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.openshift.io/os_id=rhcos\n  Annotations:        machineconfiguration.openshift.io/currentConfig: rendered-master-8736bcd3d73605acaf734e00a94680a1\n                    machineconfiguration.openshift.io/desiredConfig: rendered-master-8736bcd3d73605acaf734e00a94680a1\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\n  CreationTimestamp:  Wed, 24 Jun 2020 11:13:56 -0700\n  Taints:             <none>\n  Unschedulable:      false\n  Conditions:\n    Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n    ----             ------  -----------------                 ------------------                ------                       -------\n    MemoryPressure   False   Fri, 09 Jul 2021 08:22:14 -0700   Thu, 01 Jul 2021 13:09:45 -0700   KubeletHasSufficientMemory   kubelet has sufficient memory available\n    DiskPressure     False   Fri, 09 Jul 2021 08:22:14 -0700   Thu, 01 Jul 2021 13:09:45 -0700   KubeletHasNoDiskPressure     kubelet has no disk pressure\n    PIDPressure      False   Fri, 09 Jul 2021 08:22:14 -0700   Thu, 01 Jul 2021 13:09:45 -0700   KubeletHasSufficientPID      kubelet has sufficient PID available\n    Ready            True    Fri, 09 Jul 2021 08:22:14 -0700   Thu, 01 Jul 2021 13:09:45 -0700   KubeletReady                 kubelet is posting ready status\n  Addresses:\n    InternalIP:  10.0.0.111\n    Hostname:    master1\n  Capacity:\n  cpu:                16\n  ephemeral-storage:  156734444Ki\n  hugepages-2Mi:      0\n  memory:             32931496Ki\n  pods:               250\n  Allocatable:\n  cpu:                15500m\n  ephemeral-storage:  143372721528\n  hugepages-2Mi:      0\n  memory:             31780520Ki\n  pods:               250\n  System Info:\n  Machine ID:                                        33a0d7b66d9d4ba1becba58666ddfd2d\n  System UUID:                                       422e8c97-34fa-d310-ac45-adc46e59ba75\n  Boot ID:                                           ea7bc332-a736-422d-a2ca-fc095f5c4843\n  Kernel Version:                                    4.18.0-193.29.1.el8_2.x86_64\n  OS Image:                                          Red Hat Enterprise Linux CoreOS 46.82.202011210620-0 (Ootpa)\n  Operating System:                                  linux\n  Architecture:                                      amd64\n  Container Runtime Version:                         cri-o://1.19.0-25.rhaos4.6.gitf51f94a.el8\n  Kubelet Version:                                   v1.19.0+43983cd\n  Kube-Proxy Version:                                v1.19.0+43983cd\n  PodCIDR:                                            10.128.4.0/24\n  Non-terminated Pods:                                (56 in total)\n    Namespace                                         Name                                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n    ---------                                         ----                                                      ------------  ----------  ---------------  -------------  ---\n    dk0300st                                          example-5fb6876865-8bdhn                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         23h\n    nfs-storage                                       nfs-client-provisioner-7bd5cb954-bznt4                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         23h\n    openshift-apiserver                               apiserver-7b65f5d4b6-mflzg                                110m (0%)     0 (0%)      250Mi (0%)       0 (0%)         41h\n    openshift-authentication-operator                 authentication-operator-7496b88c8b-z4744                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-authentication                          oauth-openshift-557b49c746-dkdbs                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-cloud-credential-operator               cloud-credential-operator-7c4fff9f-7rgzm                  20m (0%)      0 (0%)      170Mi (0%)       0 (0%)         23h\n    openshift-cluster-node-tuning-operator            tuned-bnd6w                                               10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         218d\n    openshift-console                                 console-9d4f5fdbc-x4v8s                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         7d19h\n    openshift-console                                 downloads-756d9458cc-b29q2                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-controller-manager-operator             openshift-controller-manager-operator-78984f4647-528gb    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-controller-manager                      controller-manager-5r8tw                                  100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         7d19h\n    openshift-dns                                     dns-default-2g22m                                         65m (0%)      0 (0%)      110Mi (0%)       512Mi (1%)     218d\n    openshift-etcd-operator                           etcd-operator-7bb5c44558-vl2pc                            10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-etcd                                    etcd-master1                                              430m (2%)     0 (0%)      860Mi (2%)       0 (0%)         218d\n    openshift-etcd                                    etcd-quorum-guard-76769c67b4-wrq4g                        10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         7d19h\n    openshift-image-registry                          image-registry-b6df7b688-hdrb7                            100m (0%)     0 (0%)      256Mi (0%)       0 (0%)         7d19h\n    openshift-image-registry                          node-ca-lv6m8                                             10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         218d\n    openshift-ingress-operator                        ingress-operator-6b4d6848f5-d2hcr                         20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         23h\n    openshift-ingress                                 router-default-9578b6f69-dvxdv                            100m (0%)     0 (0%)      256Mi (0%)       0 (0%)         7d19h\n    openshift-insights                                insights-operator-564d567866-nnmd7                        10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         23h\n    openshift-kube-apiserver                          kube-apiserver-master1                                    340m (2%)     0 (0%)      1224Mi (3%)      0 (0%)         41h\n    openshift-kube-controller-manager-operator        kube-controller-manager-operator-7c748b799c-nfz6m         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-kube-controller-manager                 kube-controller-manager-master1                           100m (0%)     0 (0%)      500Mi (1%)       0 (0%)         218d\n    openshift-kube-scheduler                          openshift-kube-scheduler-master1                          20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         218d\n    openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-78468b7ff-7wxx7    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-kube-storage-version-migrator           migrator-7bfc77895c-8jv2p                                 100m (0%)     0 (0%)      200Mi (0%)       0 (0%)         7d19h\n    openshift-machine-config-operator                 machine-config-controller-787468c654-7824t                20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         23h\n    openshift-machine-config-operator                 machine-config-daemon-vl8g6                               40m (0%)      0 (0%)      100Mi (0%)       0 (0%)         218d\n    openshift-machine-config-operator                 machine-config-server-rjh99                               20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         218d\n    openshift-marketplace                             community-operators-6nlq6                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         146m\n    openshift-marketplace                             marketplace-operator-d8465b958-zd47x                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-marketplace                             redhat-operators-pctbs                                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         20h\n    openshift-monitoring                              alertmanager-main-0                                       8m (0%)       0 (0%)      270Mi (0%)       0 (0%)         7d19h\n    openshift-monitoring                              grafana-76fc5cc67d-r2zn5                                  5m (0%)       0 (0%)      120Mi (0%)       0 (0%)         23h\n    openshift-monitoring                              kube-state-metrics-6f78c8fc59-j5s86                       4m (0%)       0 (0%)      120Mi (0%)       0 (0%)         7d19h\n    openshift-monitoring                              node-exporter-8lzc9                                       9m (0%)       0 (0%)      210Mi (0%)       0 (0%)         218d\n    openshift-monitoring                              openshift-state-metrics-56966dd4bd-gf7rt                  3m (0%)       0 (0%)      190Mi (0%)       0 (0%)         7d19h\n    openshift-monitoring                              prometheus-adapter-66c985cd5f-h9wlw                       1m (0%)       0 (0%)      25Mi (0%)        0 (0%)         7d19h\n    openshift-monitoring                              prometheus-k8s-0                                          75m (0%)      0 (0%)      1194Mi (3%)      0 (0%)         7d19h\n    openshift-monitoring                              thanos-querier-5d966f4cc5-s5jpc                           9m (0%)       0 (0%)      92Mi (0%)        0 (0%)         7d19h\n    openshift-multus                                  multus-85mq4                                              10m (0%)      0 (0%)      150Mi (0%)       0 (0%)         218d\n    openshift-multus                                  multus-admission-controller-p7qr8                         20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         218d\n    openshift-multus                                  network-metrics-daemon-jnqcq                              20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         218d\n    openshift-network-operator                        network-operator-74646b4d69-lrwxq                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         23h\n    openshift-oauth-apiserver                         apiserver-6469975956-ws58h                                150m (0%)     0 (0%)      200Mi (0%)       0 (0%)         7d19h\n    openshift-operator-lifecycle-manager              catalog-operator-6fcf9cf98d-nwtlh                         10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         7d19h\n    openshift-operator-lifecycle-manager              olm-operator-898696f9d-qvf47                              10m (0%)      0 (0%)      160Mi (0%)       0 (0%)         7d19h\n    openshift-operator-lifecycle-manager              packageserver-596f8ddc6c-xvx5c                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-operators                               open-liberty-operator-5bb85ddbd5-2r6sn                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-operators                               openshift-pipelines-operator-7bc6fbddc6-6tdjm             0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-pipelines                               tekton-pipelines-controller-59747dc59c-2t6pg              0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-pipelines                               tekton-triggers-controller-594d8b9fd8-vsgzj               0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-pipelines                               tekton-triggers-webhook-65688c6d48-4kgk5                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-sdn                                     ovs-tvk7z                                                 100m (0%)     0 (0%)      400Mi (1%)       0 (0%)         218d\n    openshift-sdn                                     sdn-controller-zzc8z                                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         218d\n    openshift-sdn                                     sdn-p5d79                                                 110m (0%)     0 (0%)      220Mi (0%)       0 (0%)         218d\n      Allocated resources:\n    (Total limits may be over 100 percent, i.e., overcommitted.)\n    Resource           Requests      Limits\n    --------           --------      ------\n    cpu                2299m (14%)   0 (0%)\n    memory             8632Mi (27%)  512Mi (1%)\n    ephemeral-storage  0 (0%)        0 (0%)\n    Events:\n    Type    Reason                   Age                  From              Message\n    ----    ------                   ----                 ----              -------\n    Normal  Starting                 177m                 kubelet, master1  Starting kubelet.\n    Normal  NodeHasSufficientMemory  177m (x8 over 177m)  kubelet, master1  Node master1 status is now: NodeHasSufficientMemory\n    Normal  NodeHasNoDiskPressure    177m (x8 over 177m)  kubelet, master1  Node master1 status is now: NodeHasNoDiskPressure\n    Normal  NodeHasSufficientPID     177m (x7 over 177m)  kubelet, master1  Node master1 status is now: NodeHasSufficientPID\n    Normal  NodeAllocatableEnforced  177m                 kubelet, master1  Updated Node Allocatable limit across pods\n  ```\n3.\tScroll down to review the output above. You see that this node is running the Linux OS and is a master/worker node. Next, see the information about the operation of node itself.  Following that is the status of the node which indicates that it has sufficient disk and memory, and it is in Ready state. The information about capacity displays next. Next the kernel and OS information for the node. (which is followed by a list of running pods and their resource consumption) \n\n### 6.4\tCheck running pods\n1.\tRun the command **kubectl get pods -n default**. This command lists all pods in the **default** OCP Project (namespace). The **-n** switch is for Project name.  When the switch is not defined, like kubectl get pods, the current Project scope is used.\n \n  ```\n  kubectl get pods -n default\n  ```\n  ```\n  NAME                READY   STATUS      RESTARTS   AGE\n  ibm-toolkit-h9z8n   0/1     Completed   0          3d\n  ```\n2.\tRun the commands **kubectl get pods --all-namespaces** to get the list of running pods in all namespaces. \n \n  ```\n  kubectl get pods --all-namespaces\n  ```\n3.\tReview the function of each OpenShift Container Platform pod.\n \n  * docker-registry\t- Internal image registry \n  * router - Directs service requests to the service endpoints \n  * apiserver - handles all api requests\n  * controller-manager - watches etcd for changes to replication controller objects and then uses the API to enforce the desired state.\n  * master-controllers - Scheduler and Replication Controller, responsible for the placement and maintenance of pods\n  * master-etcd - kubernetes etcd database to hold state of cluster\n  * console, web-console - web console \n  * hawkular-metrics, hawkular-metrics-schema - metrics engine and schema\n  * hawkular-cassandra - Cassandra database for metrics \n  * heapster - scrapes the metrics for CPU, memory and network usage for each node then exports them into Hawkular Metrics\n  * metrics-server - Cluster-wide aggregator of resource usage data\n  * alertmanager-main - manages incoming alerts; this includes silencing, inhibition, aggregation, and sending out notifications through methods such as email, PagerDuty,\n  * cluster-monitoring-operator - watches over the deployed monitoring components and resources, and ensures that they are up to date\n  * grafana - cluster monitoring dashboard interface \n  * kube-state-metrics - converts Kubernetes objects to metrics consumable by Prometheus\n  * node-exporter\t- agent deployed on every node to collect node metrics\n  * prometheus-k8s - Prometheus\n  * node-problem-detector - monitors node health and reportsproblems to the API server\n  * sync -detects configuration map change, updates the node-config.yaml  and restarts the appropriate nodes\n  * ovs, sdn - OpenShift software-defined networking (SDN) which configures an overlay network using Open vSwitch (OVS).\n  * catalog-operator - responsible for resolving and installing ClusterServiceVersions (CSVs) and the Custom Resource Definitions (CRD) specified resources\n  * olm-operator - install, update, and manage the lifecycle of all Operators and their associated services\n4.\tThe fourth component of Kubernetes is the etcd database that holds the state of the cluster. The etcd container runs in a separate pod. Run the following **kubectl get pods --all-namespaces | grep -i etcd** command to review the etcd pods.\n\n  ```\n  kubectl get pods --all-namespaces | grep -i etcd\n  ```\n  ```\n  openshift-etcd-operator                            etcd-operator-7bb5c44558-vl2pc                                    1/1     Running     0          7d19h\n  openshift-etcd                                     etcd-master1                                                      3/3     Running     0          218d\n  openshift-etcd                                     etcd-master2                                                      3/3     Running     0          218d\n  openshift-etcd                                     etcd-master3                                                      3/3     Running     0          218d\n  openshift-etcd                                     etcd-quorum-guard-76769c67b4-5nsq2                                1/1     Running     0          7d19h\n  openshift-etcd                                     etcd-quorum-guard-76769c67b4-hlwsx                                1/1     Running     0          7d19h\n  openshift-etcd                                     etcd-quorum-guard-76769c67b4-wrq4g                                1/1     Running     0          7d19h\n  openshift-etcd                                     revision-pruner-8-master1                                         0/1     Completed   0          7d19h\n  openshift-etcd                                     revision-pruner-8-master2                                         0/1     Completed   0          7d19h\n  openshift-etcd                                     revision-pruner-8-master3                                         0/1     Completed   0          7d19h\n  ```\n### 6.5\tCreate a pod\n1.\tBuild the Docker Image.\n  \n  Note: if you have completed lab **dk0100st** in this environment, you can skip this task.\n  \n  a.\tFrom the terminal window, navigate to /home/ibmdemo/app-mod-labs/dk0100st directory.\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/dk0100st\n  ```\n  b.\tBuild a Liberty docker image named simpleapp using the Dockerfile in the directory by typing command:\n\t\n  ```\n  docker build . -t simpleapp\n  ```\n  Note: the “.” in the command indicates that a Docker image is built using the Dockerfile in the current directory.             \n2.\tCreate an Kubernetes namespace named **dk0200st** with command:\n\n  ```\n  kubectl create namespace dk0200st\n  ```\n  The output is like this:\n  ```\n  namespace/dk0200st created\n  ```\n  Note: You can also use **oc new-project dk0200st** command to create the project/namespace.\n3.\tPush the simpleapp image to the OCP internal image registry.\n  \n  You are going to use the simpleapp image to create the pod, you need to tag and place the image in the OCP image registry first.\n  \n  a.\tChange to the **dk0200st** namespace scope.\n\n  ```\n  oc project dk0200st\n  ```  \n  b. Get OCP internal image registry URL and cluster URL with commands:\n\n  ```\n  export INTERNAL_REG_HOST=`oc get route default-route --template='{{ .spec.host }}' -n openshift-image-registry`\n  export CLUSTER_URL=`echo $INTERNAL_REG_HOST | sed 's/default-route-openshift-image-registry.//g'`\n  ```\n  c.\tLog in to the OpenShift Docker registry with the command:\n    \n  ```\n  docker login -u $(oc whoami) -p $(oc whoami -t) $INTERNAL_REG_HOST\n  ```\n  The output is like this:\n  ```\n  WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n  WARNING! Your password will be stored unencrypted in /home/ibmdemo/.docker/config.json.\n  Configure a credential helper to remove this warning. See\n  https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\n  Login Succeeded\n  ```\n  d. Execute the following command to push your docker image to OpenShift image repository.\n\n  ```\n  docker tag simpleapp:latest $INTERNAL_REG_HOST/dk0200st/simpleapp:latest \n\n  docker push $INTERNAL_REG_HOST/dk0200st/simpleapp:latest \n\n  ```\n\n  When it is done, your application docker image is pushed to the OCP image registry.\n\n4.\tDeploy the application to OCP cluster.\n\n  a.\tNavigate to **/home/ibmdemo/app-mod-labs/dk0200st** directory.\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/dk0200st\n  ```\n  \n  b.\tList the contents of the directory with command:\n\n  ```\n  ls -l\n  ```\n  ```\n  total 16\n  -rw-rw-r-- 1 ibmdemo ibmdemo 946 Jul  7 14:50 Commands.txt\n  -rw-rw-r-- 1 ibmdemo ibmdemo 316 Jul  7 17:41 deployment.yaml\n  -rw-rw-r-- 1 ibmdemo ibmdemo 344 Jul  7 17:48 route.yaml\n  -rw-rw-r-- 1 ibmdemo ibmdemo 205 Jul  7 06:00 service.yaml\n  ```\n  There are three YAML files you are going to use to deploy the application to the OCP cluster, including deployment.yaml, service.yaml and route.yaml. The YAML manifest files are used extensively in Kubernetes.\n  \n  c.\tRun **cat deployment.yaml** to review the manifest file. \n\n  ```\n  cat deployment.yaml\n  ```\n \n  This is a basic YAML file to create a Pod which uses Docker image simpleapp and names the Pod simpleapp. \n  ```\n  # A simple yaml file to create a ghost pod\n  apiVersion: v1\n  kind: Pod\n  metadata:\n    name: simpleapp\n    labels:\n      app: simpleapp\n  spec:\n    containers:\n      - image: image-registry.openshift-image-registry.svc:5000/dk0200/simpleapp:latest\n        name: simpleapp\n        ports:\n        - containerPort: 9080\n          protocol: TCP\n  ```\n  A basic understanding of the organization of the manifest file is needed to create resources in Kubernetes. Notice that the kube01.yaml manifest file has the following structure:\n    *\tDefine which Kubernetes API is to be used. The name is **apiVersion**.\n    *\tThe possible values of **apiVersion** are v1, apps/v1, v1beta1, v1beta2, batch/v1, extension/v1beta1 and several others. Refer to API documentation at http://kubernetes.io.\n    *\tThe second name-value pair is **kind**, which can be Pod, PodList, Service, Deployment, DeamonSet, ReplicaSet, Job and many others.\n    *\tThe third name-value pair is **metadata**, which describes information such as name, annotations, labels, namespace etc.\n    * The fourth name-value pair is **spec**, which defines containers, their name, image name, and the commands to run with start-up options.\n\n  d. Run **kubectl apply -f deployment.yaml** to deploy the application and create the Pod.\n\n  ```\n  kubectl apply -f deployment.yaml\n  ```\n\n  The pod is created.\n\n  ```\n  pod/simpleapp created\n  ```\n\n  e.\tRun **kubectl get pods** to check the status of the pod.\n \n  ```\n  kubectl get pods\n  ```\n  ```\n  NAME        READY   STATUS    RESTARTS   AGE\n  simpleapp   1/1     Running   0          8m47s\n  ```\n  f.\tCheck the pod again with command **kubectl get pods -o wide**.\n  \n  ```\n  kubectl get pods -o wide\n  ```\n\n  ```\n  NAME        READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES\n  simpleapp   1/1     Running   0          11m   10.129.0.38   master2   <none>           <none>\n  ```\n  Notice that the pod is created, it is deployed to the **master2** node in this example, but it could be deployed on any of the nodes in the cluster. \n  \n### 6.7\tCreate a Service\n\nThe application is deployed to OCP cluster, it is isolated in the IP address assigned to a pod on a host, and there was no connection between outside world and the application residing in the pod.\nNext, you create a service to route the traffic to the application running inside a pod.\nThe service should be able to find the pod where application is running. It does this by using labels.\n\n1.\tReview **service.yaml** to create the service. \n\n  ```\n  cat service.yaml\n  ```\n\n  ```\n  # A simple yaml file to create a service\n  apiVersion: v1\n  kind: Service\n  metadata:\n    labels:\n      app: simpleapp\n    name: simpleapp\n  spec:\n    selector:\n      app: simpleapp\n    ports:\n    - port: 9080\n    type: NodePort\n  ```\n  Note that the service simpleapp is assigned through a label to app: **simpleapp** and port 9080 is automatically assigned on the host using type: **NodePort**.\n\n2.\tCreate the service using **kubectl apply -f service.yaml** then check the service viausing **kubectl get svc** (svc is shorthand for service) command.\n\n  ```\n  kubectl apply -f service.yaml\n  ```\n  ```\n  kubectl get svc\n  ```\n  \n  ```\n  NAME        TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\n  simpleapp   NodePort   172.30.169.2   <none>        9080:30650/TCP   12s\n  ```\n  \n  Note the **nodeport** is 30650 in the sample output, as it will be different in your case.\n\n3.\tOpen a web browser window by clicking its icon on the Desktop tool bar.\n \n    ![](images/firefox-icon.png)\n    \n4. Enter the URL **http<span></span>://master1.demo.ibmdte.net:NodePort/Simple** and substitute the **NodePort** with the number from the **kubectl get svc** command. \n \n  The application page is displayed.\n  \n  ![](images/kubectl-nodeport-url.png)\n\n  Since you have an environment with multiple nodes, the proxy server would be running on all nodes, so you just use any of the hosts, in this case **master1** node, to reach to the proper host pod. \n \t\n### 6.8\tCreate a Route\n\nThe serveice you created exposes the application with a unique IP address and port, allows internal communication to expose the application. There was no connection between outside world and the application residing in the pod.\n\nNext, you create a route to make the service reachable from outside the cluster. Kubernetes routers provide external host name mapping and load balancing to services in the Kunernetes cluster, it is isolated in the IP address assigned to a pod on a host.\n\n1.\tReview **route.yaml** to create the service. \n\n  ```\n  cat route.yaml\n  ```\n\n  ```\n  # A simple yaml file to create a route\n  kind: Route\n  apiVersion: route.openshift.io/v1\n  metadata:\n    name: simpleapp\n    namespace: dk0200st\n    labels:\n      app: simpleapp\n  spec:\n    host: simpleapp-dk0200st.apps.demo.ibmdte.net\n    path: /Simple\n    to:\n      kind: Service\n      name: simpleapp\n      weight: 100\n    port:\n      targetPort: 9080\n    wildcardPolicy: None\n  ```\n  Note that the service **simpleapp** and its port **9080** are assigned to the route.\n  \n2.\tCreate the route using **kubectl apply -f route.yaml** then check the route via **kubectl get route** command. \n\n  ```\n  kubectl apply -f route.yaml\n  ```\n\n  ```\n  kubectl get route\n  ```\n  \n  ```\n  # A simple yaml file to create a route\n  kind: Route\n  apiVersion: route.openshift.io/v1\n  metadata:\n    name: simpleapp\n    namespace: dk0200st\n    labels:\n      app: simpleapp\n  spec:\n    host: simpleapp-dk0200st.apps.demo.ibmdte.net\n    path: /Simple\n    to:\n      kind: Service\n      name: simpleapp\n      weight: 100\n    port:\n      targetPort: 9080\n    wildcardPolicy: None\n  ```\n  You can see your route is created and its URL is **simpleapp-dk0200st.apps.demo.ibmdte.net/Simple**.\n\n3.\tSwitch to the browser and try the URL **http<span></span>://simpleapp-dk0200st.apps.demo.ibmdte.net/Simple**.\n\n  Your application is displayed.\n  \n  ![](images/kubectl-route-url.png)\n\n\n\n### 7.\tSummary\n\nIn this lab, you have learned how to run kubectl commands and how to deploy an application to a Kubernetes cluster using the kubectl commands and YAML files. To learn more about App Mod, please continue with the rest of the lab series.\n\n**Congratulations! You have successfully completed Kubernetes Introduction Lab!**\n\n","type":"Mdx","contentDigest":"72e68029700461607635fa4afc250d59","owner":"gatsby-plugin-mdx","counter":1727},"frontmatter":{"title":"Kubernetes Introduction","description":null},"exports":{},"rawBody":"---\ntitle: Kubernetes Introduction\ndescription: \n---\nThis lab is intended primarily for system administrator/infrastructure professionals who manage the Kubernetes cluster. This lab explains Kubernetes principles and the internal working of the cluster.\n### 1.\tWhat is Kubernetes\n\nKubernetes is an open-source platform for building an ecosystem of components and tools to deploy, scale and manage containerized applications. Kubernetes is often referred to as a container orchestration framework. \n\nThe kubectl is a command line tool to communicate with the Kubernetes master node that runs an API server. The API server provides REST API endpoints and kubectl internally uses the REST APIs to communicate with the API server, which communicates with the Kubernetes Objects in the cluster. \n\n### 2. Objective\n\nThe objectives of this lab are to help you:\n\n* get familar with the kubectl commands\n*\tlearn how to push a Docker image to a Kubernetes cluster internal image registry\n* learn how to deploy an application to Kubernetes cluster\n\n### 3.\tPrerequisites\n\nThe following prerequisites must be completed prior to beginning this lab:\n*\tFamiliarity with basic Linux commands\n*\tHave internet access\n*\tHave a SkyTap App Mod Lab environment ready\n\n### 4.\tWhat is Already Completed\n\nA six Linux VMs App Mod environment has been provided for this lab. \n \n  ![](images/lab-vms.png)\n\n*\tThe Red Hat OpenShift Container Platform (OCP) v4.6, is installed in 5 VMs, the master1 VM, the master2 VM, the master3 VM, the dns VM and the nfs VM, with 3 master nodes and 3 compute nodes (the master nodes are serving as computer nodes as well).\n*\tThe Workstation VM is the one you will use to access and work with OCP cluster in this lab.\n  The login credentials for the Workstation VM are:\n  User ID: **ibmdemo**\n  Password: **passw0rd**\n  Note: Use the Password above in the Workstation VM Terminal for sudo in the Lab.\n*\tThe CLI commands used in this lab are listed in the Commands.txt file located at the /home/ibmdemo/add-mod-labs/dk0200st directory of the Workstation VM for you to copy and paste these commands to the Terminal window during the lab.\n\n### 5.\tLab Tasks\nDuring this lab, you complete the following tasks:\n*\treview the files used in the Docker image build\n*\tbuild Docker Images\n*\tcreate Docker containers and test container applications\n\n### 6.\tExecute Lab Tasks\n#### 6.1 Log in to the Workstation VM and get started \n1. If the VMs are not already started, start them by clicking the play button for the whole group.\n\n  ![](images/start-vms.png)\n\n\n2.\tAfter the VMs are started, click the Workstation VM icon to access it. \n\n  ![](images/access-workstation.png)\n\n  The Workstation Linux Desktop is displayed. You execute all the lab tasks on this workstation VM.\n\n3.\tIf requested to log in to the Workstation OS, use credentials: **ibmdemo/passw0rd**\n\n#### 6.2 Access the OCP Cluster\n\nThe Kubernetes command-line tool, **kubectl**, has been installed in the workstation.  It allows you to run commands against Kubernetes clusters, to deploy applications, to inspect, to manage cluster resources, and to view logs. kubectl must be configured for the environment that it can be running against. In this lab you are using the OpenShift Container Platform (OCP) cluster as the Kubernetes cluster. So you need to login into the OCP cluster with the OCP command line tool **oc** which can provide the required configuration\n\n1. Open a terminal window by clicking its icon on the Desktop tool bar.\n \n    ![](images/terminal-icon.png)\n\n2.\tIn the terminal window, issue the following command to navigate to the **/home/ibmdemo/app-mod-labs/dk0200st** directory.\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/dk0200st\n  ```\n3. Issue the **oc login** command to login to the OCP cluster:\n\n  ```\n  oc login https://api.demo.ibmdte.net:6443\n  ```\n\n  when promted, enter the login credientials as: **ibmadmin**/**engageibm**.\n  \n  ```\n  Authentication required for https://api.demo.ibmdte.net:6443 (openshift)\n  Username: ibmadmin\n  Password: \n  Login successful.\n\n  You have access to 66 projects, the list has been suppressed. You can list all projects with ' projects'\n\n  Using project \"default\".\n  ``` \n   \n  **oc** extends **kubectl** and offers the same capabilities as the kubectl but it is further extended to natively support OpenShift Container Platform features, such as OpenShift resources such as DeploymentConfigs, BuildConfigs, Routes, ImageStreams, and ImageStreamTags which are specific to OpenShift distributions, and not available in standard Kubernetes.\n\n  The kubectl command line interface is now configured to communicate with the OCP cluster.\n\n4. Test to verify the **kubectl** is installed.\n\n  ```\n  kubectl version --client\n  ```\n  The output is link this:\n  ```\n  Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0+724e12f93f\", GitCommit:\"4ac9078476ffe9a96e78a24d1b8083ae0283ec27\", GitTreeState:\"clean\", BuildDate:\"2019-12-04T06:58:28Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n  ```\n\n### 6.3 List Kubernetes Nodes\n\n1.\tList all the nodes in the OCP cluster with the command **kubectl get nodes**.\n\n  ```\n  kubectl get nodes\n  ```\n  ```\n  NAME      STATUS   ROLES           AGE    VERSION\n  master1   Ready    master,worker   379d   v1.19.0+43983cd\n  master2   Ready    master,worker   379d   v1.19.0+43983cd\n  master3   Ready    master,worker   379d   v1.19.0+43983cd\n  ```\n  \n  This is a 3 nodes cluster, with each node serves as master and worker node.\n    *\tMaster node(s) – runs four main components of Kubernetes. 1. API Server. 2. Scheduler, 3. Controller Manager and 4. etcd.\n    *\tWorker node(s) – run application workloads. \n\n2.\tYou can also use kubectl describe node  command to get more information about a specific node. In this sample below, the master1 node is used.\n\n  ```\n  kubectl describe node master1\n  ```\n  ```\n  Name:               master1\n  Roles:              master,worker\n  Labels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.openshift.io/os_id=rhcos\n  Annotations:        machineconfiguration.openshift.io/currentConfig: rendered-master-8736bcd3d73605acaf734e00a94680a1\n                    machineconfiguration.openshift.io/desiredConfig: rendered-master-8736bcd3d73605acaf734e00a94680a1\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\n  CreationTimestamp:  Wed, 24 Jun 2020 11:13:56 -0700\n  Taints:             <none>\n  Unschedulable:      false\n  Conditions:\n    Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n    ----             ------  -----------------                 ------------------                ------                       -------\n    MemoryPressure   False   Fri, 09 Jul 2021 08:22:14 -0700   Thu, 01 Jul 2021 13:09:45 -0700   KubeletHasSufficientMemory   kubelet has sufficient memory available\n    DiskPressure     False   Fri, 09 Jul 2021 08:22:14 -0700   Thu, 01 Jul 2021 13:09:45 -0700   KubeletHasNoDiskPressure     kubelet has no disk pressure\n    PIDPressure      False   Fri, 09 Jul 2021 08:22:14 -0700   Thu, 01 Jul 2021 13:09:45 -0700   KubeletHasSufficientPID      kubelet has sufficient PID available\n    Ready            True    Fri, 09 Jul 2021 08:22:14 -0700   Thu, 01 Jul 2021 13:09:45 -0700   KubeletReady                 kubelet is posting ready status\n  Addresses:\n    InternalIP:  10.0.0.111\n    Hostname:    master1\n  Capacity:\n  cpu:                16\n  ephemeral-storage:  156734444Ki\n  hugepages-2Mi:      0\n  memory:             32931496Ki\n  pods:               250\n  Allocatable:\n  cpu:                15500m\n  ephemeral-storage:  143372721528\n  hugepages-2Mi:      0\n  memory:             31780520Ki\n  pods:               250\n  System Info:\n  Machine ID:                                        33a0d7b66d9d4ba1becba58666ddfd2d\n  System UUID:                                       422e8c97-34fa-d310-ac45-adc46e59ba75\n  Boot ID:                                           ea7bc332-a736-422d-a2ca-fc095f5c4843\n  Kernel Version:                                    4.18.0-193.29.1.el8_2.x86_64\n  OS Image:                                          Red Hat Enterprise Linux CoreOS 46.82.202011210620-0 (Ootpa)\n  Operating System:                                  linux\n  Architecture:                                      amd64\n  Container Runtime Version:                         cri-o://1.19.0-25.rhaos4.6.gitf51f94a.el8\n  Kubelet Version:                                   v1.19.0+43983cd\n  Kube-Proxy Version:                                v1.19.0+43983cd\n  PodCIDR:                                            10.128.4.0/24\n  Non-terminated Pods:                                (56 in total)\n    Namespace                                         Name                                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n    ---------                                         ----                                                      ------------  ----------  ---------------  -------------  ---\n    dk0300st                                          example-5fb6876865-8bdhn                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         23h\n    nfs-storage                                       nfs-client-provisioner-7bd5cb954-bznt4                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         23h\n    openshift-apiserver                               apiserver-7b65f5d4b6-mflzg                                110m (0%)     0 (0%)      250Mi (0%)       0 (0%)         41h\n    openshift-authentication-operator                 authentication-operator-7496b88c8b-z4744                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-authentication                          oauth-openshift-557b49c746-dkdbs                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-cloud-credential-operator               cloud-credential-operator-7c4fff9f-7rgzm                  20m (0%)      0 (0%)      170Mi (0%)       0 (0%)         23h\n    openshift-cluster-node-tuning-operator            tuned-bnd6w                                               10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         218d\n    openshift-console                                 console-9d4f5fdbc-x4v8s                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         7d19h\n    openshift-console                                 downloads-756d9458cc-b29q2                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-controller-manager-operator             openshift-controller-manager-operator-78984f4647-528gb    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-controller-manager                      controller-manager-5r8tw                                  100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         7d19h\n    openshift-dns                                     dns-default-2g22m                                         65m (0%)      0 (0%)      110Mi (0%)       512Mi (1%)     218d\n    openshift-etcd-operator                           etcd-operator-7bb5c44558-vl2pc                            10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-etcd                                    etcd-master1                                              430m (2%)     0 (0%)      860Mi (2%)       0 (0%)         218d\n    openshift-etcd                                    etcd-quorum-guard-76769c67b4-wrq4g                        10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         7d19h\n    openshift-image-registry                          image-registry-b6df7b688-hdrb7                            100m (0%)     0 (0%)      256Mi (0%)       0 (0%)         7d19h\n    openshift-image-registry                          node-ca-lv6m8                                             10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         218d\n    openshift-ingress-operator                        ingress-operator-6b4d6848f5-d2hcr                         20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         23h\n    openshift-ingress                                 router-default-9578b6f69-dvxdv                            100m (0%)     0 (0%)      256Mi (0%)       0 (0%)         7d19h\n    openshift-insights                                insights-operator-564d567866-nnmd7                        10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         23h\n    openshift-kube-apiserver                          kube-apiserver-master1                                    340m (2%)     0 (0%)      1224Mi (3%)      0 (0%)         41h\n    openshift-kube-controller-manager-operator        kube-controller-manager-operator-7c748b799c-nfz6m         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-kube-controller-manager                 kube-controller-manager-master1                           100m (0%)     0 (0%)      500Mi (1%)       0 (0%)         218d\n    openshift-kube-scheduler                          openshift-kube-scheduler-master1                          20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         218d\n    openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-78468b7ff-7wxx7    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-kube-storage-version-migrator           migrator-7bfc77895c-8jv2p                                 100m (0%)     0 (0%)      200Mi (0%)       0 (0%)         7d19h\n    openshift-machine-config-operator                 machine-config-controller-787468c654-7824t                20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         23h\n    openshift-machine-config-operator                 machine-config-daemon-vl8g6                               40m (0%)      0 (0%)      100Mi (0%)       0 (0%)         218d\n    openshift-machine-config-operator                 machine-config-server-rjh99                               20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         218d\n    openshift-marketplace                             community-operators-6nlq6                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         146m\n    openshift-marketplace                             marketplace-operator-d8465b958-zd47x                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         7d19h\n    openshift-marketplace                             redhat-operators-pctbs                                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         20h\n    openshift-monitoring                              alertmanager-main-0                                       8m (0%)       0 (0%)      270Mi (0%)       0 (0%)         7d19h\n    openshift-monitoring                              grafana-76fc5cc67d-r2zn5                                  5m (0%)       0 (0%)      120Mi (0%)       0 (0%)         23h\n    openshift-monitoring                              kube-state-metrics-6f78c8fc59-j5s86                       4m (0%)       0 (0%)      120Mi (0%)       0 (0%)         7d19h\n    openshift-monitoring                              node-exporter-8lzc9                                       9m (0%)       0 (0%)      210Mi (0%)       0 (0%)         218d\n    openshift-monitoring                              openshift-state-metrics-56966dd4bd-gf7rt                  3m (0%)       0 (0%)      190Mi (0%)       0 (0%)         7d19h\n    openshift-monitoring                              prometheus-adapter-66c985cd5f-h9wlw                       1m (0%)       0 (0%)      25Mi (0%)        0 (0%)         7d19h\n    openshift-monitoring                              prometheus-k8s-0                                          75m (0%)      0 (0%)      1194Mi (3%)      0 (0%)         7d19h\n    openshift-monitoring                              thanos-querier-5d966f4cc5-s5jpc                           9m (0%)       0 (0%)      92Mi (0%)        0 (0%)         7d19h\n    openshift-multus                                  multus-85mq4                                              10m (0%)      0 (0%)      150Mi (0%)       0 (0%)         218d\n    openshift-multus                                  multus-admission-controller-p7qr8                         20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         218d\n    openshift-multus                                  network-metrics-daemon-jnqcq                              20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         218d\n    openshift-network-operator                        network-operator-74646b4d69-lrwxq                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         23h\n    openshift-oauth-apiserver                         apiserver-6469975956-ws58h                                150m (0%)     0 (0%)      200Mi (0%)       0 (0%)         7d19h\n    openshift-operator-lifecycle-manager              catalog-operator-6fcf9cf98d-nwtlh                         10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         7d19h\n    openshift-operator-lifecycle-manager              olm-operator-898696f9d-qvf47                              10m (0%)      0 (0%)      160Mi (0%)       0 (0%)         7d19h\n    openshift-operator-lifecycle-manager              packageserver-596f8ddc6c-xvx5c                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-operators                               open-liberty-operator-5bb85ddbd5-2r6sn                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-operators                               openshift-pipelines-operator-7bc6fbddc6-6tdjm             0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-pipelines                               tekton-pipelines-controller-59747dc59c-2t6pg              0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-pipelines                               tekton-triggers-controller-594d8b9fd8-vsgzj               0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-pipelines                               tekton-triggers-webhook-65688c6d48-4kgk5                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d19h\n    openshift-sdn                                     ovs-tvk7z                                                 100m (0%)     0 (0%)      400Mi (1%)       0 (0%)         218d\n    openshift-sdn                                     sdn-controller-zzc8z                                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         218d\n    openshift-sdn                                     sdn-p5d79                                                 110m (0%)     0 (0%)      220Mi (0%)       0 (0%)         218d\n      Allocated resources:\n    (Total limits may be over 100 percent, i.e., overcommitted.)\n    Resource           Requests      Limits\n    --------           --------      ------\n    cpu                2299m (14%)   0 (0%)\n    memory             8632Mi (27%)  512Mi (1%)\n    ephemeral-storage  0 (0%)        0 (0%)\n    Events:\n    Type    Reason                   Age                  From              Message\n    ----    ------                   ----                 ----              -------\n    Normal  Starting                 177m                 kubelet, master1  Starting kubelet.\n    Normal  NodeHasSufficientMemory  177m (x8 over 177m)  kubelet, master1  Node master1 status is now: NodeHasSufficientMemory\n    Normal  NodeHasNoDiskPressure    177m (x8 over 177m)  kubelet, master1  Node master1 status is now: NodeHasNoDiskPressure\n    Normal  NodeHasSufficientPID     177m (x7 over 177m)  kubelet, master1  Node master1 status is now: NodeHasSufficientPID\n    Normal  NodeAllocatableEnforced  177m                 kubelet, master1  Updated Node Allocatable limit across pods\n  ```\n3.\tScroll down to review the output above. You see that this node is running the Linux OS and is a master/worker node. Next, see the information about the operation of node itself.  Following that is the status of the node which indicates that it has sufficient disk and memory, and it is in Ready state. The information about capacity displays next. Next the kernel and OS information for the node. (which is followed by a list of running pods and their resource consumption) \n\n### 6.4\tCheck running pods\n1.\tRun the command **kubectl get pods -n default**. This command lists all pods in the **default** OCP Project (namespace). The **-n** switch is for Project name.  When the switch is not defined, like kubectl get pods, the current Project scope is used.\n \n  ```\n  kubectl get pods -n default\n  ```\n  ```\n  NAME                READY   STATUS      RESTARTS   AGE\n  ibm-toolkit-h9z8n   0/1     Completed   0          3d\n  ```\n2.\tRun the commands **kubectl get pods --all-namespaces** to get the list of running pods in all namespaces. \n \n  ```\n  kubectl get pods --all-namespaces\n  ```\n3.\tReview the function of each OpenShift Container Platform pod.\n \n  * docker-registry\t- Internal image registry \n  * router - Directs service requests to the service endpoints \n  * apiserver - handles all api requests\n  * controller-manager - watches etcd for changes to replication controller objects and then uses the API to enforce the desired state.\n  * master-controllers - Scheduler and Replication Controller, responsible for the placement and maintenance of pods\n  * master-etcd - kubernetes etcd database to hold state of cluster\n  * console, web-console - web console \n  * hawkular-metrics, hawkular-metrics-schema - metrics engine and schema\n  * hawkular-cassandra - Cassandra database for metrics \n  * heapster - scrapes the metrics for CPU, memory and network usage for each node then exports them into Hawkular Metrics\n  * metrics-server - Cluster-wide aggregator of resource usage data\n  * alertmanager-main - manages incoming alerts; this includes silencing, inhibition, aggregation, and sending out notifications through methods such as email, PagerDuty,\n  * cluster-monitoring-operator - watches over the deployed monitoring components and resources, and ensures that they are up to date\n  * grafana - cluster monitoring dashboard interface \n  * kube-state-metrics - converts Kubernetes objects to metrics consumable by Prometheus\n  * node-exporter\t- agent deployed on every node to collect node metrics\n  * prometheus-k8s - Prometheus\n  * node-problem-detector - monitors node health and reportsproblems to the API server\n  * sync -detects configuration map change, updates the node-config.yaml  and restarts the appropriate nodes\n  * ovs, sdn - OpenShift software-defined networking (SDN) which configures an overlay network using Open vSwitch (OVS).\n  * catalog-operator - responsible for resolving and installing ClusterServiceVersions (CSVs) and the Custom Resource Definitions (CRD) specified resources\n  * olm-operator - install, update, and manage the lifecycle of all Operators and their associated services\n4.\tThe fourth component of Kubernetes is the etcd database that holds the state of the cluster. The etcd container runs in a separate pod. Run the following **kubectl get pods --all-namespaces | grep -i etcd** command to review the etcd pods.\n\n  ```\n  kubectl get pods --all-namespaces | grep -i etcd\n  ```\n  ```\n  openshift-etcd-operator                            etcd-operator-7bb5c44558-vl2pc                                    1/1     Running     0          7d19h\n  openshift-etcd                                     etcd-master1                                                      3/3     Running     0          218d\n  openshift-etcd                                     etcd-master2                                                      3/3     Running     0          218d\n  openshift-etcd                                     etcd-master3                                                      3/3     Running     0          218d\n  openshift-etcd                                     etcd-quorum-guard-76769c67b4-5nsq2                                1/1     Running     0          7d19h\n  openshift-etcd                                     etcd-quorum-guard-76769c67b4-hlwsx                                1/1     Running     0          7d19h\n  openshift-etcd                                     etcd-quorum-guard-76769c67b4-wrq4g                                1/1     Running     0          7d19h\n  openshift-etcd                                     revision-pruner-8-master1                                         0/1     Completed   0          7d19h\n  openshift-etcd                                     revision-pruner-8-master2                                         0/1     Completed   0          7d19h\n  openshift-etcd                                     revision-pruner-8-master3                                         0/1     Completed   0          7d19h\n  ```\n### 6.5\tCreate a pod\n1.\tBuild the Docker Image.\n  \n  Note: if you have completed lab **dk0100st** in this environment, you can skip this task.\n  \n  a.\tFrom the terminal window, navigate to /home/ibmdemo/app-mod-labs/dk0100st directory.\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/dk0100st\n  ```\n  b.\tBuild a Liberty docker image named simpleapp using the Dockerfile in the directory by typing command:\n\t\n  ```\n  docker build . -t simpleapp\n  ```\n  Note: the “.” in the command indicates that a Docker image is built using the Dockerfile in the current directory.             \n2.\tCreate an Kubernetes namespace named **dk0200st** with command:\n\n  ```\n  kubectl create namespace dk0200st\n  ```\n  The output is like this:\n  ```\n  namespace/dk0200st created\n  ```\n  Note: You can also use **oc new-project dk0200st** command to create the project/namespace.\n3.\tPush the simpleapp image to the OCP internal image registry.\n  \n  You are going to use the simpleapp image to create the pod, you need to tag and place the image in the OCP image registry first.\n  \n  a.\tChange to the **dk0200st** namespace scope.\n\n  ```\n  oc project dk0200st\n  ```  \n  b. Get OCP internal image registry URL and cluster URL with commands:\n\n  ```\n  export INTERNAL_REG_HOST=`oc get route default-route --template='{{ .spec.host }}' -n openshift-image-registry`\n  export CLUSTER_URL=`echo $INTERNAL_REG_HOST | sed 's/default-route-openshift-image-registry.//g'`\n  ```\n  c.\tLog in to the OpenShift Docker registry with the command:\n    \n  ```\n  docker login -u $(oc whoami) -p $(oc whoami -t) $INTERNAL_REG_HOST\n  ```\n  The output is like this:\n  ```\n  WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n  WARNING! Your password will be stored unencrypted in /home/ibmdemo/.docker/config.json.\n  Configure a credential helper to remove this warning. See\n  https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\n  Login Succeeded\n  ```\n  d. Execute the following command to push your docker image to OpenShift image repository.\n\n  ```\n  docker tag simpleapp:latest $INTERNAL_REG_HOST/dk0200st/simpleapp:latest \n\n  docker push $INTERNAL_REG_HOST/dk0200st/simpleapp:latest \n\n  ```\n\n  When it is done, your application docker image is pushed to the OCP image registry.\n\n4.\tDeploy the application to OCP cluster.\n\n  a.\tNavigate to **/home/ibmdemo/app-mod-labs/dk0200st** directory.\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/dk0200st\n  ```\n  \n  b.\tList the contents of the directory with command:\n\n  ```\n  ls -l\n  ```\n  ```\n  total 16\n  -rw-rw-r-- 1 ibmdemo ibmdemo 946 Jul  7 14:50 Commands.txt\n  -rw-rw-r-- 1 ibmdemo ibmdemo 316 Jul  7 17:41 deployment.yaml\n  -rw-rw-r-- 1 ibmdemo ibmdemo 344 Jul  7 17:48 route.yaml\n  -rw-rw-r-- 1 ibmdemo ibmdemo 205 Jul  7 06:00 service.yaml\n  ```\n  There are three YAML files you are going to use to deploy the application to the OCP cluster, including deployment.yaml, service.yaml and route.yaml. The YAML manifest files are used extensively in Kubernetes.\n  \n  c.\tRun **cat deployment.yaml** to review the manifest file. \n\n  ```\n  cat deployment.yaml\n  ```\n \n  This is a basic YAML file to create a Pod which uses Docker image simpleapp and names the Pod simpleapp. \n  ```\n  # A simple yaml file to create a ghost pod\n  apiVersion: v1\n  kind: Pod\n  metadata:\n    name: simpleapp\n    labels:\n      app: simpleapp\n  spec:\n    containers:\n      - image: image-registry.openshift-image-registry.svc:5000/dk0200/simpleapp:latest\n        name: simpleapp\n        ports:\n        - containerPort: 9080\n          protocol: TCP\n  ```\n  A basic understanding of the organization of the manifest file is needed to create resources in Kubernetes. Notice that the kube01.yaml manifest file has the following structure:\n    *\tDefine which Kubernetes API is to be used. The name is **apiVersion**.\n    *\tThe possible values of **apiVersion** are v1, apps/v1, v1beta1, v1beta2, batch/v1, extension/v1beta1 and several others. Refer to API documentation at http://kubernetes.io.\n    *\tThe second name-value pair is **kind**, which can be Pod, PodList, Service, Deployment, DeamonSet, ReplicaSet, Job and many others.\n    *\tThe third name-value pair is **metadata**, which describes information such as name, annotations, labels, namespace etc.\n    * The fourth name-value pair is **spec**, which defines containers, their name, image name, and the commands to run with start-up options.\n\n  d. Run **kubectl apply -f deployment.yaml** to deploy the application and create the Pod.\n\n  ```\n  kubectl apply -f deployment.yaml\n  ```\n\n  The pod is created.\n\n  ```\n  pod/simpleapp created\n  ```\n\n  e.\tRun **kubectl get pods** to check the status of the pod.\n \n  ```\n  kubectl get pods\n  ```\n  ```\n  NAME        READY   STATUS    RESTARTS   AGE\n  simpleapp   1/1     Running   0          8m47s\n  ```\n  f.\tCheck the pod again with command **kubectl get pods -o wide**.\n  \n  ```\n  kubectl get pods -o wide\n  ```\n\n  ```\n  NAME        READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES\n  simpleapp   1/1     Running   0          11m   10.129.0.38   master2   <none>           <none>\n  ```\n  Notice that the pod is created, it is deployed to the **master2** node in this example, but it could be deployed on any of the nodes in the cluster. \n  \n### 6.7\tCreate a Service\n\nThe application is deployed to OCP cluster, it is isolated in the IP address assigned to a pod on a host, and there was no connection between outside world and the application residing in the pod.\nNext, you create a service to route the traffic to the application running inside a pod.\nThe service should be able to find the pod where application is running. It does this by using labels.\n\n1.\tReview **service.yaml** to create the service. \n\n  ```\n  cat service.yaml\n  ```\n\n  ```\n  # A simple yaml file to create a service\n  apiVersion: v1\n  kind: Service\n  metadata:\n    labels:\n      app: simpleapp\n    name: simpleapp\n  spec:\n    selector:\n      app: simpleapp\n    ports:\n    - port: 9080\n    type: NodePort\n  ```\n  Note that the service simpleapp is assigned through a label to app: **simpleapp** and port 9080 is automatically assigned on the host using type: **NodePort**.\n\n2.\tCreate the service using **kubectl apply -f service.yaml** then check the service viausing **kubectl get svc** (svc is shorthand for service) command.\n\n  ```\n  kubectl apply -f service.yaml\n  ```\n  ```\n  kubectl get svc\n  ```\n  \n  ```\n  NAME        TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\n  simpleapp   NodePort   172.30.169.2   <none>        9080:30650/TCP   12s\n  ```\n  \n  Note the **nodeport** is 30650 in the sample output, as it will be different in your case.\n\n3.\tOpen a web browser window by clicking its icon on the Desktop tool bar.\n \n    ![](images/firefox-icon.png)\n    \n4. Enter the URL **http<span></span>://master1.demo.ibmdte.net:NodePort/Simple** and substitute the **NodePort** with the number from the **kubectl get svc** command. \n \n  The application page is displayed.\n  \n  ![](images/kubectl-nodeport-url.png)\n\n  Since you have an environment with multiple nodes, the proxy server would be running on all nodes, so you just use any of the hosts, in this case **master1** node, to reach to the proper host pod. \n \t\n### 6.8\tCreate a Route\n\nThe serveice you created exposes the application with a unique IP address and port, allows internal communication to expose the application. There was no connection between outside world and the application residing in the pod.\n\nNext, you create a route to make the service reachable from outside the cluster. Kubernetes routers provide external host name mapping and load balancing to services in the Kunernetes cluster, it is isolated in the IP address assigned to a pod on a host.\n\n1.\tReview **route.yaml** to create the service. \n\n  ```\n  cat route.yaml\n  ```\n\n  ```\n  # A simple yaml file to create a route\n  kind: Route\n  apiVersion: route.openshift.io/v1\n  metadata:\n    name: simpleapp\n    namespace: dk0200st\n    labels:\n      app: simpleapp\n  spec:\n    host: simpleapp-dk0200st.apps.demo.ibmdte.net\n    path: /Simple\n    to:\n      kind: Service\n      name: simpleapp\n      weight: 100\n    port:\n      targetPort: 9080\n    wildcardPolicy: None\n  ```\n  Note that the service **simpleapp** and its port **9080** are assigned to the route.\n  \n2.\tCreate the route using **kubectl apply -f route.yaml** then check the route via **kubectl get route** command. \n\n  ```\n  kubectl apply -f route.yaml\n  ```\n\n  ```\n  kubectl get route\n  ```\n  \n  ```\n  # A simple yaml file to create a route\n  kind: Route\n  apiVersion: route.openshift.io/v1\n  metadata:\n    name: simpleapp\n    namespace: dk0200st\n    labels:\n      app: simpleapp\n  spec:\n    host: simpleapp-dk0200st.apps.demo.ibmdte.net\n    path: /Simple\n    to:\n      kind: Service\n      name: simpleapp\n      weight: 100\n    port:\n      targetPort: 9080\n    wildcardPolicy: None\n  ```\n  You can see your route is created and its URL is **simpleapp-dk0200st.apps.demo.ibmdte.net/Simple**.\n\n3.\tSwitch to the browser and try the URL **http<span></span>://simpleapp-dk0200st.apps.demo.ibmdte.net/Simple**.\n\n  Your application is displayed.\n  \n  ![](images/kubectl-route-url.png)\n\n\n\n### 7.\tSummary\n\nIn this lab, you have learned how to run kubectl commands and how to deploy an application to a Kubernetes cluster using the kubectl commands and YAML files. To learn more about App Mod, please continue with the rest of the lab series.\n\n**Congratulations! You have successfully completed Kubernetes Introduction Lab!**\n\n","fileAbsolutePath":"/Users/yitang/Downloads/app-mod/src/pages/labs/basic-labs/dk0200-kubernetes101/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}