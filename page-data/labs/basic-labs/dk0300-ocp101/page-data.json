{"componentChunkName":"component---src-pages-labs-basic-labs-dk-0300-ocp-101-index-mdx","path":"/labs/basic-labs/dk0300-ocp101/","result":{"pageContext":{"frontmatter":{"title":"OpenShift Container Platform Introduction","description":null},"relativePagePath":"/labs/basic-labs/dk0300-ocp101/index.mdx","titleType":"page","MdxNode":{"id":"2e166951-ea32-5e6e-befb-4bd2d93971b7","children":[],"parent":"5a764310-e9ee-5cc8-b056-1dbbf7098424","internal":{"content":"---\ntitle: OpenShift Container Platform Introduction\ndescription: \n---\nThe goal of this session is to provide a quick introduction to Red Hat OpenShift Container Platform (OCP). You can start by learning some basic OCP concepts.\n\n### 1. What is Red Hat OpenShift Container Platform\nThe Red Hat OpenShift Container Platform offers full access to an enterprise ready Kubernetes.  OCP includes a Kubernetes distribution that has undergone an extensive compatibility test matrix with many of the software elements you will find in your datacenter. As with the rest of the Red Hat software portfolio OCP includes service level agreements for support, bug fixes, and Common Vulnerabilities and Exposures (CVE) protection. OCP provides default security context constraints, pod security policies, best practice network and storage settings, service account configuration, SELinux integration, HAproxy edge routing configuration, and other out of the box protections needed for an enterprise deployment. OCP offers an integrated monitoring solution, based on Prometheus, that offer deep coverage and alerting of common Kubernetesâ€™ issues.\n\n### 2. Objective\n\nThe objectives of this lab are to help you:\n\n* learn how to work with OCP cluster through OCP web console\n* learn how to work with OCP cluster in command line\n\n### 3.\tPrerequisites\n\nThe following prerequisites must be completed prior to beginning this lab:\n*\tFamiliarity with basic Linux commands\n*\tHave internet access\n*\tHave a SkyTap App Mod Lab environment ready\n\n### 4.\tWhat is Already Completed\n\nA six Linux VMs App Mod Lab environment has been provided for this lab. \n \n  ![](images/lab-vms.png)\n\n*\tThe Red Hat OpenShift Container Platform (OCP) v4.6, is installed in 5 VMs, the master1 VM, the master2 VM, the master3 VM, the dns VM and the nfs VM, with 3 master nodes and 3 compute nodes (the master nodes are serving as computer nodes as well).\n*\tThe Workstation VM is the one you will use to access and work with OCP cluster in this lab.\n  The login credentials for the Workstation VM are:\n  User ID: **ibmdemo**\n  Password: **passw0rd**\n  Note: Use the Password above in the Workstation VM Terminal for sudo in the Lab.\n*\tThe CLI commands used in this lab are listed in the **Commands.txt** file located at the **/home/ibmdemo/add-mod-labs/dk0300st** directory of the Workstation VM for you to copy and paste these commands to the Terminal window during the lab.\n\n### 5.\tLab Tasks\n\nDuring this lab, you complete the following tasks:\n*\taccess OCP cluster Web Console\n* view OCP resources in OCP cluster Web Console\n* deploy a sample app to the OCP cluster from OCP cluster web console\n* work with the OCP cluster from command line\n\n### 6.\tExecute Lab Tasks\n\n#### 6.1 Log in to the Workstation VM and get started \n1.  If the VMs are not already started, start them by clicking the play button for the whole group.\n\n  ![](images/start-vms.png)\n\n\n2.\tAfter the VMs are started, click the Workstation VM icon to access it. \n\n  ![](images/access-workstation.png)\n\n  The Workstation Linux Desktop is displayed. You execute all the lab tasks on this workstation VM.\n\n3.\tIf requested to log in to the Workstation OS, use credentials: **ibmdemo**/**passw0rd**\n\n#### 6.2 Work with OCP cluster through OCP Web Console\n\n##### 6.2.1 Access OCP cluster Web Console\n\n1. Open a Firefox web browser window by clicking its icon on the Desktop toolbar.\n\n  ![](images/firefox-icon.png) \n\n2. From the browser window, click OCP cluster Web Console bookmark to open it.\n\n  ![](images/ocp-console-bookmark.png) \n  \n3. If prompted to log in to Red Hat OpenShift Container Platform, click **htpasswd** field. Then log in with **ibmadmin**/**engageibm** as the username and password.\n \n  ![](images/ocp-console-login-1.png) \n\n  ![](images/ocp-console-login-2.png) \n\n  The OCP cluster We Console page is displayed. The default view is the Cluster Overview.\n  \n  ![](images/ocp-console-overview-page.png)\n\n4. Scroll down to view the utilization of cluster resources and cluster inventory. Click through each item in the inventory to find out more.\n\n  Note that:\n\n    * Nodes represent physical or virtual hardware that your Openshift cluster is running.\n    * Pods are used to host and run one or more containers. Each node may run multiple pods. Containers in the same pod share the same network and storage.\n    * Storage classes represent the different types of storage configured and made available for your Openshift cluster.\n    * Persistent Volume Claims (PVCs) represent the usage of storage by the pods. After a pod is removed, data not persistent to persistent storage are gone.\n\n##### 6.2.2 View OCP projects\n\nOCP organizes deployments by **project**. A project extends a Kubernetes **namespace** by providing additional annotations, provides an easy way to observe and change all the resources such as workloads, networking, storage, etc. associated with a Project. The cluster administrator user has access to all projects and all resources.\n\nKubernetes namespaces provide:\n\n  *\tA unique scope to named resources to avoid naming collisions.\n  * Delegated management for trusted users. \n  * The ability to limit resource consumption.\n\nMost objects in the system are scoped by namespace, but some are excepted and have no namespace, including nodes and users.\n\n1.\tClick **Projects** on the left navigation menu to access the Projects page.\n \n  ![](images/ocp-console-projects-link.png) \n\n2.\tClick the **default** project from the list.\n \n  ![](images/ocp-console-deafult-project-link.png) \n  \n  The **default** project **Overview** page is displayed.\n\n  ![](images/ocp-console-deafult-project-overview.png) \n \n3.\tClick on each of the tabs of the project to view its details. \n\n  Note that: \n    \n    * The YAML tab shows the YAML representation of the project. Every resource in Openshift is represented as a REST data structure. you will be working with YAML files a lot more when interacting with Openshift via the command line. \n    * The Role Bindings tab shows you the security configurations that apply to your project. For now, just take notice that there are many different roles already defined when a project is created. Each of these roles is used for a different purpose, and already mapped to different users and groups, or service accounts.\n\n4. Click **Details**>**Annotations**.\n\n  ![](images/ocp-console-project-annotations.png) \n \n5. As previously stated, a OCP project extends a Kubernetes namespace with additional annotations and associates Kubernetes artifacts to the project. Click **Cancel** to exit this view.\n\n  ![](images/ocp-console-project-annotation-details.png) \n\n#### 6.2.3 Create a OCP project\n\nYou can create a OCP project from the OCP web console or in command line. In this task you create a project from the OCP web console.\n\n1.\tClick **Projects** and click **Create Project**.\n\n  ![](images/ocp-console-create-project-link.png)  \n\n2.\tEnter **dk0300st** for both the Name and Display Name, and **my lab dk0300st** as Description then click **Create**.\n \n  ![](images/ocp-console-create-project.png) \n\n  The project **dk0300st** is created.\n  \n  ![](images/ocp-console-project-created.png) \n\n#### 6.2.4 Create a sample application \n\nOnce you have the project create in OCP cluster, you are ready to deploy your first application.\n\nThe typical artifacts you will need to run an application in Openshift are:\n  * A container image containing your application, hosted in a container registry\n  * One or more pods that specifies where to fetch an image and how it should be hosted.\n  * A deployment to control the number of instances pods. You don't normally configure a pod directly. Instead, you configure a deployment to manage a set of pods.\n  * A service that exposes the application within the internal network, and enables the application to be load balanced within the Openshift cluster.\n  * A route or ingress to make the application accessible outside of the Openshift cluster firewall.\n\n  ![](images/typical-deployment.png) \n\n1. Make sue that you are in the **dk0300st** project scope, then from the **Workloads**, click **Deployments**, followed by **Create Deployment**:\n\n  ![](images/create-deployment.png) \n\n2. Note that the console shows you the YAML file for the deployment. Change the number of replicas from default **3** to **2**, then click **Create**:\n\n  ![](images/create-deployment-2.png) \n\n  Here is the specification of the deployment in its entirety:\n\n  ```\n  apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    name: example\n    namespace: dk0300st\n  spec:\n    selector:\n      matchLabels:\n        app: hello-openshift\n    replicas: 2\n    template:\n      metadata:\n        labels:\n          app: hello-openshift\n      spec:\n        containers:\n          - name: hello-openshift\n            image: openshift/hello-openshift\n            ports:\n              - containerPort: 8080\n  ```\n\n3. Review this resource:\n\n    - Every resource in Openshift has a group, version, and kind. For the **Deployment** resource:\n      - The group is **apps**\n      - The version is **v1**\n      - The kind is **Deployment**\n    - The metadata specifies data that is needed for the runtime:\n      - The name of this instance is **example**\n      - The namespace where the resource is running is **dk0300st**\n      - Though not shown here, any labels associated with the resource. We will see the use of labels later.\n    - The **spec** section defines the details specific to this kind of resource:\n      - The **selector** defines details of the **pods** that this **deployment** will manage. The **matchLabels** attribute with value **app: hello-openshift** means this **deployment** instance will search for and manage all pods whose labels contain **app: hello-openshift**.\n    - The **replicas: 2**  field specifies the number of instances to run.\n    - The **template** section describes information about how to run the container image and create the **pods**:\n      - The **labels** section specifies what labels to add to the pods being to be created. Note that it matches the labels defined in the **selector**.\n      - The **containers** section specifies where to fetch the container image and which ports to expose. For our example, the image to run is **openshift/hello-openshift**.\n    \n4. Wait for both pods to be running:\n\n    ![](images/DeploymentAfterCreate.png)\n\n5. Click on the YAML tab, and note the additions to the original input YAML file.\n\n    ![](images/DeploymentAfterCreateYAML.png)\n\n    Here is a sample YAML after the deployment is created :\n\n    ```\n    kind: Deployment\n    apiVersion: apps/v1\n    metadata:\n      annotations:\n        deployment.kubernetes.io/revision: '1'\n      selfLink: /apis/apps/v1/namespaces/dk0300st/deployments/example\n      resourceVersion: '2202243'\n      name: example\n      uid: 48bbff65-27d5-4823-9734-7d90e9a4a007\n      creationTimestamp: '2021-07-09T19:16:28Z'\n      generation: 1\n      managedFields:\n        - manager: Mozilla\n          operation: Update\n          apiVersion: apps/v1\n          time: '2021-07-09T19:16:28Z'\n          fieldsType: FieldsV1\n          fieldsV1:\n            'f:spec':\n              'f:progressDeadlineSeconds': {}\n              'f:replicas': {}\n              .....\n        - manager: kube-controller-manager\n          operation: Update\n          apiVersion: apps/v1\n          time: '2021-07-09T19:16:32Z'\n          fieldsType: FieldsV1\n          fieldsV1:\n            'f:metadata':\n              'f:annotations':\n                .: {}\n                'f:deployment.kubernetes.io/revision': {}\n            .....\n      namespace: dk0300st\n    spec:\n      replicas: 2\n      selector:\n        matchLabels:\n          app: hello-openshift\n      template:\n        metadata:\n          creationTimestamp: null\n          labels:\n            app: hello-openshift\n        spec:\n          containers:\n            - name: hello-openshift\n              image: openshift/hello-openshift\n              ports:\n                - containerPort: 8080\n                  protocol: TCP\n              resources: {}\n              terminationMessagePath: /dev/termination-log\n              terminationMessagePolicy: File\n              imagePullPolicy: Always\n          restartPolicy: Always\n          terminationGracePeriodSeconds: 30\n          dnsPolicy: ClusterFirst\n          securityContext: {}\n          schedulerName: default-scheduler\n      strategy:\n        type: RollingUpdate\n        rollingUpdate:\n          maxUnavailable: 25%\n          maxSurge: 25%\n      revisionHistoryLimit: 10\n      progressDeadlineSeconds: 600\n    status:\n      observedGeneration: 1\n      replicas: 2\n      updatedReplicas: 2\n      readyReplicas: 2\n      availableReplicas: 2\n      conditions:\n        - type: Available\n          status: 'True'\n          lastUpdateTime: '2021-07-09T19:16:32Z'\n          lastTransitionTime: '2021-07-09T19:16:32Z'\n          reason: MinimumReplicasAvailable\n          message: Deployment has minimum availability.\n        - type: Progressing\n          status: 'True'\n          lastUpdateTime: '2021-07-09T19:16:32Z'\n          lastTransitionTime: '2021-07-09T19:16:28Z'\n          reason: NewReplicaSetAvailable\n          message: ReplicaSet \"example-5fb6876865\" has successfully progressed.\n    ```\n\n  Note that:\n\n    - There are quite a bit more **metadata**. Metadata may be added by any number of controllers as needed to help with their function.\n    - The **spec** has more attributes filled in as well. These are default values that were not specified in our original YAML file. But sometimes it is also possible that some values are overridden by background admission controllers.\n    - The **status** sub-resource is how Openshift communicates that status of the resource. The **status** is updated regularly as the underlying state of the resource changes.\n\n6. Click on **Workloads**>**Pods**. \n\n  Note that the pods resources are managed by the controller for your **deployment**.  You do not create the pod resources yourself. That is the reason that **Pods** tab is under the **deployment** resource you just created.\n\n    ![](images/DeploymentToPods.png)\n\n8. Click on one of the pods:\n\n    ![](images/Pods.png)\n\n9. Explore the various tabs for the pod.\n\n    ![](images/ExplorePod.png)\n\n   - Overview: displays the overall resource usage for your pod. Note that for CPU usage, the unit is m, or milli-core, which is 1/1000th of one core.\n   - YAML: examine the YAML that describes your pod. This YAML is created by the deployment controller based on the specification you supplied in your deployment. Note that labels associated with your pod are what you had specified in the deployment.\n   - Environment: lists the environment variables defined for your pod. For your **hello-openshift** pod, there is none.\n   - Logs: shows the console log for your container. Note that it is the same log as the log from the Introduction to Docker lab, as the same image is being used.\n   - Terminal: Opens a remote shell into your container. As with the Introduction to Docker lab, no shell is available within the container for this image. This makes it more secure, but also more difficult to debug.\n\n#### 6.2.5 Create a service for the application in the OCP cluster\n\nA service enables the pods you just created to be load balanced within the Openshift cluster. \n\n1. Scroll down to the **Networking** tab on the left navigation, click **Services**, then click **Create Service**:\n\n    ![](images/CreateService.png)\n\n2. Update the YAML parameters as follows, then click **Create**:\n    \n    - Under spec.selector, \n      - change **MyApp** to **hello-openshift**. \n      - This is how the service will find the pods to load balance. Therefore, it matches the labels (**spec.selector.matchLabels**) that we used when creating the deployment for the hello-openshift application.\n    - Under spec.ports, \n      - change **80** to **8080** and \n      - change **9376** to **8080** (the same ports we used previously).\n\n  ![](images/CreateService_after.png)\n\n3. After the service is created, click on the YAML tab:\n\n    ![](images/CreateServiceAfterYAML.png)\n\n    The YAML file looks like:\n    ```\n    kind: Service\n    apiVersion: v1\n    metadata:\n      name: example\n      namespace: dk0300st\n      selfLink: /api/v1/namespaces/dk0300st/services/example\n      uid: 15ce0c1e-6f27-4ea6-9586-a7a8fbb7a724\n      resourceVersion: '2211781'\n      creationTimestamp: '2021-07-09T19:36:15Z'\n      managedFields:\n        - manager: Mozilla\n          operation: Update\n          apiVersion: v1\n          time: '2021-07-09T19:36:15Z'\n          fieldsType: FieldsV1\n          fieldsV1:\n            'f:spec':\n              'f:ports':\n                .: {}\n                'k:{\"port\":8080,\"protocol\":\"TCP\"}':\n                  .: {}\n                  'f:port': {}\n                  'f:protocol': {}\n                  'f:targetPort': {}\n              'f:selector':\n                .: {}\n                'f:app': {}\n              'f:sessionAffinity': {}\n              'f:type': {}\n    spec:\n      ports:\n        - protocol: TCP\n          port: 8080\n          targetPort: 8080\n      selector:\n        app: hello-openshift\n      clusterIP: 172.30.23.104\n      type: ClusterIP\n      sessionAffinity: None\n    status:\n      loadBalancer: {}\n    ```\n\n  Note that for this service, there is a cluster wide IP address created, and that it is being load balanced. Also session affinity is not set for this service.\n\n#### 6.2.6 Create a route for the application in the OCP cluster\n\nA route exposes your internal endpoints outside your cluster's built-in firewall. \n\n1. Click on the **Route** tab under **Networking** in the left navigation, then click **Create Route**:\n\n    ![](images/CreateRoute.png)\n\n2. Supply inputs to the following parameters, then click **Create**:\n\n    - Name: **example**\n    - Service: **example**\n    - Target Port: **8080 --> 8080 (TCP)**\n\n    ![](images/CreateRouteParams.png)\n\n    Note that you are ignoring TLS configuration just for the purpose of this lab.  Security will be addressed in a different lab.\n\n3. Access the app route at the link provided under **Location**:\n\n    ![](images/CreateRouteAccessRoute.png)\n\n  If you have configured everything correctly, the browser will show **Hello Openshift!**. \n\n  Congratulations, you just deployed your first application to Openshift.\n\n    ![](images/CreateRouteAccessRouteResult.png)\n\n#### 6.2.7 Changing Replica Instances\n\n1. Click on the **Projects** tab under **Home** from the left navigation, then click on **dk0300st**:\n\n    ![](images/LocateMyproject.png)\n\n2. Scroll down to the **Inventory** section and see the resources that were created. Recall that we have created one deployment with 2 pods in the specification. We also created one service, and one route.\n\n    ![](images/LocateMyprojectResources.png)\n\n3. Click on the 2 pods:\n\n   ![](images/LocateMyprojectPods.png)\n\n\n4. Delete one of the pods by clicking on the menu on the right, then selecting **Delete pod**. When prompted, click **Delete**.\n\n    ![](images/DeletePod.png)\n\n    This is not the right way to reduce number of instances. You will notice that as soon as one of the pods is being terminated, another one is being created. The reason is that the controller for the **deployment** resource knows that your specification is for 2 instances, and it honors that specification by creating another one. This also gives you automatic failure recovery should one of the pods crashes on its own.\n\n5. To change the number of instances, you will need to change the specification of your deployment. Click on the **Workloads**>**Deployments** in the left navigation, then click on **example** deployment:\n\n    ![](images/LocateDeployment.png)\n\n6. Click on the down arrow to reduce the replica size down to 1:\n\n    ![](images/DeploymentReducePod.png)\n\n7. After the operation is completed, click on the YAML tab to view the YAML file contents:\n\n    ![](images/DeploymentReducePod1.png)\n\n    ```\n    kind: Deployment\n    apiVersion: apps/v1\n    metadata:\n      annotations:\n        deployment.kubernetes.io/revision: '1'\n      selfLink: /apis/apps/v1/namespaces/dk0300st/deployments/example\n      resourceVersion: '2224790'\n      name: example\n      uid: 48bbff65-27d5-4823-9734-7d90e9a4a007\n      creationTimestamp: '2021-07-09T19:16:28Z'\n      generation: 2\n      managedFields:\n        - manager: Mozilla\n          operation: Update\n          apiVersion: apps/v1\n          time: '2021-07-09T20:03:23Z'\n          fieldsType: FieldsV1\n          fieldsV1:\n            'f:spec':\n            ....\n              'f:updatedReplicas': {}\n      namespace: dk0300st\n    spec:\n      replicas: 1\n      selector:\n        matchLabels:\n          app: hello-openshift\n      template:\n        metadata:\n          creationTimestamp: null\n          labels:\n            app: hello-openshift\n        spec:\n          ....\n\n    ```\n    Note that the console had changed the REST specification on your behalf so that the replica count is now 1:\n\n### 6.3 Work with OCP cluster through the command line\n\nYou can use both **oc**, the openshift command line tool, or **kubectl**, the Kubernetes command line tool, to interact with Openshift. \nResources in Openshift are configured via REST data structure. **oc** extends **kubectl** and offers the same capabilities as the kubectl but it is further extended to natively support OpenShift Container Platform features, such as OpenShift resources such as DeploymentConfigs, BuildConfigs, Routes, ImageStreams, and ImageStreamTags which are specific to OpenShift distributions, and not available in standard Kubernetes.\nFor the command line tools, the REST data structure may be stored either in  a YAML file, or in a JSON file.\nThe command line tools may be used to:\n\n- List available resources\n- Create resources\n- Update existing resources\n- Delete resources\n\n#### 6.3.1  Login to OCP cluster from Command Line\n\n1. Open a terminal window by clicing its icon from the Desktop toolbar.\n\n  ![](images/terminal-icon.png)\n\n2. Change directory to:  **/home/ibmdemo/app-mod-labs/dk0300st**\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/dk0300st\n  ```\n\n3. Issue the **oc login** command to login to the OCP cluster:\n\n  ```\n  oc login https://api.demo.ibmdte.net:6443\n  ```\n\n  when promted, enter the login credientials as: **ibmadmin**/**engageibm**.\n  \n  ```\n  Authentication required for https://api.demo.ibmdte.net:6443 (openshift)\n  Username: ibmadmin\n  Password: \n  Login successful.\n\n  You have access to 66 projects, the list has been suppressed. You can list all projects with ' projects'\n\n  Using project \"default\".\n  ``` \n\n#### 6.3.2 Listing OCP cluster resources\n\n1. Use **oc api-resources** command to list all available resource kinds. \n\n  ```\n  oc api-resources\n  ```\n\n  Note that resources in Openshift have a group, version, and kind. Some resources are global (not in a namespace), while others are scoped to a namespace. Many resources also have short names to save typing when using the command line tool. For example, you may use **cm** instead of ConfigMap as a command line parameter when the parameter is for a **KIND**.\n  Example output:\n\n  ```\n  NAME                                  SHORTNAMES       APIGROUP                              NAMESPACED   KIND\n  bindings                                                                                     true         Binding\n  componentstatuses                     cs                                                     false        ComponentStatu\n  s\n  configmaps                            cm                                                     true         ConfigMap\n  endpoints                             ep                                                     true         Endpoints\n  events                                ev                                                     true         Event\n  limitranges                           limits                                                 true         LimitRange\n  namespaces                            ns                                                     false        Namespace\n  nodes                                 no                                                     false        Node\n  ```\n\n2. List all projects with the following command:\n\n  ```\n  oc get projects\n  ```\n  Example output:\n  ```\n  NAME          DISPLAY NAME   STATUS\n  default                      Active\n  ibm-observe                  Active\n  kube-node-lease              Active\n  kube-public                  Active\n  kube-system                  Active\n  ...\n  ```\n\n\n3. List all pods in all namespaces:\n\n  ```\n  oc get pods --all-namespaces\n  ```\n  Example output:\n  ```\n  NAMESPACE                                          NAME                                                              READY   STATUS      RESTARTS   AGE\n  default                                            ibm-toolkit-h9z8n                                                 0/1     Completed   0          3d6h\n  dk0200st                                           simpleapp                                                         1/1     Running     0          175m\n  dk0300st                                           example-5fb6876865-mtpmt                                          1/1     Running     0          118m\n  nfs-storage                                        nfs-client-provisioner-7bd5cb954-bznt4                            1/1     Running     0          29h\n  openshift-apiserver-operator                       openshift-apiserver-operator-67fbc98b9d-27xsx                     1/1     Running     0          29h\n  openshift-apiserver                                apiserver-7b65f5d4b6-9fcwd                                        2/2     Running     0          29h\n  openshift-apiserver                                apiserver-7b65f5d4b6-jqlr7                                        2/2     Running     0          47h\n  ...\n  ```\n\n4.  List all cluster nodes:\n\n  ```\n  oc get nodes\n  ```\n\n  ```\n  NAME      STATUS   ROLES           AGE    VERSION\n  master1   Ready    master,worker   380d   v1.19.0+43983cd\n  master2   Ready    master,worker   380d   v1.19.0+43983cd\n  master3   Ready    master,worker   380d   v1.19.0+43983cd\n  ```\n\n5. List all projects with command:\n\n  ```\n  oc get projects\n  ```\n\n    ```\n    NAME                                                    DISPLAY NAME   STATUS\n    default                                                                Active\n    ibm-cert-store                                                         Active\n    ibm-system                                                             Active\n    kube-node-lease  \n    ....                                                      Active\n    ```\n\n6. Get a specific project **dk0300st** from the **default** project. \n\n  ```\n  oc project dk0300st\n  ``` \n\n  ```\n  Now using project \"dk0300st\" on server \"https://api.demo.ibmdte.net:6443\".\n  ```\n\n#### 6.3.3 Work with existing resources\n\n1. View the deployment you did in the section above using the OCP web console.\n\n  ```\n  oc get deployments\n  ```\n  ```\n  NAME      READY   UP-TO-DATE   AVAILABLE   AGE\n  example   1/1     1            1           2d18h\n  ```\n2. Check the status of deployment.\n\n  ```\n  oc get deployment example -o yaml\n  ```\n  \n  ```\n  apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n    creationTimestamp: \"2021-07-09T19:16:28Z\"\n    generation: 2\n    managedFields:\n    - apiVersion: apps/v1\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:spec:\n          ...\n      manager: Mozilla\n      operation: Update\n      time: \"2021-07-09T20:03:23Z\"\n    - apiVersion: apps/v1\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:metadata:\n          ....\n      manager: kube-controller-manager\n      operation: Update\n      time: \"2021-07-09T20:03:23Z\"\n    name: example\n    namespace: dk0300st\n    resourceVersion: \"2224790\"\n    selfLink: /apis/apps/v1/namespaces/dk0300st/deployments/example\n    uid: 48bbff65-27d5-4823-9734-7d90e9a4a007\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        app: hello-openshift\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: hello-openshift\n      spec:\n        containers:\n        - image: openshift/hello-openshift\n          imagePullPolicy: Always\n          name: hello-openshift\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          ...\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2021-07-09T19:16:28Z\"\n      lastUpdateTime: \"2021-07-09T19:16:32Z\"\n      message: ReplicaSet \"example-5fb6876865\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    - lastTransitionTime: \"2021-07-09T19:57:13Z\"\n      lastUpdateTime: \"2021-07-09T19:57:13Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    observedGeneration: 2\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n\n  ```\n\n3. List the running pods created by the controller for the deployment.\n\n  ```\n  oc get pods\n  ```\n\n  ```\n  NAME                      READY   STATUS    RESTARTS   AGE\n  example-5fb6876865-mtpmt   1/1     Running   0          2d18h\n  ```\n\n4. Get the detail information of the pod.\n\n  ```\n  oc describe pod example-5fb6876865-mtpmt\n  ```\n  ```\n  Name:         example-5fb6876865-mtpmt\n  Namespace:    dk0300st\n  Priority:     0\n  Node:         master1/10.0.0.111\n  Start Time:   Fri, 09 Jul 2021 12:16:28 -0700\n  Labels:       app=hello-openshift\n                pod-template-hash=5fb6876865\n  Annotations:  k8s.v1.cni.cncf.io/network-status:\n                  [{\n                      \"name\": \"\",\n                      \"interface\": \"eth0\",\n                      \"ips\": [\n                          \"10.128.0.14\"\n                      ],\n                      \"default\": true,\n                      \"dns\": {}\n                  }]\n                k8s.v1.cni.cncf.io/networks-status:\n                  [{\n                      \"name\": \"\",\n                      \"interface\": \"eth0\",\n                      \"ips\": [\n                          \"10.128.0.14\"\n                      ],\n                      \"default\": true,\n                      \"dns\": {}\n                  }]\n                openshift.io/scc: restricted\n  Status:       Running\n  IP:           10.128.0.14\n  IPs:\n    IP:           10.128.0.14\n  Controlled By:  ReplicaSet/example-5fb6876865\n  Containers:\n    hello-openshift:\n      Container ID:   cri-o://72d60b9a47ff457d0ae60a30e7aed7b3809251989de6ebcf6725731f9645da7d\n      Image:          openshift/hello-openshift\n      Image ID:       docker.io/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\n      Port:           8080/TCP\n      Host Port:      0/TCP\n      State:          Running\n        Started:      Mon, 12 Jul 2021 06:12:54 -0700\n      Ready:          True\n      Restart Count:  0\n      Environment:    <none>\n      Mounts:\n        /var/run/secrets/kubernetes.io/serviceaccount from default-token-twzqj (ro)\n  Conditions:\n    Type              Status\n    Initialized       True \n    Ready             True \n    ContainersReady   True \n    PodScheduled      True \n  Volumes:\n    default-token-twzqj:\n      Type:        Secret (a volume populated by a Secret)\n      SecretName:  default-token-twzqj\n      Optional:    false\n  QoS Class:       BestEffort\n  Node-Selectors:  <none>\n  Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                  node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n  Events:\n    Type    Reason   Age   From     Message\n    ----    ------   ----  ----     -------\n    Normal  Pulling  19m   kubelet  Pulling image \"openshift/hello-openshift\"\n    Normal  Pulled   19m   kubelet  Successfully pulled image \"openshift/hello-openshift\" in 1.147946396s\n    Normal  Created  19m   kubelet  Created container hello-openshift\n    Normal  Started  19m   kubelet  Started container hello-openshift\n\n  ```\n\n5. Show the logs of the pod.\n\n  ```\n  oc logs example-5fb6876865-mtpmt\n  ```\n\n  ```\n  serving on 8888\n  serving on 8080\n  Servicing request.\n  Servicing request.\n  Servicing request.\n  Servicing request.\n  serving on 8888\n  serving on 8080\n  ```\n\n#### 6.3.4 Changing Replica Instance\n\n1. List pods.\n\n  ```\n  oc get pods\n  ```\n\n  ```\n  NAME                      READY   STATUS    RESTARTS   AGE\n  example-5fb6876865-mtpmt   1/1     Running   0          2d18h\n  ```\n\n2. Delete the pod with command **oc delete pod PODNAME**\n\n  ```\n  oc delete pod example-5fb6876865-mtpmt\n  ```\n\n  ```\n  pod \"example-5fb6876865-mtpmt\" deleted\n  ```\n\n3. List pods again with command **oc get pods** and note that a new instance of the pod has been created as expected. The deployment specified 1 instance, so the controller tries to maintain 1 instance.\n\n  ```\n  NAME                      READY   STATUS    RESTARTS   AGE\n  example-5fb6876865-qkj4r   1/1     Running   0          13s    example-75778c488-rhjrx   1/1     Running   0          28s\n  ```\n\n4. To increse the number of pod instances, we can patch the resource in one of two ways:\n   - Scripted patch using the **patch** option of the command line:\n      ```\n      oc patch deployment example -p '{ \"spec\": { \"replicas\": 1 } }'\n      ```\n   - Interactive patch using the **edit** option of the command line through **vi** editor:\n      ```\n      oc edit deployment example\n      ```\n      Under the **spec** section (not under the **status** section), change **replicas: 1** to **replicas: 2**, and save the change (by **:wq**).\n\n    ![](images/edit-deployment.png)\n\n      The output:\n      ```\n      deployment.extensions/example edited\n      ```\n\n      Note: The above edits the copy that is stored in Openshift.\n\n  Once the change is made, it will be applied to the pod automatically.\n5. List the pods to show only 1 pod is running: **oc get pods**.\n\n    ```\n    NAME                       READY   STATUS    RESTARTS   AGE\n    example-5fb6876865-76klt   1/1     Running   0          16s\n    example-5fb6876865-qkj4r   1/1     Running   0          8m23s\n    ```\n\n#### 6.3.5 Deploy new applications\n\nYou can also using **oc** commands to deploy applications to OCP cluster, the deplyment process is simliar to the Kubernetes deployment introduced in the **[DK0200 - Kubernetes Introduction](/labs/basic-labs/dk0200-kubernetes101/)** Lab, you just need to replace the **kubectl** with **oc** in the commands.  If you are intrested, please learn from the  **[DK0200 - Kubernetes Introduction](/labs/basic-labs/dk0200-kubernetes101/)** Lab.\n\n\n### 7.\tSummary\nIn this lab, you have learned some basic features and functions of Red Hat OpenShift Container Platform (OCP) and how to use them. To learn more about App Mod and DevOps, please continue with the rest of the lab series.\n\n**Congratulations! You have successfully completed OCP Introduction Lab!**\n\n","type":"Mdx","contentDigest":"a48eb661244a387544a0733dd1e457d6","owner":"gatsby-plugin-mdx","counter":2875},"frontmatter":{"title":"OpenShift Container Platform Introduction","description":null},"exports":{},"rawBody":"---\ntitle: OpenShift Container Platform Introduction\ndescription: \n---\nThe goal of this session is to provide a quick introduction to Red Hat OpenShift Container Platform (OCP). You can start by learning some basic OCP concepts.\n\n### 1. What is Red Hat OpenShift Container Platform\nThe Red Hat OpenShift Container Platform offers full access to an enterprise ready Kubernetes.  OCP includes a Kubernetes distribution that has undergone an extensive compatibility test matrix with many of the software elements you will find in your datacenter. As with the rest of the Red Hat software portfolio OCP includes service level agreements for support, bug fixes, and Common Vulnerabilities and Exposures (CVE) protection. OCP provides default security context constraints, pod security policies, best practice network and storage settings, service account configuration, SELinux integration, HAproxy edge routing configuration, and other out of the box protections needed for an enterprise deployment. OCP offers an integrated monitoring solution, based on Prometheus, that offer deep coverage and alerting of common Kubernetesâ€™ issues.\n\n### 2. Objective\n\nThe objectives of this lab are to help you:\n\n* learn how to work with OCP cluster through OCP web console\n* learn how to work with OCP cluster in command line\n\n### 3.\tPrerequisites\n\nThe following prerequisites must be completed prior to beginning this lab:\n*\tFamiliarity with basic Linux commands\n*\tHave internet access\n*\tHave a SkyTap App Mod Lab environment ready\n\n### 4.\tWhat is Already Completed\n\nA six Linux VMs App Mod Lab environment has been provided for this lab. \n \n  ![](images/lab-vms.png)\n\n*\tThe Red Hat OpenShift Container Platform (OCP) v4.6, is installed in 5 VMs, the master1 VM, the master2 VM, the master3 VM, the dns VM and the nfs VM, with 3 master nodes and 3 compute nodes (the master nodes are serving as computer nodes as well).\n*\tThe Workstation VM is the one you will use to access and work with OCP cluster in this lab.\n  The login credentials for the Workstation VM are:\n  User ID: **ibmdemo**\n  Password: **passw0rd**\n  Note: Use the Password above in the Workstation VM Terminal for sudo in the Lab.\n*\tThe CLI commands used in this lab are listed in the **Commands.txt** file located at the **/home/ibmdemo/add-mod-labs/dk0300st** directory of the Workstation VM for you to copy and paste these commands to the Terminal window during the lab.\n\n### 5.\tLab Tasks\n\nDuring this lab, you complete the following tasks:\n*\taccess OCP cluster Web Console\n* view OCP resources in OCP cluster Web Console\n* deploy a sample app to the OCP cluster from OCP cluster web console\n* work with the OCP cluster from command line\n\n### 6.\tExecute Lab Tasks\n\n#### 6.1 Log in to the Workstation VM and get started \n1.  If the VMs are not already started, start them by clicking the play button for the whole group.\n\n  ![](images/start-vms.png)\n\n\n2.\tAfter the VMs are started, click the Workstation VM icon to access it. \n\n  ![](images/access-workstation.png)\n\n  The Workstation Linux Desktop is displayed. You execute all the lab tasks on this workstation VM.\n\n3.\tIf requested to log in to the Workstation OS, use credentials: **ibmdemo**/**passw0rd**\n\n#### 6.2 Work with OCP cluster through OCP Web Console\n\n##### 6.2.1 Access OCP cluster Web Console\n\n1. Open a Firefox web browser window by clicking its icon on the Desktop toolbar.\n\n  ![](images/firefox-icon.png) \n\n2. From the browser window, click OCP cluster Web Console bookmark to open it.\n\n  ![](images/ocp-console-bookmark.png) \n  \n3. If prompted to log in to Red Hat OpenShift Container Platform, click **htpasswd** field. Then log in with **ibmadmin**/**engageibm** as the username and password.\n \n  ![](images/ocp-console-login-1.png) \n\n  ![](images/ocp-console-login-2.png) \n\n  The OCP cluster We Console page is displayed. The default view is the Cluster Overview.\n  \n  ![](images/ocp-console-overview-page.png)\n\n4. Scroll down to view the utilization of cluster resources and cluster inventory. Click through each item in the inventory to find out more.\n\n  Note that:\n\n    * Nodes represent physical or virtual hardware that your Openshift cluster is running.\n    * Pods are used to host and run one or more containers. Each node may run multiple pods. Containers in the same pod share the same network and storage.\n    * Storage classes represent the different types of storage configured and made available for your Openshift cluster.\n    * Persistent Volume Claims (PVCs) represent the usage of storage by the pods. After a pod is removed, data not persistent to persistent storage are gone.\n\n##### 6.2.2 View OCP projects\n\nOCP organizes deployments by **project**. A project extends a Kubernetes **namespace** by providing additional annotations, provides an easy way to observe and change all the resources such as workloads, networking, storage, etc. associated with a Project. The cluster administrator user has access to all projects and all resources.\n\nKubernetes namespaces provide:\n\n  *\tA unique scope to named resources to avoid naming collisions.\n  * Delegated management for trusted users. \n  * The ability to limit resource consumption.\n\nMost objects in the system are scoped by namespace, but some are excepted and have no namespace, including nodes and users.\n\n1.\tClick **Projects** on the left navigation menu to access the Projects page.\n \n  ![](images/ocp-console-projects-link.png) \n\n2.\tClick the **default** project from the list.\n \n  ![](images/ocp-console-deafult-project-link.png) \n  \n  The **default** project **Overview** page is displayed.\n\n  ![](images/ocp-console-deafult-project-overview.png) \n \n3.\tClick on each of the tabs of the project to view its details. \n\n  Note that: \n    \n    * The YAML tab shows the YAML representation of the project. Every resource in Openshift is represented as a REST data structure. you will be working with YAML files a lot more when interacting with Openshift via the command line. \n    * The Role Bindings tab shows you the security configurations that apply to your project. For now, just take notice that there are many different roles already defined when a project is created. Each of these roles is used for a different purpose, and already mapped to different users and groups, or service accounts.\n\n4. Click **Details**>**Annotations**.\n\n  ![](images/ocp-console-project-annotations.png) \n \n5. As previously stated, a OCP project extends a Kubernetes namespace with additional annotations and associates Kubernetes artifacts to the project. Click **Cancel** to exit this view.\n\n  ![](images/ocp-console-project-annotation-details.png) \n\n#### 6.2.3 Create a OCP project\n\nYou can create a OCP project from the OCP web console or in command line. In this task you create a project from the OCP web console.\n\n1.\tClick **Projects** and click **Create Project**.\n\n  ![](images/ocp-console-create-project-link.png)  \n\n2.\tEnter **dk0300st** for both the Name and Display Name, and **my lab dk0300st** as Description then click **Create**.\n \n  ![](images/ocp-console-create-project.png) \n\n  The project **dk0300st** is created.\n  \n  ![](images/ocp-console-project-created.png) \n\n#### 6.2.4 Create a sample application \n\nOnce you have the project create in OCP cluster, you are ready to deploy your first application.\n\nThe typical artifacts you will need to run an application in Openshift are:\n  * A container image containing your application, hosted in a container registry\n  * One or more pods that specifies where to fetch an image and how it should be hosted.\n  * A deployment to control the number of instances pods. You don't normally configure a pod directly. Instead, you configure a deployment to manage a set of pods.\n  * A service that exposes the application within the internal network, and enables the application to be load balanced within the Openshift cluster.\n  * A route or ingress to make the application accessible outside of the Openshift cluster firewall.\n\n  ![](images/typical-deployment.png) \n\n1. Make sue that you are in the **dk0300st** project scope, then from the **Workloads**, click **Deployments**, followed by **Create Deployment**:\n\n  ![](images/create-deployment.png) \n\n2. Note that the console shows you the YAML file for the deployment. Change the number of replicas from default **3** to **2**, then click **Create**:\n\n  ![](images/create-deployment-2.png) \n\n  Here is the specification of the deployment in its entirety:\n\n  ```\n  apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    name: example\n    namespace: dk0300st\n  spec:\n    selector:\n      matchLabels:\n        app: hello-openshift\n    replicas: 2\n    template:\n      metadata:\n        labels:\n          app: hello-openshift\n      spec:\n        containers:\n          - name: hello-openshift\n            image: openshift/hello-openshift\n            ports:\n              - containerPort: 8080\n  ```\n\n3. Review this resource:\n\n    - Every resource in Openshift has a group, version, and kind. For the **Deployment** resource:\n      - The group is **apps**\n      - The version is **v1**\n      - The kind is **Deployment**\n    - The metadata specifies data that is needed for the runtime:\n      - The name of this instance is **example**\n      - The namespace where the resource is running is **dk0300st**\n      - Though not shown here, any labels associated with the resource. We will see the use of labels later.\n    - The **spec** section defines the details specific to this kind of resource:\n      - The **selector** defines details of the **pods** that this **deployment** will manage. The **matchLabels** attribute with value **app: hello-openshift** means this **deployment** instance will search for and manage all pods whose labels contain **app: hello-openshift**.\n    - The **replicas: 2**  field specifies the number of instances to run.\n    - The **template** section describes information about how to run the container image and create the **pods**:\n      - The **labels** section specifies what labels to add to the pods being to be created. Note that it matches the labels defined in the **selector**.\n      - The **containers** section specifies where to fetch the container image and which ports to expose. For our example, the image to run is **openshift/hello-openshift**.\n    \n4. Wait for both pods to be running:\n\n    ![](images/DeploymentAfterCreate.png)\n\n5. Click on the YAML tab, and note the additions to the original input YAML file.\n\n    ![](images/DeploymentAfterCreateYAML.png)\n\n    Here is a sample YAML after the deployment is created :\n\n    ```\n    kind: Deployment\n    apiVersion: apps/v1\n    metadata:\n      annotations:\n        deployment.kubernetes.io/revision: '1'\n      selfLink: /apis/apps/v1/namespaces/dk0300st/deployments/example\n      resourceVersion: '2202243'\n      name: example\n      uid: 48bbff65-27d5-4823-9734-7d90e9a4a007\n      creationTimestamp: '2021-07-09T19:16:28Z'\n      generation: 1\n      managedFields:\n        - manager: Mozilla\n          operation: Update\n          apiVersion: apps/v1\n          time: '2021-07-09T19:16:28Z'\n          fieldsType: FieldsV1\n          fieldsV1:\n            'f:spec':\n              'f:progressDeadlineSeconds': {}\n              'f:replicas': {}\n              .....\n        - manager: kube-controller-manager\n          operation: Update\n          apiVersion: apps/v1\n          time: '2021-07-09T19:16:32Z'\n          fieldsType: FieldsV1\n          fieldsV1:\n            'f:metadata':\n              'f:annotations':\n                .: {}\n                'f:deployment.kubernetes.io/revision': {}\n            .....\n      namespace: dk0300st\n    spec:\n      replicas: 2\n      selector:\n        matchLabels:\n          app: hello-openshift\n      template:\n        metadata:\n          creationTimestamp: null\n          labels:\n            app: hello-openshift\n        spec:\n          containers:\n            - name: hello-openshift\n              image: openshift/hello-openshift\n              ports:\n                - containerPort: 8080\n                  protocol: TCP\n              resources: {}\n              terminationMessagePath: /dev/termination-log\n              terminationMessagePolicy: File\n              imagePullPolicy: Always\n          restartPolicy: Always\n          terminationGracePeriodSeconds: 30\n          dnsPolicy: ClusterFirst\n          securityContext: {}\n          schedulerName: default-scheduler\n      strategy:\n        type: RollingUpdate\n        rollingUpdate:\n          maxUnavailable: 25%\n          maxSurge: 25%\n      revisionHistoryLimit: 10\n      progressDeadlineSeconds: 600\n    status:\n      observedGeneration: 1\n      replicas: 2\n      updatedReplicas: 2\n      readyReplicas: 2\n      availableReplicas: 2\n      conditions:\n        - type: Available\n          status: 'True'\n          lastUpdateTime: '2021-07-09T19:16:32Z'\n          lastTransitionTime: '2021-07-09T19:16:32Z'\n          reason: MinimumReplicasAvailable\n          message: Deployment has minimum availability.\n        - type: Progressing\n          status: 'True'\n          lastUpdateTime: '2021-07-09T19:16:32Z'\n          lastTransitionTime: '2021-07-09T19:16:28Z'\n          reason: NewReplicaSetAvailable\n          message: ReplicaSet \"example-5fb6876865\" has successfully progressed.\n    ```\n\n  Note that:\n\n    - There are quite a bit more **metadata**. Metadata may be added by any number of controllers as needed to help with their function.\n    - The **spec** has more attributes filled in as well. These are default values that were not specified in our original YAML file. But sometimes it is also possible that some values are overridden by background admission controllers.\n    - The **status** sub-resource is how Openshift communicates that status of the resource. The **status** is updated regularly as the underlying state of the resource changes.\n\n6. Click on **Workloads**>**Pods**. \n\n  Note that the pods resources are managed by the controller for your **deployment**.  You do not create the pod resources yourself. That is the reason that **Pods** tab is under the **deployment** resource you just created.\n\n    ![](images/DeploymentToPods.png)\n\n8. Click on one of the pods:\n\n    ![](images/Pods.png)\n\n9. Explore the various tabs for the pod.\n\n    ![](images/ExplorePod.png)\n\n   - Overview: displays the overall resource usage for your pod. Note that for CPU usage, the unit is m, or milli-core, which is 1/1000th of one core.\n   - YAML: examine the YAML that describes your pod. This YAML is created by the deployment controller based on the specification you supplied in your deployment. Note that labels associated with your pod are what you had specified in the deployment.\n   - Environment: lists the environment variables defined for your pod. For your **hello-openshift** pod, there is none.\n   - Logs: shows the console log for your container. Note that it is the same log as the log from the Introduction to Docker lab, as the same image is being used.\n   - Terminal: Opens a remote shell into your container. As with the Introduction to Docker lab, no shell is available within the container for this image. This makes it more secure, but also more difficult to debug.\n\n#### 6.2.5 Create a service for the application in the OCP cluster\n\nA service enables the pods you just created to be load balanced within the Openshift cluster. \n\n1. Scroll down to the **Networking** tab on the left navigation, click **Services**, then click **Create Service**:\n\n    ![](images/CreateService.png)\n\n2. Update the YAML parameters as follows, then click **Create**:\n    \n    - Under spec.selector, \n      - change **MyApp** to **hello-openshift**. \n      - This is how the service will find the pods to load balance. Therefore, it matches the labels (**spec.selector.matchLabels**) that we used when creating the deployment for the hello-openshift application.\n    - Under spec.ports, \n      - change **80** to **8080** and \n      - change **9376** to **8080** (the same ports we used previously).\n\n  ![](images/CreateService_after.png)\n\n3. After the service is created, click on the YAML tab:\n\n    ![](images/CreateServiceAfterYAML.png)\n\n    The YAML file looks like:\n    ```\n    kind: Service\n    apiVersion: v1\n    metadata:\n      name: example\n      namespace: dk0300st\n      selfLink: /api/v1/namespaces/dk0300st/services/example\n      uid: 15ce0c1e-6f27-4ea6-9586-a7a8fbb7a724\n      resourceVersion: '2211781'\n      creationTimestamp: '2021-07-09T19:36:15Z'\n      managedFields:\n        - manager: Mozilla\n          operation: Update\n          apiVersion: v1\n          time: '2021-07-09T19:36:15Z'\n          fieldsType: FieldsV1\n          fieldsV1:\n            'f:spec':\n              'f:ports':\n                .: {}\n                'k:{\"port\":8080,\"protocol\":\"TCP\"}':\n                  .: {}\n                  'f:port': {}\n                  'f:protocol': {}\n                  'f:targetPort': {}\n              'f:selector':\n                .: {}\n                'f:app': {}\n              'f:sessionAffinity': {}\n              'f:type': {}\n    spec:\n      ports:\n        - protocol: TCP\n          port: 8080\n          targetPort: 8080\n      selector:\n        app: hello-openshift\n      clusterIP: 172.30.23.104\n      type: ClusterIP\n      sessionAffinity: None\n    status:\n      loadBalancer: {}\n    ```\n\n  Note that for this service, there is a cluster wide IP address created, and that it is being load balanced. Also session affinity is not set for this service.\n\n#### 6.2.6 Create a route for the application in the OCP cluster\n\nA route exposes your internal endpoints outside your cluster's built-in firewall. \n\n1. Click on the **Route** tab under **Networking** in the left navigation, then click **Create Route**:\n\n    ![](images/CreateRoute.png)\n\n2. Supply inputs to the following parameters, then click **Create**:\n\n    - Name: **example**\n    - Service: **example**\n    - Target Port: **8080 --> 8080 (TCP)**\n\n    ![](images/CreateRouteParams.png)\n\n    Note that you are ignoring TLS configuration just for the purpose of this lab.  Security will be addressed in a different lab.\n\n3. Access the app route at the link provided under **Location**:\n\n    ![](images/CreateRouteAccessRoute.png)\n\n  If you have configured everything correctly, the browser will show **Hello Openshift!**. \n\n  Congratulations, you just deployed your first application to Openshift.\n\n    ![](images/CreateRouteAccessRouteResult.png)\n\n#### 6.2.7 Changing Replica Instances\n\n1. Click on the **Projects** tab under **Home** from the left navigation, then click on **dk0300st**:\n\n    ![](images/LocateMyproject.png)\n\n2. Scroll down to the **Inventory** section and see the resources that were created. Recall that we have created one deployment with 2 pods in the specification. We also created one service, and one route.\n\n    ![](images/LocateMyprojectResources.png)\n\n3. Click on the 2 pods:\n\n   ![](images/LocateMyprojectPods.png)\n\n\n4. Delete one of the pods by clicking on the menu on the right, then selecting **Delete pod**. When prompted, click **Delete**.\n\n    ![](images/DeletePod.png)\n\n    This is not the right way to reduce number of instances. You will notice that as soon as one of the pods is being terminated, another one is being created. The reason is that the controller for the **deployment** resource knows that your specification is for 2 instances, and it honors that specification by creating another one. This also gives you automatic failure recovery should one of the pods crashes on its own.\n\n5. To change the number of instances, you will need to change the specification of your deployment. Click on the **Workloads**>**Deployments** in the left navigation, then click on **example** deployment:\n\n    ![](images/LocateDeployment.png)\n\n6. Click on the down arrow to reduce the replica size down to 1:\n\n    ![](images/DeploymentReducePod.png)\n\n7. After the operation is completed, click on the YAML tab to view the YAML file contents:\n\n    ![](images/DeploymentReducePod1.png)\n\n    ```\n    kind: Deployment\n    apiVersion: apps/v1\n    metadata:\n      annotations:\n        deployment.kubernetes.io/revision: '1'\n      selfLink: /apis/apps/v1/namespaces/dk0300st/deployments/example\n      resourceVersion: '2224790'\n      name: example\n      uid: 48bbff65-27d5-4823-9734-7d90e9a4a007\n      creationTimestamp: '2021-07-09T19:16:28Z'\n      generation: 2\n      managedFields:\n        - manager: Mozilla\n          operation: Update\n          apiVersion: apps/v1\n          time: '2021-07-09T20:03:23Z'\n          fieldsType: FieldsV1\n          fieldsV1:\n            'f:spec':\n            ....\n              'f:updatedReplicas': {}\n      namespace: dk0300st\n    spec:\n      replicas: 1\n      selector:\n        matchLabels:\n          app: hello-openshift\n      template:\n        metadata:\n          creationTimestamp: null\n          labels:\n            app: hello-openshift\n        spec:\n          ....\n\n    ```\n    Note that the console had changed the REST specification on your behalf so that the replica count is now 1:\n\n### 6.3 Work with OCP cluster through the command line\n\nYou can use both **oc**, the openshift command line tool, or **kubectl**, the Kubernetes command line tool, to interact with Openshift. \nResources in Openshift are configured via REST data structure. **oc** extends **kubectl** and offers the same capabilities as the kubectl but it is further extended to natively support OpenShift Container Platform features, such as OpenShift resources such as DeploymentConfigs, BuildConfigs, Routes, ImageStreams, and ImageStreamTags which are specific to OpenShift distributions, and not available in standard Kubernetes.\nFor the command line tools, the REST data structure may be stored either in  a YAML file, or in a JSON file.\nThe command line tools may be used to:\n\n- List available resources\n- Create resources\n- Update existing resources\n- Delete resources\n\n#### 6.3.1  Login to OCP cluster from Command Line\n\n1. Open a terminal window by clicing its icon from the Desktop toolbar.\n\n  ![](images/terminal-icon.png)\n\n2. Change directory to:  **/home/ibmdemo/app-mod-labs/dk0300st**\n\n  ```\n  cd /home/ibmdemo/app-mod-labs/dk0300st\n  ```\n\n3. Issue the **oc login** command to login to the OCP cluster:\n\n  ```\n  oc login https://api.demo.ibmdte.net:6443\n  ```\n\n  when promted, enter the login credientials as: **ibmadmin**/**engageibm**.\n  \n  ```\n  Authentication required for https://api.demo.ibmdte.net:6443 (openshift)\n  Username: ibmadmin\n  Password: \n  Login successful.\n\n  You have access to 66 projects, the list has been suppressed. You can list all projects with ' projects'\n\n  Using project \"default\".\n  ``` \n\n#### 6.3.2 Listing OCP cluster resources\n\n1. Use **oc api-resources** command to list all available resource kinds. \n\n  ```\n  oc api-resources\n  ```\n\n  Note that resources in Openshift have a group, version, and kind. Some resources are global (not in a namespace), while others are scoped to a namespace. Many resources also have short names to save typing when using the command line tool. For example, you may use **cm** instead of ConfigMap as a command line parameter when the parameter is for a **KIND**.\n  Example output:\n\n  ```\n  NAME                                  SHORTNAMES       APIGROUP                              NAMESPACED   KIND\n  bindings                                                                                     true         Binding\n  componentstatuses                     cs                                                     false        ComponentStatu\n  s\n  configmaps                            cm                                                     true         ConfigMap\n  endpoints                             ep                                                     true         Endpoints\n  events                                ev                                                     true         Event\n  limitranges                           limits                                                 true         LimitRange\n  namespaces                            ns                                                     false        Namespace\n  nodes                                 no                                                     false        Node\n  ```\n\n2. List all projects with the following command:\n\n  ```\n  oc get projects\n  ```\n  Example output:\n  ```\n  NAME          DISPLAY NAME   STATUS\n  default                      Active\n  ibm-observe                  Active\n  kube-node-lease              Active\n  kube-public                  Active\n  kube-system                  Active\n  ...\n  ```\n\n\n3. List all pods in all namespaces:\n\n  ```\n  oc get pods --all-namespaces\n  ```\n  Example output:\n  ```\n  NAMESPACE                                          NAME                                                              READY   STATUS      RESTARTS   AGE\n  default                                            ibm-toolkit-h9z8n                                                 0/1     Completed   0          3d6h\n  dk0200st                                           simpleapp                                                         1/1     Running     0          175m\n  dk0300st                                           example-5fb6876865-mtpmt                                          1/1     Running     0          118m\n  nfs-storage                                        nfs-client-provisioner-7bd5cb954-bznt4                            1/1     Running     0          29h\n  openshift-apiserver-operator                       openshift-apiserver-operator-67fbc98b9d-27xsx                     1/1     Running     0          29h\n  openshift-apiserver                                apiserver-7b65f5d4b6-9fcwd                                        2/2     Running     0          29h\n  openshift-apiserver                                apiserver-7b65f5d4b6-jqlr7                                        2/2     Running     0          47h\n  ...\n  ```\n\n4.  List all cluster nodes:\n\n  ```\n  oc get nodes\n  ```\n\n  ```\n  NAME      STATUS   ROLES           AGE    VERSION\n  master1   Ready    master,worker   380d   v1.19.0+43983cd\n  master2   Ready    master,worker   380d   v1.19.0+43983cd\n  master3   Ready    master,worker   380d   v1.19.0+43983cd\n  ```\n\n5. List all projects with command:\n\n  ```\n  oc get projects\n  ```\n\n    ```\n    NAME                                                    DISPLAY NAME   STATUS\n    default                                                                Active\n    ibm-cert-store                                                         Active\n    ibm-system                                                             Active\n    kube-node-lease  \n    ....                                                      Active\n    ```\n\n6. Get a specific project **dk0300st** from the **default** project. \n\n  ```\n  oc project dk0300st\n  ``` \n\n  ```\n  Now using project \"dk0300st\" on server \"https://api.demo.ibmdte.net:6443\".\n  ```\n\n#### 6.3.3 Work with existing resources\n\n1. View the deployment you did in the section above using the OCP web console.\n\n  ```\n  oc get deployments\n  ```\n  ```\n  NAME      READY   UP-TO-DATE   AVAILABLE   AGE\n  example   1/1     1            1           2d18h\n  ```\n2. Check the status of deployment.\n\n  ```\n  oc get deployment example -o yaml\n  ```\n  \n  ```\n  apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n    creationTimestamp: \"2021-07-09T19:16:28Z\"\n    generation: 2\n    managedFields:\n    - apiVersion: apps/v1\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:spec:\n          ...\n      manager: Mozilla\n      operation: Update\n      time: \"2021-07-09T20:03:23Z\"\n    - apiVersion: apps/v1\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:metadata:\n          ....\n      manager: kube-controller-manager\n      operation: Update\n      time: \"2021-07-09T20:03:23Z\"\n    name: example\n    namespace: dk0300st\n    resourceVersion: \"2224790\"\n    selfLink: /apis/apps/v1/namespaces/dk0300st/deployments/example\n    uid: 48bbff65-27d5-4823-9734-7d90e9a4a007\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        app: hello-openshift\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: hello-openshift\n      spec:\n        containers:\n        - image: openshift/hello-openshift\n          imagePullPolicy: Always\n          name: hello-openshift\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          ...\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2021-07-09T19:16:28Z\"\n      lastUpdateTime: \"2021-07-09T19:16:32Z\"\n      message: ReplicaSet \"example-5fb6876865\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    - lastTransitionTime: \"2021-07-09T19:57:13Z\"\n      lastUpdateTime: \"2021-07-09T19:57:13Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    observedGeneration: 2\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n\n  ```\n\n3. List the running pods created by the controller for the deployment.\n\n  ```\n  oc get pods\n  ```\n\n  ```\n  NAME                      READY   STATUS    RESTARTS   AGE\n  example-5fb6876865-mtpmt   1/1     Running   0          2d18h\n  ```\n\n4. Get the detail information of the pod.\n\n  ```\n  oc describe pod example-5fb6876865-mtpmt\n  ```\n  ```\n  Name:         example-5fb6876865-mtpmt\n  Namespace:    dk0300st\n  Priority:     0\n  Node:         master1/10.0.0.111\n  Start Time:   Fri, 09 Jul 2021 12:16:28 -0700\n  Labels:       app=hello-openshift\n                pod-template-hash=5fb6876865\n  Annotations:  k8s.v1.cni.cncf.io/network-status:\n                  [{\n                      \"name\": \"\",\n                      \"interface\": \"eth0\",\n                      \"ips\": [\n                          \"10.128.0.14\"\n                      ],\n                      \"default\": true,\n                      \"dns\": {}\n                  }]\n                k8s.v1.cni.cncf.io/networks-status:\n                  [{\n                      \"name\": \"\",\n                      \"interface\": \"eth0\",\n                      \"ips\": [\n                          \"10.128.0.14\"\n                      ],\n                      \"default\": true,\n                      \"dns\": {}\n                  }]\n                openshift.io/scc: restricted\n  Status:       Running\n  IP:           10.128.0.14\n  IPs:\n    IP:           10.128.0.14\n  Controlled By:  ReplicaSet/example-5fb6876865\n  Containers:\n    hello-openshift:\n      Container ID:   cri-o://72d60b9a47ff457d0ae60a30e7aed7b3809251989de6ebcf6725731f9645da7d\n      Image:          openshift/hello-openshift\n      Image ID:       docker.io/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\n      Port:           8080/TCP\n      Host Port:      0/TCP\n      State:          Running\n        Started:      Mon, 12 Jul 2021 06:12:54 -0700\n      Ready:          True\n      Restart Count:  0\n      Environment:    <none>\n      Mounts:\n        /var/run/secrets/kubernetes.io/serviceaccount from default-token-twzqj (ro)\n  Conditions:\n    Type              Status\n    Initialized       True \n    Ready             True \n    ContainersReady   True \n    PodScheduled      True \n  Volumes:\n    default-token-twzqj:\n      Type:        Secret (a volume populated by a Secret)\n      SecretName:  default-token-twzqj\n      Optional:    false\n  QoS Class:       BestEffort\n  Node-Selectors:  <none>\n  Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                  node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n  Events:\n    Type    Reason   Age   From     Message\n    ----    ------   ----  ----     -------\n    Normal  Pulling  19m   kubelet  Pulling image \"openshift/hello-openshift\"\n    Normal  Pulled   19m   kubelet  Successfully pulled image \"openshift/hello-openshift\" in 1.147946396s\n    Normal  Created  19m   kubelet  Created container hello-openshift\n    Normal  Started  19m   kubelet  Started container hello-openshift\n\n  ```\n\n5. Show the logs of the pod.\n\n  ```\n  oc logs example-5fb6876865-mtpmt\n  ```\n\n  ```\n  serving on 8888\n  serving on 8080\n  Servicing request.\n  Servicing request.\n  Servicing request.\n  Servicing request.\n  serving on 8888\n  serving on 8080\n  ```\n\n#### 6.3.4 Changing Replica Instance\n\n1. List pods.\n\n  ```\n  oc get pods\n  ```\n\n  ```\n  NAME                      READY   STATUS    RESTARTS   AGE\n  example-5fb6876865-mtpmt   1/1     Running   0          2d18h\n  ```\n\n2. Delete the pod with command **oc delete pod PODNAME**\n\n  ```\n  oc delete pod example-5fb6876865-mtpmt\n  ```\n\n  ```\n  pod \"example-5fb6876865-mtpmt\" deleted\n  ```\n\n3. List pods again with command **oc get pods** and note that a new instance of the pod has been created as expected. The deployment specified 1 instance, so the controller tries to maintain 1 instance.\n\n  ```\n  NAME                      READY   STATUS    RESTARTS   AGE\n  example-5fb6876865-qkj4r   1/1     Running   0          13s    example-75778c488-rhjrx   1/1     Running   0          28s\n  ```\n\n4. To increse the number of pod instances, we can patch the resource in one of two ways:\n   - Scripted patch using the **patch** option of the command line:\n      ```\n      oc patch deployment example -p '{ \"spec\": { \"replicas\": 1 } }'\n      ```\n   - Interactive patch using the **edit** option of the command line through **vi** editor:\n      ```\n      oc edit deployment example\n      ```\n      Under the **spec** section (not under the **status** section), change **replicas: 1** to **replicas: 2**, and save the change (by **:wq**).\n\n    ![](images/edit-deployment.png)\n\n      The output:\n      ```\n      deployment.extensions/example edited\n      ```\n\n      Note: The above edits the copy that is stored in Openshift.\n\n  Once the change is made, it will be applied to the pod automatically.\n5. List the pods to show only 1 pod is running: **oc get pods**.\n\n    ```\n    NAME                       READY   STATUS    RESTARTS   AGE\n    example-5fb6876865-76klt   1/1     Running   0          16s\n    example-5fb6876865-qkj4r   1/1     Running   0          8m23s\n    ```\n\n#### 6.3.5 Deploy new applications\n\nYou can also using **oc** commands to deploy applications to OCP cluster, the deplyment process is simliar to the Kubernetes deployment introduced in the **[DK0200 - Kubernetes Introduction](/labs/basic-labs/dk0200-kubernetes101/)** Lab, you just need to replace the **kubectl** with **oc** in the commands.  If you are intrested, please learn from the  **[DK0200 - Kubernetes Introduction](/labs/basic-labs/dk0200-kubernetes101/)** Lab.\n\n\n### 7.\tSummary\nIn this lab, you have learned some basic features and functions of Red Hat OpenShift Container Platform (OCP) and how to use them. To learn more about App Mod and DevOps, please continue with the rest of the lab series.\n\n**Congratulations! You have successfully completed OCP Introduction Lab!**\n\n","fileAbsolutePath":"/Users/yitang/Downloads/app-mod/src/pages/labs/basic-labs/dk0300-ocp101/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}